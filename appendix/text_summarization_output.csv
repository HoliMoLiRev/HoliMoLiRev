CLN9Q2KM;"<pad> this paper reviews the framework to develop a digital twin. the living model will continually adapt to changes in the environment or operation. it allows the prediction of the remaining useful life (RUL) of the physical twin. the key technology to propel a digital twin is the ""data and information fusion""</s>";"

The Role of Data Fusion in Predictive MaintenanceUsing 
Digital Twin
Zheng Liu1,a),, Norbert Meyendorf2,b), and Nezih Mrad3,c), 
 
1Faculty of Applied Science, University of British Columbia 
2Department of Aerospace Engineering, Iowa State University
3Defence Research and Development Canada, Department of National Defence Canada
a)Corresponding author: zheng.liu@ieee.org
b)norbertm@iastate.edu
c)nezih.mrad@forces.gc.ca
Abstract. Modern aerospace industry is migrating from reactive to proactive and predictive maintenance to increase platform operational availability and efficiency, extend its useful life cycle and reduce its life cycle cost. Multiphysics modeling together with data-driven analytics generate a new paradigm called “Digital Twin.” The digital twin is actually a living model of the physical asset or system, which continually adapts to operational changes based on the collected online data and information, and can forecast the future of the corresponding physical counterpart. This paper reviews the overall framework to develop a digital twin coupled with the industrial Internet of Things technology to advance aerospace platforms autonomy. Data fusion techniques particularly play a significant role in the digital twin framework. The flow of information from raw data to high-level decision making is propelled by sensor-to-sensor, sensor-to-model, and model-tomodel fusion. This paper further discusses and identifies the role of data fusion in the digital twin framework for aircraft predictive maintenance.
INTRODUCTION
According to [1], 42% of delayed flights are caused primarily by airline processes, such as maintenance. Airlines want better maintenance, repair, and overhaul (MRO) performance and improved quality, cost, and turnaround times within budget and schedule. Today’s MRO and in-service support (ISS) functions are facing challenges on several fronts, from slow turnaround time and poor data integrity to aging systems and outdated manual processes. Meanwhile, the MRO market is expanding and its growth is anticipated to have a big impact on the future of aircraft maintenance. To address these challenges for MRO, new technologies with powerful capabilities are needed to help make faster and more informed decisions for optimal maintenance avoiding any catastrophic failure.
An exciting chance to address this need is coming with the fourth industrial revolution triggered by new informa- tion and communication technology (ICT) and data-intensive methodologies (i.e., artificial intelligence and big data techniques). Internet Protocol version 6 (IPv6) is the most recent version of the Internet protocol, which provides practically unlimited IP addresses. IPv6 enables the “Internet of Things (IoT)” to happen as anything with an IP address can be connected through the Internet. The IoT brings together sensors, cloud computing, and big data analytics and will profoundly transform our society to a digital world. Industrial IoT, also known as IIoT, is the use of IoT technolo- gies in industrial applications where robustness, reliability, and security are highly desired performance requirements for IIoT. For aviation, the direct economic impact from IIoT is the implementation of predictive maintenance, which will turn the aggregated data and information into actionable decisions for aircraft maintenance. The predictive main- tenance has the capability to determine when maintenance should be performed based on the actual conditions of aircraft structures, components, and sub-systems. Once in place, predictive maintenance capabilities could eliminate
added expenses such as expedited shipping costs for parts or supplies, reduce overtime expenses for crews and, most importantly, lead to fewer unplanned maintenance downtime events [2].
The “digital twin” is a disruptive technology that creates a living model of a physical asset for predictive maintenance. The living model will continually adapt to changes in the environment or operation using real-time sensory data and can forecast the future of the corresponding physical assets [3]. A digital twin can be used to proactively identify potential issues with its real physical counterpart. It allows the prediction of the remaining useful life (RUL) of the physical twin by leveraging a combination of physics-based (physics from first principles) models and data-driven analytics.
The digital twin ecosystem comprises the sensor and measurement technologies, industrial Internet of Things, simulation and modeling, and machine learning. From the computational perspective, the key technology to propel a digital twin is the “data and information fusion,” which facilitates the flow of information from raw sensory data to high-level understanding and insights. The digital twin generally incorporates three-level fusion, e.g. signal-, feature-, and decision-level fusion,into itscomputational framework. At eachimplementationlevel,thedata-information fusion technologies contribute to high-quality signals, distinctive features, and optimal decisions. This paper identifies the role of data-information fusion in the implementation of digital twin for aircraft predictive maintenance and MRO-ISS business. The impact of industry digital transformation on MRO-ISS is analyzed and discussed. The technology trends are also highlighted.

DIGITAL TWIN FOR MRO APPLICATIONS
A definition of digital twin is “An integrated multiphysics, multiscale, probabilistic simulation of an as-built system, enabled by digital thread, that uses the best available models, sensor information, and input data to mirror and predict activities/performance over the life of its corresponding physical twin” [4]. The overall architecture of a digital twin ecosystem is illustrated in Fig. 1. The key functionality of digital twin is implemented through physics-basedmodels and data-driven analytics to provide accurate operational pictures of the assets [5]. Thus, the digital twin can mirror the activities of its corresponding physical twin with the capabilities of early warning, anomaly detection, prediction and optimization.
The industrial Internet of Things system carries out the real-time data acquisition through its smart gateway and edge computing devices. The pre-processed online sensory data will then be fused to feed the digital twin model. The offline data will be processed with text/data mining algorithms and then inputted to the digital twin model as well. The offline computing resources can be utilized to train the deep learning models used by the digital twin. The digital twin combines modeling and analytics techniques to create a model of a specific target, e.g., flight-critical component, and derive an actionable outcome from the model. These insights can be obtained by fusing the outputs from physicsbased models and data-driven analytics. Then, the digital twin will be used in a specified predictive maintenance workflow to enable the delivery of accurate forecasting, using the data that is continuously acquired with the IIoT system.
New technology aircrafts will generate terabytes of data from a cross-country flight with onboard sensors [6]. How to benefit from using these data to maintain these aircrafts remains a topic for MRO-ISS industry. Currently, the aerospace industry is moving from reactive to proactive maintenance for the purpose of reducing maintenance costs, operational downtime, and capital investment by extending the useful life of aircraft components. The aerospace industry has requested advanced analytics coupled with industrial IoT to achieve these objectives. The essential technologies are integrated into the digital twin ecosystem for the MROapplications.
ROLE OF DATA FUSION IN THE DIGITAL TWIN ECOSYSTEM
The data fusion techniques were applied to non-destructive evaluation years ago [7, 8, 9], while the research on   data fusion has a much longer history [10]. The more general concept of “information fusion” is defined as “the study of efficient methods for automatically or semi-automatically transforming information from different sources and different points in time into a representation that provides effective support for human or automated decision making.” [11]. In this paper, we use the term “data fusion” to refer to data and information fusion.
The role of data fusion in the digital twin ecosystem is illustrated in Fig. 3. The core elements of a digital twin’s “multiphysics, multiscale, and probabilistic simulation” are implemented by the physics-based models and data-driven models. The source of information can be treated as a “sensor,” either a “hard” sensor or a “soft” sensor. The potential of data fusion operations are identified from Fig. 2 [12]. The benefits of the fusion operations are listed as follows:
Sensor fusion – better signal quality
Physics model fusion – better modelperformance
Data model fusion – better modelperformance
Sensor and physics-based model fusion – adaptive physics-based model
Sensor and data model fusion – robust data-driven model
Physics and data model fusion – improvedprediction
Sensor, physics, and data model fusion – reliable decision making

FIGURE 2. Possible fusion operations in digital twin development.
Data are collected through aircraft onboard sensors and offline non-destructive inspection (NDI). The offline inspection data should be made available online as a reference or baseline for monitoring sensors. The raw data can be collected and processed with the “fog” or “edge” computing devices [13]. The NDI could serve as the baseline while the online sensors will monitor the working status and loads. The signal-level fusion will be conducted to achieve    a better signal with higher SNR or fidelity for further processing or interpreting while the data will be put into the historical records for future use. The sensory data and signals are often employed by physics or data models for model updating [14]. All the models need to be adaptive to the changing environment. The fused sensor data or extracted features will be fed into the models forprediction.

FIGURE 3. The role of data fusion in the digital twin framework.
Domain knowledge and experience are valuable for condition assessment and diagnoses. Incorporating human knowledge into the data-driven model should be accomplished by the “soft” and “hard” data fusion [15]. Modeling human knowledge still remains a topic for research community, e.g., human-centered computing [16]. The popular solutions include fuzzy inference, case-based reasoning, and ontology-based approaches [17, 16, 18]. The ontology uses classes, properties, and instances to represent the terms and relations among specific knowledge [17]. This modeling method can provide common semantic and query heterogeneous databases [17]. The ontologies have been applied  to information fusion [19, 20]. Web-based technologies also enable the retrieving, analyzing, and processing relevant information online [21, 22] and could provide another source of information or tool for the digital twin ecosystem. Data-driven modeling is propelled with recent advances in machine learning and artificial intelligence. It exclusively relies on the data directly or indirectly related to the target object. Upon the availability of historical data, it is possible to train a comprehensive model to predict the remaining useful life (RUL) of the flight-critical components or subsystems. The data-driven models can be used for diagnosis and prognosis. The performance can be further enhanced through the ensemble and fusion of multiple individual models [23]. Thanks to the capability offered by IIoT for acquiring a large amount of relevant data, the data fusion will also evolute into evidence-driven or learning-based approaches for big data sets from the classic methods, such as Bayesian inference and Dempster-Shafer reasoning.
Physics-based models are based on the first principles of physics. This depends on the physical processes involved, e.g., damage growth or an aerodynamic application. Due to the limited knowledge of our human beings on  a complex mechanism, individual or even multiple models can only partially simulate the overall process.  Multiple models need to run in parallel to reflect the varied aspects of a physical mechanism of interest. The physics-based models also receive inputs from acquired measurement and sensory data from the fog/edge computing to adapt the models to the dynamic environment. Both the data-driven models and physics-based models are running in a cloud computing environment as the cloud is the place to centralize and process all the pre-processed data from fog/edge computing devices.
Both the data-driven and physics-based models output the prediction of remaining useful life (RUL), which can be used in the following decision-making process. Knowledge of the RUL will enable an efficient maintenance schedule by avoiding any unpredicted system shutdowns [24]. Complementing each other, the two types of modeling need to be fused for more accurate and reliable prediction [25]. The decision will be made based on the multi-model fusion outcomes. The action will be taken based on the derived evidence. The future MRO can be performed based on the risk estimated from the RULaccordingly.
TRENDS FOR TECHNOLOGY DEVELOPMENT
Towards the future digital twin ecosystem, the technologies also evolve with the advances of the “information and communication technology” (ICT) as well as the revolution initiated by Industrial 4.0. One field is the non-destructive testing. There are a number of trends for future non-destructive testing (NDT):
 Implement real-time and flexible NDT with modern ICT;
 Integrate NDT into manufacturing process through onlinemonitoring;  Achieve decision making in NDTservices.
Conventional NDI is usually conducted offline manually. The real-time digital technology makes the NDT results available in a timely manner. The current innovations available on the market include the USB-based devices, which can stream inspection data to central data repository in real time [26, 27]. This change makes it possible to manage and store NDI data online. More importantly, the historical inspection data become available. The “mixed reality” technology, such as Microsoft HoloLens, can be applied to visualization in digital twin as well as implement remotely supervised inspection. 3D model is critical for digital twin development. Modern technologies enable precision scanning of parts and components with laser and camera based vision technology [28]. This technology will make the 3D model of existing components available, which were not manufactured digitally. While sensors for online monitoring are being actively developed, the conventional NDT has well-established standards and performance metrics, i.e., probability of detection. Replacing NDT with online sensors will not happen by tomorrow, but the convergence of these two technologies will be the future. Decision making is the objective of digital twin, and NDT is an essential tool to achieve such objective.
SUMMARY
This paper briefly describes and reviews the digital twin technology for aircraft maintenance, repair, and overhaul. The digital twin ecosystem integrates the modeling and simulation while taking advantage of the industrial Internet of Things for data acquisition and information processing with cloud computing. The focus is on the role of data fusion in the digital twin development. The data fusion also evolves with the advances of ICT and deal with increasing data in terms of its volume, velocity, and variety. The flow of information from raw data to high-level understanding is propelled by data fusion techniques, which are implemented and will function at different levels. In addition, the technology advances and requirements also have a great impact on the conventional industry such as non-destructive testing. All the technology advances will change the future MRO business in the aerospace industry.
"
7PZIFQPX;<pad> the aim was to prototype a digital twin (DT) which meets the logistic behavior of a new family of automated guided vehicles (AGVs) the code ensures that the CGV makes decisions autonomously, on the basis of the data provided in real-time by the other CPSs. the code ensures that the CGV makes decisions autonomously, on the basis of the data provided in real-time by the other CPSs.</s>;"From the Cyber-Physical System to the Digital Twin: 
the process development for behaviour modelling of a Cyber Guided Vehicle in M2M logic 
Bottani E.*, Cammardella A.**, Murino T.**, Vespoli S.**  
*Dipartimento di Ingegneria e Architettura – Università di Parma, Parco Area delle Scienze, 181/A – 43124 
Parma – Italy (eleonora.bottani@unipr.it) 
** Dipartimento di Ingegneria Chimica dei Materiali e della Produzione Industriale (DICMAPI) - University of 
Naples Federico II, P.le Tecchio, 80 – 80125, Naples – Italy (murino@unina.it, assunta.cammardella@unina.it, silvestro.vespoli@unina.it) 

Abstract: This paper describes a research whose aim was to prototype a Digital Twin (DT) which meets the logistic behavior of a new family of automated guided vehicles (AGVs), based on the Cyber-Physical System (CPS) paradigm. The research consists of two steps. First, the implementation of CPS logic on an existing micro-controlled rover is examined; then, a traditional discrete event simulation (DES) software is used to simulate different environment application for the DT, including some modifications that allow identifying the most suitable solutions for the research aim. The specific design process has limited the stochastic variability of the simulated system to the mechanical component of the CPS-AGV. This because of the absolute identity of the logistic logic, operating both in the code used by the CPS-AGV micro-controller and in the code of simulating the system. The results show that the combined CPS-DT architecture allows a  strategic optimization of the plant resources in Industry 4.0 configuration. To this end, different policy have been implemented to optimize the autoadaptive behavior of CPS-AGV, and each one of them has proven to be effective in a specific scenario. Outcomes of this study provide an industrial justification to the design and managing costs of Digital Twin implementation in an Industry 4.0 production system. 
Keywords: Industry 4.0, Cyber-Physical Systems, Digital Twin, Machine to Machine 

1.Introduction 
The current industrial world is characterized by a significant evolution which increasingly leads large areas of manufacturing systems to the Industry 4.0, thanks to its greater perspective of sustainability (Stock and Selige 2016). To the best knowledge of the authors, several works in literature tried to define this revolutionary production approach, but none of them provided a clear and unique ontology. Therefore, it is possible to identify some pillars for all the different possible vision of Industry 4.0: Machine-to-Machine (M2M), IoT (Internet of Things), Big Data, Cyber-Physical Systems (CPSs), Digital Twins (DT), Augmented Reality, Additive Manufacturing, Cybersecurity and Cloud Cooperation. 
Each pillar represents a component of the new theorized architecture. Although, only some of these are strictly required to characterize a production system as a 4.0 configuration. In particular, an Industry 4.0 production system is typically featured by the attribution of an advanced capability of machines in an autonomous decision-making process.  
This new machine’s ability can be realized exploiting different computational architectures and technological solutions. Each equipment could be set up in a particular configuration, depending on the required objective, features, aims or designs. However, in any of this case, they are all characterized by a CPS architecture. This new class of equipment is expected to replace the so-called production centers (e.g. generic automated machine or flexible manufacturing systems), integrating the traditional production features with the capability of communicating with other interconnected CPSs and of embodying algorithms oriented to an independent decision-making. This integration leads to a proactive equipment able to execute and optimize the production goals using a selfadaptive behavior. 
In this scenario, it is important to outline a path for effectively introduces the design of this innovative behavior, primarily from a technological point of view. The novel relationship between DT and CPSs, with the help of the acquired knowledge from the analysis of the Industrial Big Data, makes possible to build a first attempt of the CPS’s self-adaptive behaviour. In this sense, this paper shows the first prototype of Digital Twin in the complete step for building its architecture and logic. In particular, we focused on the development of a Cyber-Physical Automated Guided Vehicle Digital Twin for solving the typical Material Handling problem of a Job-Shop manufacturing system. We investigated about the simulation logic to be implemented in a DT and analysed the benefits that the introduction of self-adaptive behaviour (and, more in general, of the Industry 4.0) will bring in the current manufacturing system.  
2. State of the art 
Even if the Industry 4.0 is only a recent industrial topic, an extensive literature is already focused on this topic and its hypothetic scenario evolution (Hermann et al. 2015). In some of the available studies, a significant diffusion of enabling technologies for Industry 4.0 is predicted (Wan 2015). Nonetheless, just their implementations are not expected to be able to transform the old production systems in Industry 4.0 ones (Posada 2015). In particular, according to (Jian et al. 2016), the main element featuring a 4.0 system is the presence of the CPS paradigm, the Industrial Big Data, Internet of Things in the M2M interaction logic and the Digital Twin. The relationship between these components is characterized by a high complexity and different configurations, each one of them connected to a specific objective (Oesterreich and Teuteberg 2016). In this context, the most important feature is the empowerment of the equipment with the above decision-making capability, which definitely transforms a simple automated machine in a CPS (Jazdi 2014). 
However, to make possible this behaviour, it is recommendable  and necessary to implement three capabilities to the CPS: an intercommunication ability between CPS through the IoT for allow a continuously exchange of data, a comprehensive knowledge of the system state and, most of all,  a computational and autonomous ability (Galaske and Anderl 2016). As said above, Industrial Big Data (Thiede et al. 2016), DT (Schroeder et al., 2016) and M2M logic (Guizzi et al. 2016) play a critical role in embodying all these capabilities in a CPS.  
Some example of hardware and CPS architecture are carefully and specifically developed in the literature (Lee et al., 2015), even if a hierarchical relationship between the components of the system seems to be always necessary to solve the potential conflicts caused by the adoption of M2M logic. In fact, without such configuration, the continuous communication between the industrial equipment and the consequent cooperation in the decisionmaking processes would be impossible at this stage (Bagheri et al. 2015). With the Industry 4.0 and the introduction of the CPS’s capability, emerges the old dichotomy between a centralized or decentralized production logic of the manufacturing system. (Schuhmacher and Hummel 2016). To the best knowledge of the authors, even doing a careful literature review, neither policies and procedures for designing nor a systematic theory for evaluating the performance of the two different logic is available in order to solve the proposed dichotomy.  
According to (Rosen et al. 2015) and (Monostori 2014), a CPS may achieve its autonomy in industrial production process decision-making with the help of a DT which could replace the production system time by time. In this context, arise the importance of the DT in the Industry 4.0 revolution, as a necessary step for improving the selfadaptive behaviour of the interconnected CPSs.   
As a matter of fact, despite the substantial investment, it is possible to show that the implementation of a DT allows an exponential improving of production quality (Grieves 2014).  
Several authors describe the implementation of DTs developed with different mathematic methods. Grounding a DT modelling on the use of simulation software is not an original idea. For example, (Schluse et al. 2016) focused their attention on an agent-based simulation approach with the introduction of “Experimentable Digital Twin” concept. The results have been very encouraging and, the proposed approach, show its feasibility and promises in a manufacturing environment. In short, the use of the DTs concept, allow a practical integration of the simulation, like the Discrete Event Simulation (DES) (Converso et al., 2015) or System Dynamic Approach (Ascione et al. 2014), in the manufacturing context. In the opinion of the authors, there are three innovative scenarios for the DT uses.  
First of all, it is possible to use the DT for the development of the future system. In this context, the DT have itself the same rules and structure of the future equipment (i.e.,.it represent the virtual substitutes of the real object with the same code and behaviour). This scenario allows simulating, developing, characterizing and verifying, the behaviour that the real equipment will show inside the manufacturing system.  
Secondly, the DT bring all the simulation technology available for use in the real system, allowing the selfadaptive behaviour of the equipment. The machine, in this scenario, can simulate the different environment, establishing the best decision to take in a particular situation. In this scenario, the environment is simulated. However, the DT replicates the same decision that the real equipment would have taken (without introducing another stochastic event). 
Thirdly, the DT helps to populate relevant Industrial Big Data with the support of the simulation tools. In fact, simulating the different environment in which the real equipment could work, a large set of data could be collected and analysed. 
3. The CPS-AGV’s Digital Twin experiment 
To build, implement and apply a DT proof of concept focused on the Industry 4.0 paradigm, a simple Job-Shop production system was built.  It consists of four processing stations, three of them configured as Manual Assembling Station and the other as a Warehouse/Source. All the stations are connected through a predefined circuit in which the AGVs can move between the different 
Processing Station (Fig. 1). It should be noted that the three Manual Assembling Station was enabled with a various and a different number of operations, depending on the setup costs supposed to simulate. In this environment, it is necessary to solve the scheduling problem of the plant. Due to its layout, it is a typical Job-Shop scheduling problem in which the AGV are responsible for the Material Handling (MH) of the plant. In Industry 4.0, the AGV considered should be based on the Machine-To-Machine (M2M) interaction logic. For this reason, it is a CPS-AGV or, more concisely, a Cyber Guided Vehicle (CGV) with a selfadaptive behaviour for solving the Material Handling problem of the plant.  
Industrial Material Handling (MH) is one of the most exciting Industry 4.0 application field (Seitz and Nyhuis 2015). The innovation of a supply chain in a 4.0 scenario needs the achievement of a 4.0 configuration for each logistic component (Santillo et al. 2013). Scientific literature highlights a large set of examples of AGV use: all the application shows the same common feature: the capability of carrying a load of material, running an automated guiding schedule (Gallo et al., 2012). In the following paragraph, we investigate on all the process necessary for the development of the AGV’s DT. 
 
Fig. 1 – The Job-Shop Production System built 
 
3.1 The Cyber Guided Vehicle 
For the real construction of the above theorized CGV, the Zumo Robot (Pololu Robotics, 2017) has been selected and chosen. Zumo is an automated rover able to follow a track on the floor. The choice of Zumo Robot was motivated by the wide programming library, usable to control the rover, and most importantly, by the opportunity to implement the control tools developed for Arduino UNO microcontroller, assembled on the rover (Fig.2). 
 
Fig. 2 – Zumo Robot 
The Rover is equipped with a proximity sensor, which allows detecting the way line (i.e. the path of the CGV). This feature is handy, even if in its traditional configuration Zumo is just an AGV. Its behaviour can be programmed and elaborated by an algorithm on the microcontroller. In this context, Arduino UNO allows an easy implementation of the algorithm, thanks to a C++ interpreter. This feature allows writing the same algorithm in both the real CGV and the simulation software (the DT). The code ensures that the CGV makes decisions autonomously, on the basis of the data provided in real-time by the other CPSs and the use of industrial Big Data, through a dedicated Wi-Fi software infrastructure (access point). It should be noted that the utilization of the same code in both the real CPS and the DT avoid inserting another stochastic variable inside the simulation. The CGV is not simulated in its behaviour during the simulation. The Simulation will simulate only the environment, but the answer of the CGV are the same of the real system because the behaviour is an exact copy of it, not a simulated one. 
3.2 The Conceptual Design of the experiment 
This research has been focused on the conceptual design shown in (Fig.1). 
The conceptual model consists of the following items: 
an Input Station that generates orders (source); 
a virtual backlog warehouse; 
a warehouse with unitary capacity that hosts the order ready to be worked; 
the vehicle for handling the orders/items that are termed the CGV to recall that it belongs to the CPS class; 
three Manual Assembling Station for the manufacturing of the item (named Processor 1, 
2,3); 
six buffers for compliant (green) and not compliant (red) items; 
two big warehouses to collect the finished items, again one green for compliant and one red for non-compliant items; 
two pallet trucks that pick up the finished items; 
twelve pairs of proximity sensors to map the logistic state of the system.  
The plant layout of the establishment in question is designed in such a way that the CGV driven vehicle can move on a predefined circuit to which the unit storage capacity and the three machineries belong. The path, on which the two transpallets move, being guided by man, is irrelevant to our discussion. 
To implement the whole system, the DT runs the following processes: 
The source serves as the input station and generates orders using a known statistic frequency. 
The orders generated by the source are moved using baskets to the buffer with unitary capacity, where they are ready to be picked up. 
The CGV receives the order of loading of the generated basket. 
The CGV interacts with the three machines (Processor 1, 2, 3) arranged on the circuit and selects the equipment where to deliver the basket. Then, it entrusts processing, according to a chosen production criterion and the known self-detected state of the machinery. 
The chosen machine processes the materials basing on the known stochastic behaviour of time, reliability and quality. 
Downstream of the production, the processed basket is moved to one of the buffers (either red or green). 
The pallet trucks move, through the action of an operator, picking up the processed items and carrying them to the respective main buffers, located outside the path. 
Meanwhile, the CGV repeats its path, receives a new order of loading and makes a new decision. Such a decision is based on the self-adaptive algorithm that considers the new data of each workstation and the status of the equipment updated during the simulated process.  
The rover process the tasks four and eight using selfadaptive decision-making algorithms (Fig.3-4), which optimize the chosen policy for the system: 
 
Fig. 3 - Algorithm for Minimization of the Process Time  
 
Fig.4 - Algorithm for Minimization of the Logistic Time 
In both codes, the self-adaptive behaviour of the system is achieved using a simple exponential smoothing algorithm, applied to the workstation behaviour time series, retrieved from the Industrial Big Data of the system. 
3.2 The Simulated Policy 
The real CGV and the correspondent DT is designed for operating in three different decision-making logics, correspondent to three different behaviour: 
Pre-Planned Policy, in which the CGV represents the AGV without its ability to think. It represents 
the old traditional scenario of Industry with a centralized planner. 
Minimization of the Processing Time Policy, in which the CGV will cooperate in the scheduling of the plant, preferring to assign less consequential job possible to a Processing Station, minimizing the continuous use of each station with a more flexibility of the plant. Even this plan represent an Industry 4.0 scenario, in which CGV is a part of the CPSs network with the common goal of minimizing the continuous utilization time of Station. 
Minimization of the Logistic Time Policy, in which the CGV will cooperate in the scheduling of the plant, preferring to assign more consequential job possible to a Processing station, minimizing the movement and the time for the internal logistic. It represents an Industry 4.0 scenario, in which every CGV is a part of the CPSs network with the common goal of minimizing the Logistic Costs and Time.  
Results 
In the simulation, ten tests for each Policy were performed, and the values obtained as a result of the individual simulations were reported in the following Figures. 
The different scenarios will be determined by the variation of two parameters: 
The frequency with which Source generates pieces ( a type of Demand). 
Selling Price. 
The obtained data are reported in the table, and we have charted each the value of the objective function “Gross Profit” for each scenario in both the Policy applicable 
The aim of the analysis is to understand what decisionmaking policy to set on the rover, to maximize Gross Profit of the system, i.e. the objective function in eq.1 below: 
(€) = 	∗	−	∗	−	∗	−	( ℎ ∗	) 
where: 
= selling price; 
= number of compliant items; 
= penalty for non-compliant items; - 	= number of non-compliant items; 
= penalty for out-of-stock items; 
= number of out-of-stock products; 
ℎ =production hourly cost of machine i;  - 	= production time of machine i. 
The policies chosen for comparison by the DT arise from two big issues, i.e.: the variability of (external and uncontrolled) production factors and the trade-off between logistics and production costs. In order to highlight the effectiveness of coupling a DT with a CPS-CGv, we compared a traditional centralized (pre-planned) policy (Policy 1) with the two Industry 4.0 Policy shown in the previous paragraph (i.e., Minimization of the Logistic Time (Policy 2) and the Minimization of the Processing Time Policy (Policy 3)). 
4.1 Simulation and policy comparison 
The first scenario is representative of a situation where demand has a low variation, and the selling price is high. Under this condition, the plant material handling tasks is likely to be rescheduled several times. Running simulations with DT in the loop, the gross profit for each short period was estimate, with ten simulations per policy, computing their average values and comparing them by recording the results in proper tables and graphs. Fig. 5 presents the results of the first scenario. 
 
Fig. 5 - First Scenario results 
What we notice right away is that by implementing a logic 4.0 and then implementing in our CGV an auto-adaptive decision-making autonomy, the average profit is higher. Under these conditions, the winning policy is the Minimization of the Logistic Time Policy. 
The second scenario is representative of a situation where demand has a high variation, and the selling price is low. Even in this scenario, compared to a preventive schedule and therefore to a static scheduling, logic 4.0 prevails. In particular, compared to what has been obtained before, the variation of the scenario will also lead to a variation of the best criterion. In fact, in that scenario where the part generation frequency is low as well as the cost of the work piece worked, the best logic is the Minimization of the Processing Time Policy. The Fig. 6 shows the results of the second scenario for each policy considered 
 
Fig. 6 - Second scenario results 
In the third scenario, the MH system operates in a process plant where the production equipment reliability, the market demand, and the selling price are characterized by a low degree of variability. Under this conditions, the process rescheduling of the plant MH system decreases significantly.  In this scenario, every Policy applied in the DT do not make a significant change in the objective function. Instead the situation of the other scenario, in this case the use of Industry 4.0 hasn’t a competitive impact on the Production goal. The Fig. 7 shows the results relating to each policy applied to this scenario. 
 
Fig. 7 - Third scenario results 
4.2 Results analysis 
From an analytic point of view, it is appropriate to summarize the recorded behaviour in the following (Table 1), where the average gross profit (in €) is reported: 
Table 1: Scenario vs. Policy behaviour 
The main consideration that arises from the above Table is that when changing the demand typology and the industrial contingencies, the policies should be changed as well. To this end, the implementation of a DT expands the operation opportunities and the industrial potential of CPSs. As a matter of fact, using the results of the DT, not only CPSs can apply a decision-making autonomy  (to optimize their goal), but they can choose “autonomously” between their optimized available policies.  
5. Conclusions 
This paper highlights how CPSs, DT and industrial Big Data are strongly connected in an Industry 4.0 manufacturing system. The combination of these three pillars allows meeting the concepts of decision-making autonomy and self-adaptation of machines operating in an industrial plant. This conclusion is supported by the possibility of determining (and, therefore, choosing) the optimal production schedule depending on the market scenario, by different possible choices on the tactical criterion concerning the short period considered. 
The experimental approach also demonstrated that Industry 4.0 is not always the “best” configuration for a production system. When the strategic positioning of an industrial plant highlights operating conditions characterized by uniformity of performance and behaviour, both internal and external to the production domain, the tactical approach can be managed with greater convenience without implementing autonomous decision-making algorithms in the machines. 
In the case of equipment reliability or (high) production variability, the self-adaptive reaction feature of a “4.0 plant” is a non-negligible competitive factor. In this context, M2M logics are a fundamental pillar on which the reaction and optimizing capacity of the system should be based. Considering a strictly M2M oriented system, the results achieved highlight that the application of DES to the DT logic allows a complete and well-performing operations of the model. 
With this aim, exploiting a general simulation model to develop a DT is a cheap and, at the same time, useful and suitable solution, agreeable to the industrial needs and design purpose. As a matter of fact, our results highlight that, under the project hypothesis of developing a plant system (or subsystem), strictly M2M oriented, built and managed with the use of CPSs, the DT built is effective in reproducing the CGV and helpfully in optimizing its schedule. 
The main innovative idea of this paper consists in basing the DT implementation on the use of same operational software code of self-adaptive behaviour and scheduling optimization routine, both in the CPSs microcontroller, both in the DT (that is). The simulated experiment shows that the adopted design process just reduces the stochastic variability of the simulated system to the natural variations of the physical factors not controlled by the DT. This conclusion implies that, under the hypotheses made, the simulation software can be implemented in practice to support industrial DT. On the one hand, in fact, using the proposed process there is no variability introduced by the operational code of simulation; on the other hand, all the graphic and computational tools used in a simulation software make it possible to achieve several advantages.  
The AGV systems fall within the hypothesis of the system provided for the proposed model. In a particular way the rover used in work, already able (as every automated equipment) to perform an assigned path, may be transformed with success in a CGV, by writing the code items relating to the transfer of decision-making autonomy to the machine in its microcontroller. 
As a discussion theme for future developments, it is appropriate to underline that the results of this work depend strictly on the hypothesis of a production system operating in M2M logic. This assumption is very restrictive for most of the real production plants, which need tools that centralize at least partially the flow of decisions. The original aim of this research was to build and implement a concrete example of DT, usable for small industrial subsystems oriented to the smart manufacturing and Industry 4.0 approaches. This aim has been completely achieved; nonetheless, the complexity of the real industrial contexts could generate more structured equipment interactions. This involves conflicts between resources, which cannot be managed using a rigorous M2M logic. In this circumstance, implementing a plant, system or subsystem DT requires the introduction of production centralization tools and the careful review of the DES logic, which would probably need to be replaced with an AgentBased logic.  
 
"
U6QPZNLA;<pad> the purpose of work is creation of a digital twin of the existing laboratory installation. key parameters of work of a column are registered by means of sensors installed in a column. the mathematical model offered for use will be created in a Matlab/Simulink package. it is possible to judge reliability of the digital twin in process of coincidence of data of his work with the schedule of real work of a column.</s>;"IOP Conference Series: Materials Science and Engineering
PAPER • OPEN ACCESS
Development and creation a model of a digital twin of the cubepart rectification installation for the separation of a binary water-alcohol mixture
To cite this article: R A Khakimov and N S Shcherbo 2018 IOP Conf. Ser.: Mater. Sci. Eng. 450 062006
 
View the  for updates and enhancements.
This content was downloaded from IP address 131.188.6.12 on 12/01/2020 at 10:08

Development and creation a model of a digital twin of the cubepart rectification installation for the separation of a binary water-alcohol mixture 
	R A Khakimov and N S Shcherbo 	 
Faculty of Magistracy and Elite Education Omsk state technical university, 644050, Omsk, Pr. Mira, 11, Russia 
 
Abstract. In article the way of creation of development of the digital twin of a cube part of a 
rectification column for the separation of a binary water-alcohol mixture, using the software Matlab/Simulink is considered. The technology solution on creation and verification of the digital twin of the available laboratory installation and structure of digital model, which is the basis for the digital twin, are developed. 
Introduction 
As part of the piecemeal oncoming Industry 4.0, digital twins are one of the main elements on which the industry will begin its development. Therefore, now many leading engineering collectives and laboratories are engaged in creation and development of digital twins, and, achievements in the field of digital modeling, even have experience of introduction on productions abroad, mainly, at oil refineries. 
Basis of simulars of technological process are the general principles of calculations of material - thermic balances of oil processing productions (productions, connected with change of aggregate state and also the component and chemical composition of material streams concern them).Each production consists of several stages, each of which converts energy and produces a certain effect on material flows. Technological schemes are usually used to describe the sequence of stages, where each element corresponds to a specific technological process.Connections between elements in technological schemes represent material and power streams. As the basis of Modeling of the technological scheme is used the general principles of thermodynamics, both to separate elements of the scheme, and to system in general. 
 	 
Problem definition 
The purpose of work is creation of the digital twin of the existing laboratory installation of a rectifying column for division of binary mix. And also a research of a technique of creation of such doubles for her further use in creation of other digital models. For achievement of this purpose it is required to solve a number of problems, such as research of laboratory installation; creation of the digital twin of installation in the modeling environment; verification and assessment of accuracy of work of the double. 
 
Theory 
Rectification of ethanol - the containing mix is carried out by means of laboratory to rectifying installation (figure 1). As a heating element the tiny tile is used. Process of rectification happens to 
 Content from this work may be used under the terms of the Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
Published under licence by IOP Publishing Ltd
periodic refueling of ethanol-containing raw materials. Installation comprises three main parts — a evaporative cube, the mass-exchanged section, filled with a nozzle and the condenser.  

Figure 1. Schematic diagram of laboratory installation. 
The main part of installation is the compound column consisting from top and lower rectifying parts. Conditionally, the top part includes the condenser, the refrigerator for condensation of the selected mix, the alcohol selection regulator and also the system of binding branch pipes. At rectification in the refrigerator and the condenser executed according to the scheme ""pipe in a pipe"", constantly a countercurrent reverse water which creates necessary outflow of heat from mix arrives. In the top part of vat capacity conclusions of a manometrical tube are drawn. 
During the experiment, concentration of the received alcohol is determined by the current values of its density (static pressure of a calibrated column of the liquid which is taken away in alcohol receiver taking into account her temperature). Key parameters of work of a column, such as temperature, pressure, content of alcohol in the selecting mix are registered by means of a set of the sensors installed in a column and operated by the computer. 
First of all, it is necessary to determine a set of independent parameters and indicators on the basis of which the model will work. Among such parameters: 
Operating mode of the heater, in Watts; 
The volume and composition of the mix which is filled in in a cube; 
Reference temperature of the liquid which is filled in in a cube;  	Sizes and form of a cube; 
Air temperature. 
The main control parameters are indicators poured into the mixture container and the power of the heater as they can be changed freely before start of a column for verification of model.   
In general, the obtained experimental data of work of installation show that pressure cubed of a column increases slightly. Therefore, the decision not to consider influence of change of pressure upon some dependent parameters, such as specific heat of steam formation or coefficients of a thermolysis has been made. Therefore, a state equation role in this model - creation of curve dependence of pressure from time is exclusive, according to this task the equation was chosen. 
Based on the relevant literature on heat transfer, a model was created, shown in figure 2. 
In the beginning, during the analysis the decision to use the cubic equation of Peng-Robinson which well proved to be when modeling vapor-liquid balance and calculating density of liquid of hydrocarbonic systems has been made. However, this equation in case of its application to nonideal systems which alcohol-containing mix is, yielded too inexact results. Therefore, instead of the original cubic equation of Peng-Robinson it has been decided to use his modification offered by Strizheck and Vera in 1986. This modification extended application of an initial method to nonideal systems. Also this equation corresponds to curves of elasticity of vapors of clean components and mixes much more precisely and has more flexible rule of mixture. [12,13] 
 
Figure 2. Model of distribution of thermal streams in a cube part of a column. 1 - A heating tile, 2 - cube Material, 3 - Mix of ethanol and water, 4 - Steam-gas mix. 
Modification of the equation of a condition of Peng-Robinson has been offered in 1986 by scientists Strizhek and Vera. This cubic equation of a state which for brevity has the designation PRSV has one additional adjustable parameter for clean connection. Using suitable two-parametrical rules of mixture this equation can be used for reproduction of data on balance of steam and liquid mix with accuracy, comparable with some general double methods in which activity coefficients for a liquid phase and also the special equations of a state for a steam phase are used. Subsequently, additional modification of the created PRSV of the equation which reflected processes of unideal systems even more precisely has been offered. 
The recent research has also shown that the values of fugitives calculated from the equation of PRSV reflect a real PVT picture in vapor-liquid system most precisely, even more precisely than the data obtained by dual methods. [14] 
PRSV modification of the equation of a state formally corresponds to the equation offered by Peng and Robinson in 1976: 
		(1) 
 where P — pressure of saturated steam in system; T — temperature in system; V \molar volume; andα and b — the special coefficients calculated on the following formulas: 
(2) 
(3) 
 (4) 
 
The way of calculation of coefficient of k for further calculation of parameter α in the equation was the main change offered within PRSV of modification (4.10): 
		(5) 
 where 
	 	(6) 
And in the equation (6) the k0 parameter is called Single Pure Compound Adjustable Paramter. The k0 value depends on critical values of temperature Tc and pressure of Pc and also on an acentric factor  which are individual for each parameter of mix. 
The given formulas (1) — (6) are fair only for the clean systems consisting of one component. For the systems containing two and more components it is necessary to use Wang-der-Vaals rules of mixture for difficult systems. For work with difficult systems first of all it is necessary to transform the equation (1) to a polynomial appearance: 
	 	(7) 
Where 
(8) 
(9) 
 (10) 
 
Rules of mixture for parameters in multicomponent systems have the following appearance: 
(11) 
(12) 
(13) 
 
(14) 
(15) 
(16) 
 
In the used equations of kij is a coefficient of binary interaction between i and j component; yi is a molar maintenance of a component in mix. [14,15,16] 
 The mathematical model offered for use which will be created in a Matlab/Simulink package is guided by the basic fundamental laws and the equations which describe behavior of binary mix cubed of a column. The full graphic type of mathematical model is presented in the figure 3, it reflects not only conditional division of model into separate blocks of processing of various data, but also shows interrelations between these blocks. On this basis the digital model of a cube which will be verified by real data of his work will be created, and, on the basis of this verification the result whether it is possible to consider the created model the digital double will be summed up. 
Results of an experiment 
Following the results of work, the model creates three schedules of dependences of Pressure, Temperature of liquid and temperature in gas area of a cube. These data remain in the form of special objects of Simulink library then are in a special way processed and verified (figure B1). Verification of model on experimental data happens in the same graphic coordinates, and it is possible to judge reliability of the digital twin in process of coincidence of data of his work with the schedule of real work of a column. At the same time, because of not ideality of the registering devices it is required to make linearization of basic graphic data for experimental data in the beginning. As a linearization technique for data the way of nonlinear regression of the eighth order (Non-linearregression 8thgrade) as he most precisely reflects average value on an experiment has been chosen. 
 

Figure 3. Mathematical model. 
Schedules of verification of the data obtained from digital model are submitted in figures A1-A3. As it is possible to be convinced result of work of digital model corresponds to real data quite precisely, on average with small deviations to 5%. 
Table 1. Results of quality of modeling. 
Deviations from real data generally appear for the account: 
Insufficient degree of accuracy of model; 
Errors of discrete blocks; 
Insufficient degree of study and popularity of some constants; for example, thermolysis coefficients; 
Lack of interrelations of model with external systems. 
In general, as it is possible to be convinced, despite dot deviations, schedules correspond to experimental data quite precisely. Therefore, the created model corresponds to definition of the digital twin, and requirements imposed to them. 
Conclusions 
 As a result of the done work, the main problems in the chemical industry have been analyzed today and also the existing ways of their decision have been established. As the main solved problem the problem of transition of the chemical industry to digital data has been chosen. As one of the technologies which are directly connected with the solution of this problem the technology of digital doubles is. Development of technology of creation of the last is the main objective pursued during this final qualification work. 
First of all, for the solution of the chosen problem laboratory installation for division of binary aqueous-alcoholic mix for which cube part it was planned to create the digital twin has been chosen. In the beginning rectifying installation has been modified in a necessary way. In the picked-up, most important points of a column control sensors from which data, after the column run, through the operating Arduino payment, got to the computer where were processed in in advance prepared system created on the basis of the software of MatLab have been installed. The processed data remained further for verification of the model created in the future.  
After has been launched installations and necessary data have been obtained, the market of the modern software for digital modeling has been analysed, and, after the analysis the modeling Simulink environment has been chosen. 
 Creation of mathematical model which included all fundamental laws and the principles of work of heating capacity was the following step after software choice. This mathematical model has been transferred further to Wednesday digital modeling of Simulink in which the digital double of the modelled object has been created. These works of the digital double have been verified according to work of the real column equipped with the heater. On the basis of the done work it is possible to draw the following conclusions:  
By results of verification of results of work it is possible to tell that the accuracy of the created digital model is sufficient to be called the digital twin. Moreover, reaction of model to change of external factors very probably reflects reaction of real installation.  
The environment of creation Matlab/Simulink is rather exact and convenient tool with extensive opportunities. Therefore, for creation of models in the future, this environment can be recommended for use. 
The approach to creation of model used in work in general was quite rational and fast. The used equipment, and technology solutions attached to this equipment in general satisfy the accuracy of the digital twin. 
 
 
 
 
Application A 
 
Figure A1. model verification results for cube fluid temperature. 
 
Figure A2. model verification results for cube gas temperature. 
 
Figure A3. Pressure model verification results. Application B 
 
 
Figure B1. Simulink model. 
 
 
 
 
 
 
"
5AQLPFJQ;<pad> the digital twin of a disaster city can provide a vision for convergence of streams of research related to ICT and AI in disaster response and emergency management. the four components of the envisioned Digital Twin are: (1) multi-data sensing for data collection, (2) data integration and analytics, (3) multiactor game-theoretic decision making, and (4) dynamic network analysis. crowdsourcing is not only a way to collect high-quality disaster situational information, but also contribute to processing remote and social sensing data.</s>;"
Disaster City Digital Twin: A vision for integrating artificial and human intelligence for disaster management
Chao Fana,*, Cheng Zhanga, Alex Yahjab, Ali Mostafavia
a Zachry Department of Civil and Environmental Engineering, Texas A&M University, College Station, TX, USA b EpiSys Science Inc, San Diego, CA, USA

A R T I C L E I N F O	A B S T R A C T

Introduction
The objective of this paper is to present a vision for a Disaster City Digital Twin as an integrated paradigm for converging different streams of research related to artificial intelligence in disasters and crisis informatics. With the increased frequency of natural and man-made disasters and crises, the need for disaster responses and humanitarian relief actions is growing rapidly (Jackson et al., 2018). The need for improving the effectiveness and efficiency of disaster management has been widely recognized, especially in the mission and policies of the United Nations (United Nations, 2019). One important area to improve disaster response and emergency management is the employment of information and communication technologies. To this end, various streams of research across different disciplines such as information science, computer science and social science have devoted efforts to developing and employing information and communication technology (ICT) and artificial intelligence (AI) approaches for enhancing disaster response and emergency management processes (Fan & Mostafavi, 2019). For example, researchers in information and social science have focused on disaster coordination networks to improve the efficiency of information spread and communications (Hamadache, SeridiBouchelaghem, & Farah, 2016; Pearson, Pearce, & Kingham, 2013). In addition, AI techniques including natural language processing (e.g., event extraction, named entity recognition, and speech recognition) and image processing (e.g., image segmentation and denoising) have been utilized for leveraging intelligence learned by machines from disaster situational data (Nweke, Teh, Mujtaba, & Al-garadi, 2019). Researchers from computer science primarily focus on improving the accuracy and capabilities of AI techniques to capture the dynamic situations in disasters (Nam & Pardo, 2011; Pohl, Bouchachia, & Hellwagner, 2012). For example, the Center for Integrated Emergency Management (CIEM) in Norway has carried out research to develop AI and ICT techniques such as information sharing and crowdsourcing tools for crisis responders. For instance, the CIEM proposed an agenda for design of disaster-specific ICT to enhance information sharing (Gjøsæter, Radianti, & Chen, 2018). Specifically, the agenda includes data reliability examination (Lazreg et al., 2018), data analytics for extracting situational information (Sharma, Granmo, & Goodwin, 2018), and information sharing and communication (Alsalamah et al., 2018).
Given the growing literature in the interdisciplinary field of disaster/crisis information processing and AI in disaster management,
there is a dire need for interdisciplinary convergence towards a unified vision. Recognizing this gap, this paper aims to provide a vision for Disaster City Digital Twin as a unifying paradigm for interdisciplinary convergence. Digital twin is an emerging paradigm which aims to build a digital version of the physical and human environment with the situational information (Mohammadi & Taylor, 2017). The digital twin paradigm has been envisioned in the management and operation of smart cities (Min, Lu, Liu, Su, & Wang, 2019; Neirotti, De Marco, Cagliano, Mangano, & Scorrano, 2014). The disaster context is a special scenario in smart cities, in which the situation evolves rapidly and becomes complex. A digital twin paradigm in a city affected by disasters could provide numerous benefits for enhanced situation assessment, decision making, coordination, and resource allocation. In a Disaster City Digital Twin, spatiotemporal dynamics of disaster regions and humanitarian actions are integrated into an analytics platform fusing datasets from crowdsourcing tools and agencies. Through fusion, learning, and exchange of spatiotemporal information with various relief actors (enabled through data integration and visualization) and the virtual coordination, the digital twin of a disaster city can provide a vision for convergence of various streams of research related to ICT and AI in disaster response and emergency management.
Integrating the ICT and AI techniques from research conducted across different disciplines into a digital twin paradigm would require four main components in a Disaster City Digital Twin. The four components of the proposed Digital Twin paradigm are: (1) multi-data sensing for data collection, (2) data integration and analytics, (3) multiactor game-theoretic decision making, and (4) dynamic network analysis (see Fig. 1). The first component focuses on AI technologies and methods for situational data collection from multiple sources in disasters and humanitarian emergencies. Disaster situations and humanitarian crises are often characterized as data-starved due to constraint resources for data gathering and analysis. However, the advances in AI has brought opportunities to address this challenge and gather, store, and analyze various types of data related to a disaster city. In particular, AI-enabled remote sensing, social sensing and crowdsourcing technologies are discussed as important elements of the Digital Twin for near real-time gathering and analysis of disaster and crisis situations. The second component of the Digital Twin is geared towards the employment of AI in integration of heterogeneous data in order to draw important insights needed by responders and relief actors. Specifically, we discuss the challenges of dealing with different types of information (e.g., social media posts, volunteer and crowdsourced data, aerial photos, maps, reports, and news articles) and AI solutions (such as knowledge graph and network embedding) to implement machine learning on heterogeneous data to inform disaster management and relief actions. The third component of the Digital Twin includes a Serious Game Learning Environment to enable multi-actor decision making and networked coordination. In this component, we examine the employment of AI for improving disaster response training and network-centric coordination based on approaches for multi-actor gaming scenarios. The fourth component of the envisioned Digital Twin involves dynamic network analysis capturing the interactions among various types of networks such as actors and information networks (e.g., who coordinates with who; who does what relief tasks; and who needs what information) for performance assessment of disaster management and humanitarian actions. In this component of the Digital Twin, we discuss the employment of AI to examine missing links in the network and to record temporal information in order to improve the efficiency of disaster management efforts.
Through collection, analytics, training, and exchange of situational information with the humanitarian actors (enabled through data integration and visualization) and the virtual coordination (enabled by serious gaming and dynamic network analysis), the digital twin of a disaster city and its human users become smarter over time and gain


Fig. 1. Overview of the Disaster City Digital Twin Paradigm.

Fig. 2. Multi-data sensing for data collection.

predictive insights into the planning and response operations in humanitarian actions. Here, humanitarian and emergency response actors are broadly defined as robot-human teaming. The following sections present each of the four components of the Disaster City Digital Twin paradigm. For each component, we discuss clusters of recently published papers and the-state-of-the-art techniques that can be integrated into this Digital Twin paradigm and discuss the opportunities for effective employment of AI towards the realization of a vision for the Disaster City Digital Twin.
Multi-data sensing for data collection
The information for the Disaster City Digital Twin paradigm is mainly obtained from three sources: remote sensing, social sensing, and crowdsourced data collection (see Fig. 2). Remote sensing includes satellite and UAV-based aerial imagery (Akter & Wamba, 2017), which is primarily used for capturing the anomaly and changes of natural and built environment before, during, and after disasters. Satellite imagery can provide multiple types of information for disaster management and response. For example, for tropical cyclone disasters (i.e. hurricanes and Typhoons), satellite imagery can produce data about land use, land cover, storm surge height, and cyclone wind speed. UAV-based aerial imagery and video captured via unmanned aerial vehicles (UAVs) are playing an increasingly important role in disaster response, due to its efficiency for rapid mapping (Gomez & Purdie, 2016). In addition, aerial imagery can produce detailed 3D point clouds of the environment and infrastructure in two ways. First, a CCD camera on UAVs can generate mutually overlapped 2D images (or video frames) to be transferred to 3D point clouds through Structure from Motion (SfM) algorithm (Westoby, Brasington, Glasser, Hambrey, & Reynolds, 2012). Second, some UAVs carry Light Detection and Ranging (LiDAR) sensors can directly produce 3D point clouds (Hoque, Phinn, & Roelfsema, 2017). Another advantage of UAV-based aerial imagery over satellite images or piloted aerial imagery is that they can deploy efficiently in disaster environments with minimum supporting infrastructure facilities (e.g. satellites and airports) (Yu et al., 2018). Therefore, UAVs are now an emerging technology to inspect the impacts of disasters, such as small-scale changes and cracks of buildings infrastructures due to earthquakes, hurricanes, or landslides (Haworth & Bruce, 2015).
Different artificial intelligence techniques can contribute to improving the deployment of remote sensing in disasters (Walker, 2016). In the Digital Twin Paradigm, images and point cloud models obtained from remote sensing could be automatically analyzed through different AI techniques. First, image segmentation can divide remote sensing images into regions according to the semantic content (e.g. urban areas, farm, forest, etc.), which is critical in acquiring land use and land cover information. Second, object detection and tracking can be used to identify infrastructure components (e.g. bridges, road networks) to improve the efficiency of assessing the damages to critical infrastructure. Change detection can identify the differences between two remote-sensing images that were spatially registered, which is a convent way to detect deformation of nature or built environment due to floods, droughts, or earthquakes (Cooner, Shao, & Campbell, 2016).
Social sensing based on Natural Language Processing (NLP) and data mining techniques is another emerging methodology to extract and analyze social media information for evaluating disaster phenomena (Atefeh & Khreich, 2013). Social media provide rich information source about disaster situations. Data analytics on publicly-available social media contents can support: 1) detecting or predicting critical events (e.g., power outage or need for shelter); and 2) identifying underlying patterns of social media users to understand human behaviors in disasters (Arthur, Boulton, Shotton, & Williams, 2018). Compared to traditional data collection approaches such as survey and field interviews, social sensing has several significant advantages (Arthur et al., 2018; Zhang, Fan, Yao, Hu, & Mostafavi, 2019). First, social media data analytics mainly rely on the content generated by the public instead of disaster responders, which do not put a burden on the human resources for disaster management. Second, social media analytics can provide near real-time transitions of disaster situations, which can potentially improve the decision-making process for efficient and effective disaster response.
Despite the usage and benefits of social sensing for disaster response, the potential of social sensing for situation awareness in disasters has not been fully achieved. For example, posts on social media platforms contain rich content about infrastructure disruptions (e.g., power outage or contaminated water) as well as people’s reaction, adjustment, and needs related to the service disruptions (Castillo, 2016). Therefore, analyzing the spatiotemporal patterns and correlation between infrastructure and societal impacts based on social media posts can provide valuable insights for infrastructure agencies and researchers to understand the interactions between human and physical environment and develop planning and response strategies. In addition, existing social sensing approaches suffer from a weakness related to fake information. Achieving real-time detection and elimination of fake information on social media requires further achievements in many areas about AI in social sensing, such as rumor dissemination pattern, characteristics of fake posts, and recognition of rumor-debunking behaviors.
Crowdsourcing is another trending data collection and analysis method with cost-effectiveness and time-efficiency during disasters. AI techniques can potentially improve the performance of crowdsourcing techniques in disaster response and emergency management by providing high quality and structured data for training and testing machine learning algorithms. Crowdsourcing refers to a family of data collection and/or decision-making approaches based on aggregating the thoughts of agents for higher decision quality than that based on the information provided by individuals (Arganda-Carreras et al., 2015). In the domain of disaster management, crowdsourcing approaches generally have two roles: collecting data and processing data (Ofli et al., 2016). Crowdsourcing for data collection allows people to proactively report firsthand, real-time information about disaster situations to online platforms. Crowdsourcing for data processing refers to approaches in which people implement human-easy and computer-difficult tasks (e.g. labeling images, adding coordinates, tagging reports with categories, etc.) to generate structured, high quality, interpreted data for decision making or machine learning. One typical application of crowdsourcing for data processing is segmenting and labeling damaged buildings in remote sensing imageries and social media texts. Hence, crowdsourcing is not only a way to collect high-quality disaster situational information, but also contribute to processing remote and social sensing data by providing labeled images and social media posts for training machinelearning models in the Digital Twin paradigm.
The growing use of crowdsourcing technologies could also improve the performance of machine learning algorithms (e.g., Human-in-theloop (HITL) machine learning) for classifying data and extracting insights in the Digital Twin. HITL machine learning could improve the effectiveness and efficiency of machine learning algorithms by combining human and machine intelligence. In HITL machine learning, human interactions are involved in both training and testing stages to reduce an exponential search space (Holzinger et al., 2016) and improve learning accuracy (Xin et al., 2018). In the training stage, humans provide ground-truth labels for the training data in order to make accurate predictions; and in the testing stage, humans can give quick feedback to correct the results where the algorithms are not confident about a judgment. Hence, HITL machine learning algorithms can improve the performance of different components of the Disaster City Digital Twin paradigm. For example, disaster situation information (e.g., texts, images, and videos) from social media and crowdsourced platforms can be labeled (e.g., determine the level of building damages and the needs of relief actions) by human volunteers in preparing the training set and correct training errors to improve deep learning algorithms used for damage classification.
Hence, remote sensing, social sensing and crowdsourcing can provide multimodal information related to the natural and built environment, infrastructure disruptions, life-threatening emergencies, and impact on social dimensions. This information is pivotal in mapping and visualizing evolving stations in the Disaster City Digital Twin paradigm. Remote sensing provides imagery information about the natural and built environment for detecting information disruption via AI-based computer vision algorithms. Social sensing, on the other hand, will provide information about life-threatening emergencies witnessed by people and societal considerations of the public during the disasters, such as the impact on social dimensions due to infrastructure disruptions. Finally, crowdsourcing techniques enable collecting complementary disaster situational information that is difficult to capture by remote of social sensing in a targeted manner. Furthermore, crowdsourcing can integrate human knowledge into AI approaches by generating training datasets of labeled remote sensing imageries or social media posts. Together, remote sensing, social sensing, and crowdsourcing information processed via the state-of-the-art AI approaches can support the efficiency and effectiveness of data collection and processing for the Disaster City Digital Twin paradigm.
Data integration and analytics
Being aware of the dynamic situation in disasters is significantly important for effective disaster preparedness, response and recovery (Fan & Mostafavi, 2018; Hashem et al., 2016). One of the key components in understanding the disaster situation is the exploitation of situational information in disasters (Fan, Mostafavi, Gupta, & Zhang, 2018). As mentioned in the previous section, available techniques for gathering information from different types of detectors (a.k.a. “modality”) have increased over the past decade (Dalla Mura et al., 2015). The complex nature of disaster situations is difficult to be characterized by information from a single modality, and hence, requires integration of multiple modalities to provide complete knowledge about disaster situations and phenomena (Lahat, Jutten, Lahat, Lahat, & Jutten, 2015). However, the increased availability of multimodal data raises challenges in data integration and analytics due to the lack of theoretical and algorithmic strategies in representation of heterogeneous data (Nweke et al., 2019). The primary issues related to dealing with heterogeneous data in disasters are twofold: first, data generated from various sources: social media, remote sensors, and other sources are stored in multiple different data structures (von Lubitz, Beakley, & Patricelli, 2008); second, different types of data exist in a single modality, which increases the complexity of data characterization (Lahat et al., 2015). For example, social media as an emerging modality includes texts, images, videos, geolocations, and hyperlinks describing their emotions, activities or built environment disruptions during disasters (Alam, Imran, & Ofli, 2017; Alam, Ofli, & Imran, 2018). In addition, remote sensing technologies such as synthetic aperture radar (SAR) and LiDAR can acquire highly precise thermal images and large periodic numeric data for condition monitoring and damage assessment (Dalla Mura et al., 2015). The heterogeneity of data formats, timeframes, and semantics makes it very challenging to efficiently and accurately understand the spatiotemporal fluctuations in disasters and humanitarian actions in the Digital Twin paradigm.
To deal with the challenges arising from multimodal data, existing studies have attempted to develop new models and algorithms to integrate multi-source data and then extract insights from the data content for the optimization of multisensory systems, understanding of biomedical functionality, and environment observations (see Fig. 3) (de Albuquerque, Herfort, Brenning, & Zipf, 2015). One of the proposed strategies for data integration is constructing a knowledge graph in which entities are represented as nodes and relationships between entities are captured as edges; both the nodes and edges in a knowledge graph are high-dimensional (Paulheim, 2015). Knowledge graph is based on Resource Description Framework (RDF) which has an < Subject, Predicate, Object > model, where Subject and Object represent entities and Predicate represents relationships (Imran, Castillo, Diaz, & Vieweg, 2014). In particular, for disaster conditions, the key idea for a unified knowledge graph is to define the central entities in disaster cities and then project the entities in the high-dimensional space. Well-defined entities enable filtering relevant information from multimodal data and preparing precise representation of the information in knowledge graphs.
In addition, modalities are inherently interrelated and could provide complementary information since they may describe the same entities through different features. Thus, multiple modalities can potentially help us achieve better representation, in the sense that each modality brings to the whole some types of added value that cannot be deduced or obtained from any of the other modalities (Lahat et al., 2015). Lack of consideration of the relationships among modalities may induce information redundancy or loss. Hence, identifying and assessing the complementarity (or non-linear relationships) among multimodal data is important to obtaining complete knowledge and reduce information redundancy or loss. However, little of the existing work has examined the complementarity in multimodal data, especially in the context of disasters. Therefore, one important area for further integration of AI for data fusion in the knowledge graph is to measure similarities of the information, distinguish complementary components, and learn projection functions for data representation through different feature spaces (Bengio, Courville, & Vincent, 2013). The multimodal information without redundancy would enhance the accuracy and efficiency of situation assessment in the Disaster City Digital Twin.
In addition, knowledge graphs, as heterogeneous information networks (HINs), can be used as input to various analyses in the Digital Twin paradigm to understand the relationships among different entities. This capability may hold the key to inference and prediction of the dynamic fluctuations and situation in disasters, and further lead to an improvement of disaster response and community resilience. In particular, to leverage inference and learning capabilities of deep learning (DL) algorithms, there is a need to transform the graph or network structures into a low-dimensional vector space. Note that recent work (Schlichtkrull et al., 2018) has made progress on graph neural networks but currently the most effective DL algorithms are based on vector input. Network embedding, as a representation approach, can transform the vertices of knowledge graphs and HINs into low-dimensional (embedded space) vectors, such that both geometrical properties of the networks and attribute information about the nodes can be preserved in the vectors (Huang, Li, & Hu, 2017). Machine learning algorithms then operate in the embedded space (Ying et al., 2018). With this use of embedded space of HINs, the Disaster City Digital Twin can have knowledge-rich deep learning to enhance the assistive abilities to:
Process and understand human speech;
Process and classify sounds (natural and man-made);
Classify the content of images, sensor/IoT data, and remote-sensing data;
Generate dense captions for video streams, identifying what activities are in the video;
Predict future sequences of events based on past event sequences;
Translate human speech and written languages;
Generate action sequences based on observed states toward a goal in an optimal manner; and
Imagine things unseen and future scenarios.
The existing methodologies for solving and implementing network embedding, including hierarchical graph aggregation and graph neural networks, offer a powerful paradigm for node classification, visualization and relation inference in HINs. Furthermore, representation vectors can generate 2D visualizations of graphs, discover hidden structures, classify entities involved, and identify different roles of different entity classes. The latent relationships can better model and depict the interdependencies and interactions among different entities such as critical infrastructure and humanitarian response organizations in the Digital Twin paradigm.
Multi-actor game-theoretic decision making
In addition to supporting situation assessment, another main function of the Disaster City Digital Twin is improving networked coordination among heterogeneous actors involved in disaster response and emergency management. Effective and efficient disaster management and relief operations not only rely on the precise situational information, but also require networked coordination and decision making among different relief actors. To enhance collaborative decision making, training relief actors with simulated disaster situations and learning their behaviors for policy and guideline making are needed. This component of the Disaster City Digital Twin aims to enable conducting interactive virtual simulations to identify and examine coordination strategies among different actors involved in disaster response. Accordingly, the Disaster Response Digital Twin can achieve multi-actor training for collaborative operation and decision making in different disaster scenarios.
Serious Gaming Learning and Coordination Environment (SGLCE) would be an important part of the Digital Twin to allow decision-aware serious gaming, coordination, and visualization of disaster response actions. Serious Game is the use of scientific game-engine simulations of real-world phenomena to solve a problem. In SGLCE, actors and decision-makers can engage in decision-making scenarios and collaborate with other actors and decision-makers. Actors can access data about situations, ask what-if queries, analyze scenarios, visualize scenarios, and go through a vigorous and open process of deciding (Roy et al., 2010). What makes SGLCE an important element in the Digital Twin is its ability to simulate the coordination, information flow, and dynamic interactions in disaster response and emergency management. In addition, the visualization component of SGLCE includes the integration of spatiotemporal data and game visuals onto the real world with GPS coordinates and GIS capabilities. GIS-aware agents associated with humanitarian actors can be modeled with a polygonal boundary corresponding to their area of authority/operations.
The decisions of an actor, made to achieve a specific task, can benefit or punish the actor and influence their neighbors (Jackson & Zenou, 2015). For example, an actor may need to decide on an ensemble of tasks that will produce an outcome in the future with the presence of uncertainties. Given the tasks to arrive at the outcome, in the digital twin, these tasks can be formulated as a graph of tasks and dependencies culminating in the attainment of the outcome (Sohn, Oh, & Lee, 2018). Fig. 4 shows an illustration of the task network for disaster management. The decision-maker (actor) needs to find the optimal path to, say, remove debris (I). The task graph (as shown) denotes which tasks depend on subtasks (e.g., Task D depends on Task A and Task B) and the resource competition (e.g., Task D competes with Task E to get trucks of Task B). Then, dynamic programming, deep learning and other techniques (e.g., hierarchical reinforcement learning (Nachum, Gu, Lee, & Levine, 2018) and risk analysis (De La Maza, Davis, Gonzalez, & Azevedo, 2019)) can be applied to find optimal task sequences in the task graph. The analysis of potential paths elucidates the trade-off between exploitation and exploration, and between shortterm and long-term reward pursuits. The task graph itself can be learned from empirical data. This is important in the fluid situation where tasks change dynamically. The envisioned Digital Twin will include intelligent task graph generator from HIN and task (networked) sequence optimizer for disaster management that are more flexible and smarter than standard operating procedure currently in use (Abdeyazdan, Parsa, & Rahmani, 2013).
Decision making in the real world, however, needs to take into account other actors. There may be not just two opposing actors, but many diverse actors/stakeholders with complementary goals as well. This illustration is for a single actor, but the formalism straightforwardly extends to multiple actors in both collaboration and competitive (game) environment. This calls for gamified and game-theoretic approach, for which the Digital Twin is envisioned to include deep learning techniques similar to DeepMind’s pioneering line of winning automated game players such as AlphaGo and Alpha Zero (Wang et al., 2016). However, in the Disaster City Digital twin, the challenge is going from perfect-information board games such as Go to imperfect-information, partially observable, continuous real-time, and irreversible consequential world in disaster cities. In the Digital Twin paradigm, there are a multitude of actors who have different interests and knowledge and they all make their decisions either alone or in coalition. Thus, the outcome is based on a confounded multitude of factors and uncertainties. Alpha Zero with its policy and value network and Monte-Carlo tree search (MCTS) provides a way to model a decisionmaking process of an actor in the face of adversaries. The policy and value network learn from self-play experiences, while MCTS explores options. The game-theoretic approach can have more than two actors and have schedules for actors to make and execute their decisions, with mixed and hidden human and AI players. The Digital Twin endows game experiments with a gamified scientific visualization, e.g., using Unreal Engine game engine. The visual aspects are essential in the Digital Twin to enable intensive engagement among algorithms and human actors. Integrating these techniques to record, analyze and evaluate the multi-actor interactions in the Disaster City Digital Twin enhances the capability of this system and its human users.
Dynamic network analysis
The Disaster City Digital Twin enables increasing the visibility of network dynamics among heterogeneous relief actors, tasks, information, infrastructure and resources in disasters to better understand and enhance the efficiency of response and emergency management. The efficiency of disaster response and emergency management processes is critical to reducing the adverse impacts of disasters on communities. Capturing and addressing inefficiencies such as un-optimized resource distribution and limited information sharing, is essential to improving the performance of humanitarian actions and disaster management. Existing studies have shown that inefficiencies tend to arise from dynamic and complex relationships among different entities and the dynamic variation of their statuses (e.g., emergence or disappearance of new relationships) in disasters (Fan & Mostafavi, 2019). Besides the coordination among multiple relief actors (discussed in Section 4), there are fragmentations (such as the capability of actors to get situational information, the availability of resources for relief tasks, and the functionality of infrastructure for task execution) that can lead to inefficiencies in disaster response and management processes. Hence, an important function of the Digital Twin is to increase the visibility of complex relationships among the entities to improve performance assessment in emergency response.
Existing studies have highlighted the merits of network approaches in analyzing heterogeneous entities and complex relationships (Mcculloh, Lospinoso, & Carley, 2010; Shiau, Dwivedi, & Yang, 2017). Meta-network analysis approach is one of the emerging network approaches, which can abstract various types of entities (e.g., relief actors, tasks, information, and resources) and their interactions. The utility of meta-network approach in analyzing the coordination and communication among various types of entities in disasters have been examined and demonstrated in existing studies (Fan, Zhang et al., 2018; Zhu & Mostafavi, 2018). Meta-networks can include, but are not limited to social networks (who knows who), knowledge networks (who knows what), tasks networks (who does what), and needs networks (what knowledge is needed to do a task) (see Fig. 5) (Altman, Carley, & Reminga, 2017). The Digital Twin with meta-network analysis component could enable evaluating the interactions among different entities, identifying inefficiencies induced by their complex relationships, and providing solutions to enhance the performance of disaster management processes (Misra, Goswami, Mondal, & Jana, 2017). In addition, temporal information recorded in the Disaster City Digital Twin enables monitoring, analyzing and predicting the dynamic structures of the networks and the potential effects on the efficiency of relief and response actions.
Integrating AI methods into dynamic meta-network analysis can improve the capabilities of the Disaster City Digital Twin to enhance the
Fig. 5. An illustration of Disaster City Meta-Network (A) and its Dynamic Process (B). T1, T2, and T3 represent timestamps.
performance of disaster management. One important capability in this component of the Digital Twin is examining the criticality of entities and the influence of the entities’ statuses on the functioning of other entities in the network. For example, removal of certain entities such as relief actors may cause some other entities inactive (e.g., some tasks cannot be conducted, or some information may be blocked) in the network. Bayesian network analysis methods can learn such effects from the interactions among entities using empirical data, and further infer the status of the nodes at following timestamps based on the changes of other entities (Plomaritis, Costas, & Ferreira, 2017). By doing so, the Digital Twin can provide a deep understanding of the importance of the entities and develop strategies to strengthen the capacities of the critical entities to enhance the overall performance of disaster response and emergency management processes.
Another capability of this component in the Digital Twin is predicting the latent connections and interactions among different entities to enhance the synergetic operation and management. It is often the case that different relief actors work on their relief tasks in isolation. This may raise inefficiencies such as increased costs for resources transport and incomplete information about the disaster situation. Capturing the latent connections among the entities based on their shared resources, similar tasks, and empirical experiences and knowledge can enable the collaboration and cooperation among these entities to enhance the performance of disaster relief (Hu, Wang, Peng, Liang, & Du, 2017). Techniques such as common neighbors detection, preferential attachment, hierarchical structural decomposition have been proposed to predict the connections among entities in a network (Dong et al., 2018). The identification of the potential connections would enable the Disaster City Digital Twin to capture opportunities for cooperation among the entities and improve their performance through synergetic operations.
Discussion and recommendations for future research
In this paper, different streams of research related to employing AI in disaster management have been discussed to establish an integrative vision for the Disaster City Digital Twin paradigm. The proposed Disaster City Digital Twin vision offers important contributions and implications for research and practice of AI and city management in disasters to inform future research.
Table 1 summarizes the state of the art, gaps and opportunities, and functions that AI techniques can play in the Disaster City Digital Twin paradigm. To achieve the vision for the Disaster City Digital Twin paradigm, there is a real need for integration of different streams of research to create AI-based converging solutions related to each component. The key functions of each components of the Disaster City Digital Twin paradigm could be summarized as follows:

The first component of the digital twin paradigm mainly focuses on collecting and visualizing different types of data using AI techniques in disasters (Stieglitz, Mirbabaie, Ross, & Neuberger, 2018). Despite the advances in employing AI to rapidly map and estimate disaster impacts (Gao, Barbier, & Goolsby, 2011) based on textual (Ragini, Anand, & Bhaskar, 2018) and imagery information (ArgandaCarreras et al., 2015), the quality and resolution of the information still require improvement.
The second component of the digital twin leverages the capabilities of data analytics (i.e., knowledge graph (Paulheim, 2015) and network embedding (Huang, Li, & Hu, 2017)) for fusing multimodal data to provide a comprehensive understanding of disaster situations (Lazreg et al., 2018). To overcome existing challenges such as information redundancy, new methods are needed to determine effective projection functions.
The third component of the digital twin is geared towards conducting AI-based multi-actor and game-theoretic simulations to enable engaging multiple relief actors for collective decision making (Dwivedi et al., 2019). Learning and modeling the decision making processes of human actors in the digital twin requires the development of task graphs (Abdeyazdan et al., 2013), as well as MXNet deep learning frameworks (Chen et al., 2015).
The last component of the digital twin focuses on analyzing the performance of disaster management using network learning and prediction techniques (Fan & Mostafavi, 2019). The spatiotemporal fluctuations in the cyber-human-physical systems of cities in disasters can be represented with multiplex dynamic models and be subsequently analyzed to provide timely assessment of the performance of disaster management processes and systems through various network measure and missing link identification techniques (Ben Yahia, Eljaoued, Bellamine Ben Saoud, & Colomo-Palacios, 2019).
6.1. Theoretical contributions and implications
The vision presented in this paper contributes to a theoretical understanding of the convergence of various streams of disaster informatics and AI research and the advances in the interdisciplinary field. In particular, the proposed vision for digital twin contributes to establishing a common vision for interdisciplinary researchers across various fields. The gaps identified related to each component of the Disaster City Digital Twin paradigm could inform “core AI” research about techniques and methods required for smart and intelligent disaster Management. In addition, by providing a converging vision and a shared mental model for researchers from different fields (e.g., information sciences, civil engineering, social sciences, computer and data sciences), the Disaster City Digital Twin could expedite use-inspired AI solutions and innovations in the field of disaster informatics. 6.2. Implications for practice
From a practical perspective, the proposed Disaster City Digital Twin paradigm could provide important capabilities for real-time monitoring, data analytics, and scenario simulation. First, the real-time monitoring capability of the digital twin is achieved through collection of situational data and examination of the spatiotemporal fluctuations related to disaster events such as community disruptions and damages (Fan, Jiang, & Mostafavi, 2020). With automated techniques and tools for continuous data gathering through remote and social sensing, the digital twin enables visualizing human activities, damages, and relief needs across time and space. Accordingly, the public officials, victims, first responders, and volunteers can capture the changes of the situation by tracking the temporal information in disaster-affected areas. Second, the data analytics capability in the digital twin could enable better optimization of disaster management and emergency response operations among various actors. In particular, since the Disaster City Digital Twin paradigm compiles, fuses, and analyzes the data in an integrative and consistent way, the flow of information among different components of the disaster management systems and processes can be significantly enhanced (Ismagilova, Hughes, Dwivedi, & Raman, 2019). This capability would enable a better collaboration and cooperation among different departments and agencies in response to disasters through learning and exchanges of situational information. Finally, the digital twin enables the scenario-play and simulation capabilities for training and planning purposes and to improve cooperation among various and fair allocation of resources. By doing so, the digital twin of a disaster city and its human users could become smarter and more resilient to extreme events over time (Hashem et al., 2016).
As different streams of research converge, the Disaster City Digital Twin can assist the disaster management and emergency response processes. For example, when a hurricane lands in a city, heavy rainfall would result in flooding over metropolitan districts. Roads would be congested or closed, and a great number of residents would lose power and water supplies. Satellites and drones are used to collect image data. Crowdsourcing platforms and social media are employed to gather information about the needs of affected people and the damages in the affected areas. The Digital Twin then aggregates all types of data from these data sources and cleans the data through rumor detection and noise removal using HITL machine learning approaches. Built upon the cleaned situational data, a knowledge graph with entities involved in disasters would be created in the Digital Twin to accurately capture the disaster situation as events unfold. Relief actors have been trained in simulated scenarios with similar knowledge graphs so that they can rapidly coordinate with each other to develop the strategies and conduct relief tasks. Through the interactions among different entities recorded, the Digital Twin would identify the entities with a high demand of capacities and develop improvement strategies in time to enhance the performance of the critical entities as well as the efficiency of the whole network. As envisioned in this simple use case, the Disaster City Digital Twin will augment the capability of communities to cope with future disasters. This vision could only be realized through a convergence of interdisciplinary research streams related to ICT and AI in disaster response and emergency management (Duan, Edwards, & Dwivedi, 2019).
6.3. Limitations and future research direction
The capabilities specified for each component of the Disaster City Digital Twin paradigm could not be fully achieved without further advancements in AI techniques. There are important limitations that need to be addressed in future research to bridge the gaps among different streams of studies for enhancing disaster management processes through integrative ICT and AI techniques. First, as the massive situational data is generated in disasters, the data sensing and collection component enable rapid collection of a variety of data using satellite, crowdsourcing platforms and social media. However, these data tend to have noises, rumors, and false information, which impede the extraction of insightful information for the awareness of disaster situations (Stieglitz, Mirbabaie, Ross, & Neuberger, 2018). The capabilities of this paradigm would be sensitive to the conditions that the situational data is doubtable. Integrating machine learning and crowdsourcing techniques to empower the Digital Twin with the robust capability of data labeling and rumor detection is essential to achieve this capability in the Digital Twin. Second, the component of data integration in the Digital Twin enables integrating the data from the first component to create knowledge graphs for a better understanding of disaster situations. However, the projection function in existing AI methods to reduce the dimensionality and redundancy of multimodal data is still in its infancy. Hence, developing effective projection functions to improve data integration and analytics is needed in future studies. The third component of the Digital Twin focuses on the coordination of actors for decision-making processes in disasters. Despite the advances of AI

C. Fan, et al.
techniques such as AlphaZero and Monte-Carlo tree search, modeling and analyzing the decision-making processes of relief actors remain challenging (Dwivedi et al., 2019). Future research can develop automated multi-agent models with deep learning capability to better learn and characterize actors’ interactions in emergency responses. Finally, the Disaster City Digital Twin enables the examination of the dynamic networks of actors, tasks, information, resources, and infrastructure to improve the efficiency of disaster management and emergency response. Integrating AI methods such as Bayesian networks can lead to learning the influence of an entity on its connected entities and quantify their interactions to infer the synergetic operations. To improve this function of the Digital Twin, future studies can improve methods for prediction of dynamic network processes in disaster response and emergency response processes. Through addressing these challenges and opportunities, the adoption of the Disaster City Digital twin as the unifying paradigm can enable integrating various AI and ICT research streams related to disaster response and management.
Concluding remarks
This paper provides a vision for a Digital Twin paradigm to enable interdisciplinary convergence in the field of ICT and AI for disaster response and emergency management. The proposed paradigm is composed of four components: multi-data sensing for data collection, data integration and analytics, multi-actor game-theoretic decision making, and dynamic network analysis. The convergence of interdisciplinary research streams holds a strong promise for enhancing the performance of disaster management processes.
CRediT authorship contribution statement
Chao Fan: Conceptualization, Methodology, Visualization, Writing - original draft, Writing - review & editing. Cheng Zhang: Methodology, Writing - original draft. Alex Yahja: Methodology, Writing - original draft. Ali Mostafavi: Conceptualization, Supervision, Writing - original draft, Writing - review & editing.
Acknowledgments
This material is based in part upon work supported by the National Science Foundation under Grant Number IIS-1759537. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
"
897TQYWA;<pad> the aim of this paper is to identify and analyse the sources of risk associated with building and implementation of a digital twin in the marine industry. learning from the aviation industry where the digital twin concept has been well established. digital twin makes real-time updates accessible by all users, enables virtual system integration and test, reduce development costs and the risk of design problems.</s>;"Journal of Physics: Conference Series
PAPER • OPEN ACCESS
On Risk of Digital Twin Implementation in Marine Industry: Learning from Aviation Industry
To cite this article: Michaela Ibrion et al 2019 J. Phys.: Conf. Ser. 1357 012009
 
View the  for updates and enhancements.

This content was downloaded from IP address 131.188.6.12 on 16/01/2020 at 18:45

On Risk of Digital Twin Implementation in Marine
Industry: Learning from Aviation Industry
Michaela Ibrion1, Nicola Paltrinieri1, Amir R. Nejad2
1 Mechanical and Industrial Engineering Department, Norwegian University of Science & Technology, NO-7491, Trondheim, Norway
2
Marine Technology Department, Norwegian University of Science & Technology, NO-7491,
Trondheim, Norway
E-mail: Michaela.Ibrion@ntnu.no
Abstract. This paper presents some aspects of the risk and challenges associated with digital twin implementation in the marine industry by learning from the aviation industry where the digital twin is more widely employed. The digital twin applications in aviation and marine industries are presented and the main steps of developing a digital twin are discussed. The three main steps of sensors (measurements), model, and data analysis are identified and used in the study. The lessons from two recent accidents in the aviation industry (Boeing 737 MAX crashes in Indonesia and Ethiopia in 2018 and 2019) are studied in details and discussed. It was found that the sensor reliability, model failure and wrong decisions as the output of the data processing are among the risks associated with digital twin implementations. In particular, from the case study accidents, it was found that the digital twin may not be able to represent all the possible scenarios which a system may experience in its life time. The digital twin presents many advantages, however the implementation of the digital twin is associated with risk and high uncertainties, even in the industries like the aviation industry, where the digital twin is well established and at a higher advanced level than in the marine industry.
Introduction
Digitalization is becoming an integral part of the engineering arena, and an interesting technology which aspires to move industries forward is the digital twin. In terms of historical roots of the digital twin term, it seems that Michael Grieves from the University of Michigan was the first to write and use the digital twin term, back in 2002. Nevertheless, the National Aeronautics and Space Administration (NASA) was using the term and applied the concept of “pairing technology” which is a predecessor of the digital twin technology, since the beginning of space explorations [1].
Nowadays, the digital twin is appreciated to become a key for digital transformations of many industries. The digital twin becomes so increasingly popular, as it is seen to drive many innovative solutions and to increase performance and profitability.
With regards to the aviation industry, Dennis Muilenburg, the Boeing CEO, mentioned in his speech at the panel discussion in Morgan Stanley Laguna Conference – California, on 12 September 2018 – that the digital twins usage for component production has increased the quality of parts and systems used in aircraft production by 40 percentage. Muilenburg highlighted that because of digital twin usage “We are seeing things like 40 to 50 percentage improvements in first time quality”. Boeing is looking further to reduce development costs and to create new alternatives for supply chains. Digital twin makes real-time updates accessible by all users, enables virtual system integration and test, reduce development costs and the risk of design problems. As per Muilenburg “That digital life cycle – think of it as a digital twin of our airplanes – will unleash incredible value in the future” [2]. Furthermore, Canaday [3] emphasized about the increasing role of the digital twin in the aircraft health monitoring. The digital twin is estimated that will improve the maintenance and can bring an estimated 30 percentage improvement in cycle time for critical processes. According to Domone [4], by using a digital twin, engineers can more accurately determine the life of structural components for aircraft. Digital twin contribute to improve performance, to generate savings and to increase operational flexibility [4].
In terms of investments to the digital twin’s implementation, Domone [4] advised that the digital twin can definitely offer improvements for the civil aircraft fleet life management, but an important consideration in usage of digital twin is given by the cost-to-benefit ratio. A low– fidelity digital twin approach which incorporates measures operational weight for civil aircraft operations is associated with relatively low cost and has the potential to extend life for asset. On the other hand, a high–fidelity geometric approach is of a high interest, but it is not providing much benefits, as it is associated with high–costs. A low fidelity approach digital twin is bringing a good value with reference to cost versus benefits, as it contributes to optimize operations and fleet usage. This type of digital twin is already in–use by engine manufacturers in civil aerospace. A high-fidelity digital twin can add knowledge about detailed component geometry, mechanical assemblies, system calibration details and can updates these through the collected data over the asset’s life. A high-fidelity digital twin has the potential to increase component fatigue life, but with present technology and cost, it requires huge amount of data and particularly, high investment costs [4].
Digital twin is seen as a definitive technological trend in the marine industry. Primarily, the digital twin offers the contribution that an asset is monitored in operational conditions and the data is digitally represented in virtually real-time [5]. Moreover, there are many more to be offered by digital twin; the digital twin allows to perform virtual tests, to reduce time in physical tests, can improve design, allows failing and to check errors before manufacturing process, and decreases the price for manufacturing and development time. The digitalization and particularly, usage of the digital twin, has the potential to increase efficiency of the Condition Based Maintenance (CBM) through reduction of time to assess equipment, assures a more efficient reporting, increases usage of condition monitoring systems, and reduces the cost for running CBM [6]. Furthermore, DNV GL has developed a methodology for hull condition monitoring that incorporates the usage of the ship’s digital twin, a virtual model prepared during the design stage. Combined with waves, position and sensor monitoring, the digital twin usage enhances the value of predictive and preventive maintenance. Moreover, it was seen that a combination between data provided by sensors and digital twin extends considerably structural details accessible for monitoring and makes more accurate the condition monitoring of a vessel or other offshore structures and assure a cost-effective instrumentation [7].
DNV-GL emphasized that digital twin can reduce the cost over the operation phase, can contribute to safety and integrity and adequate preparedness and planning can be done with regards to emergency response and planning. Digital twin can indicate possible changes to design and operation and can contribute to efficiency and life extension of an asset. In terms of decommissioning, the digital twin can support labor safety issues and environmental matters
[6].
The application of digital twin in marine industry is not limited to vessels, but offshore renewable technologies can also take advantage of digital twin [8]. As per Johansen [9], the offshore wind turbine market is growing and many maintenance and downtime issues are linked to drivetrains, more precisely to bearings. This matter, in addition to remote and rough offshore environments poses challenges for maintenance and elevates the costs for operation and maintenance. The development of digital twin can be a solution to these challenges. For instance, the modelling of a digital twin for drivetrain in a Multi-Body Simulation (MBS) software requires attention and further investigations. However, creating a high-fidelity digital twin model in offshore wind energy is an issue still to be settled. For the wind farms, the digital technology together with data science, industry expertise and a condition monitoring system can enable outcomes such as automated detection of early gear damage, in time inspections and repairs, reduction of downtime, and reduction of operational costs [10]. Johansen and Nejad [8] emphasized that a digital twin is highly relevant, particularly, for the condition monitoring of marine drivetrains and machinery, the holistic condition monitoring and predictive health monitoring of an asset. Moreover, when considering difficult to access and high values assets in offshore wind industry and shipping, the digital twin implementation can contribute in decreasing maintenance costs and downtime.
Despite many benefits that the digital twin can offer, less is written about the challenges linked with implementing the digital twin. Johansen and Nejad [8] through the case study of Wilhelmsen Ship Management Norway, brought to attention that when the shift from offline Condition-Monitoring (CM) to online CM will occur and cost challenges linked to sensors and their connectivity will be solved, then progress will be done towards implementation of the digital twin as the future of ship operations. However, the need to bring further to attention the challenges associated with digital twin implementation still requires further studies which is the main objective of this article.
Research approach
The aim of this paper is to identify and analyse the sources of risk associated with building and implementation of a digital twin in marine industry by learning from aviation industry which is an industry where the digital twin concept has been well established. With reference to the building and implementation of a digital twin, Johansen and Nejad [8] identified common steps among the industries. Essentially, three steps are the key points of a digital twin development: sensors, model, and the Remaining Useful Life (RUL), fault prediction, and operational decision making, see Figure 1.
The first step of digital twin development refers to sensors which come with their own challenges in terms of high failure, reliability, calibration, efficient and optimized placement and proper selections, just to name a few [8].
The second step for building a digital twin refers to model development. The question which may raise here is which type of model is needed in digital twin development. In order to analyze the data from sensors, in general, there are two approaches: physical-based modeling (PBM) and data-driven modeling (DDM) approaches. The PBM is built up based on real physics of the system, for instance utilizes the equation of motion to predict the system behaviour. The DDM is a model which is built based on data features like for example, after collection of data, it considers the correlations with certain faults or it is evaluating the trend, without relating to physics of the system. This type of model requires collection of large amount of data [8].
The third step considers the output from a model which needs to be analyzed for the RUL and fault prediction. Further on, data interpretations and validations are performed, and recommendations are given for the system performance [8].
The case studies of two recent disasters in aviation industry were analyzed through the above mentioned steps in order to identify critical factors and risk in the implementation of digital twin in aviation industry. The marine industry needs to learn from the aviation industry which is well-known as an advanced industry with reference to the building and implementation of digital twin [2, 3, 4].

Figure 1. Digital twin main steps.
Digital twin in aviation and marine industries
Digital twin signifies basically a virtual representation of the system containing all information available on site. According to Marr [1] the digital twin is a virtual model of a product or process, and acts as a bridge between the physical and digital world. The origins of the digital twin term can be traced back in 2002, at University of Michigan when Michael Grieves introduced it during a presentation to industry as part of the Product Lifecycle Management (PLM) center and during first executives PLM courses. As per Michael Grieves a digital twin has always existed before a physical product comes to existence. A digital twin is seen as a virtual image which contains all the information of a physical product and reflects it throughout the whole product lifecycle. This means that there is a mirroring or twinning of physical and virtual systems, and the conceptual model was called initially as Mirrored Spaces Model or Information Mirroring Model [1, 11].
A digital twin as presented by General Electric is “a dynamic digital representation of an industrial asset, that enables better understanding and prediction of the performance of machines”. Each asset can have its own digital twin and operational details can be accessed throughout its life [4]. Erikstad [12] warned that the digital twin is not an end product in itself. Furthermore, the interpretation of digital twin concept can varies according to different stakeholders and industries. Within the followings, insights about the digital twin in aviation industry and marine industry are introduced.
Digital Twin and Aviation Industry
Through the digital twin a virtual replication of physical airplane parts is provided by usage of simulation software. The virtual model of highly complex systems and components, such as those featured on the Boeing’s airplanes are subject to simulations about the lifecycle of the environments and conditions that component(s) or systems might experience [2]. The digital twin can bring improvements in structural life prediction and can enable better management of an aircraft over its service life [13]. The Boeing started to embrace the digital twin concept from 2017, and the digital twin was launched through an official letter called the Innovation Quarterly.
The digital twin or the model based-engineering was used to design the Air Data Reference Function (ADRF) for the 777X. The ADRF is a key avionics function which process signals from pressure and temperature probes, computing aircraft state parameters like airspeed and altitude, processes signals and data, convert the physical information about flight environment into digital information on cockpit displays for usage by pilots. It was assessed that the digital twin contributed to reduce the cost and time in order to develop the ADRF on the 777X Boeing
[2].
A digital twin in aviation has been explained as being the virtual replica of a physical asset, like for example, an aircraft engine, which can display to engineers on the ground how the engine is running while the aircraft is still in the air. A question has emerged about how the health monitoring through a digital twin differs from a traditional health monitoring. The answer is that the digital twin applies the monitoring approach much earlier, deeper and in more detail, tracks and monitors an asset in real-time taking in account for instance, the temperature of engine, pressure and airflow rate. In this way, through a digital twin and through a virtual model of aircraft, major systems and components, early warnings and predictions of the components behavior and possible likely failure can be received, identified and consequently, action plans can be built. For instance, GE developed the first digital twin for an airplane’s landing gear, and sensors were placed on typical failure points such as hydraulic pressure and brake temperature in order to provide real-time data. This helped to predict the early malfunctions and to diagnose the remaining lifecycle of the airplane landing gear. Data gathered by sensors on the asset are compared with data from its digital twin; simulations are going on about the regular wear and tear and environmental conditions, for instance. In case the two data sets do not match up, then, actions are being taken and asset can enter into service [3].
Digital Twin and Marine Industry
Erikstad [12] identified five characteristics of the digital twin in maritime industry: identity, representation, state, behavior and context. However, there is no universal acceptance of these characteristics which might be neither required, nor sufficient.
DNV-GL [6] defined the digital twin as a “digital representation of a physical asset, its related processes, systems and information”. Furthermore, DNV-GL [6] advised that a digital twin is constructed prior to and in parallel with the actual building of a vessel. The digital twin is a tool which can support for instance, a vessel, over its life-cycle [14]. The digital twin can support the feasibility stage, early design, prior to and during construction phase and commissioning. During the operation of an asset, a continuous learning and updating occurs with reference to the digital twin of asset which receives various data about operational aspects, environmental data, data provided by sensors and input from experts with a relevant industry knowledge [6]. Furthermore, the digital twins facilitates exchanging information among stakeholders such as owners, manufacturers, operators, service providers, system integrator, authorities and different suppliers. A common understanding and exchange of information can be done through various ways such as a cloud-based platform which offers access to operational data, data from environmental conditions such as weather, current waves, sensors data, analysis and analytical and time-domain simulation models [6]. In 2018, as an example, a group of students from the Marine Technology Department, NTNU has developed a digital twin of NTNU’s research vessel Gunnerus in collaboration with DNV-GL. The work on digital twin has continued further developing a method for the RUL calculation of thruster and it will be integrated into the digital twin platform.
According to Bekker [5], the digital twin is “digital, real-time, in-context, operational mime of an asset, which connects the digital and real world representations”. For instance, the polar supply and research ship SA Agulhas II from South Africa can contribute high value to marine industry if the digital twin technology is best implemented. It was found out that a successful implementation of the digital twin will depend on technology readiness, cost of implementation, associated requirements in terms of accuracy, quality and time resolution. Moreover, digital twin allows the measurements and analysis of extreme operational loading like experiments were performed in an operational laboratory [5].
Tian [15] applied digital twin for monitoring and maintaining marine systems such as propulsion systems. The parameters of a test rig from the Lab of Marine Systems Dynamics and Vibration, Department of Marine Technology, NTNU were used to build a physical based digital twin; the tool of Ansys Twin Builder was employed for building the model.
With regards to the offshore wind industry, GE [10] emphasized that through a constant collection of data about environmental conditions, component information, service reports, performance of similar models in the GE (General Electric) fleet, a predictive digital wind model is being built. In Trondheim, Norway, the Fedem Technology (SAP SE) has developed a digital twin and applied it to several systems, like for instance to offshore wind turbines.
Learning from failures in aviation: Boeing 737 MAX crashes in 2018 & 2019
The Boeing’s current in-production models include the newest and top selling models such as 737 MAX, 777X and 787. The Boeing 737 Max series represent the latest model within the Boeing’s 737 line and comprise the Max 7, 8, 9 and 10 models. The 737 has been very successful line for Boeing, but the Boeing 737 Max has been the fastest-selling in the Boeing’s history, as around 100 different operators around the world ordered together more than 4500 Boeing 737 Max. Boeing launched the 737 Max on 30 August 2011, and the first 737 Max was rolled-out of factory in December 2015. This model have been in commercial usage since 2017, when the Malaysia-based Malindo Air received the first delivery on 17 May 2017. Since 2017, Boeing had delivered 350 pieces of the Boeing 737 Max 8 model by the end of January 2019. A small number of Boeing’s Max 9s are also operating around the world, and the Max 7 and 10 models, are not yet delivered, but are due for roll-out within the next years [16, 17].
However, the Boeing 737 Max which has been the best-sold aircraft in the world, has come under intense scrutiny by 2018. Within a period of 6 months, two deadly Boeing 737 MAX accidents took place, one in 2018, and another in 2019. The two disasters occurred shortly after take-off during the critical climb phase. The first disaster crashed the Lion Air Flight 610 into the Java Sea, 12 minutes after taking off from Jakarta, in Indonesia, on 29 October 2018. The second disaster occurred six months later, on 10 March 2019, and the Ethiopian Airlines Flight 302 crashed six minutes after take off from Addis Ababa, Ethiopia [16, 17].
After the crash of the Ethiopian Airlines Flight ET302 on 12 March 2019, the national aviation authorities and airlines around the world have grounded the Boeing 737 MAX 8 due to big safety concerns. At the the time of crash in Ethiopia, more than 350 airplanes of the 737 MAX 8 were in operation around the world. The biggest operator in the world for the Boeing 737 MAX is the Southwest Airlines in the US. However, the United States was one of the last countries to ground this model of airplane. Globally, the China Southern Airlines is also one of the biggest Boeing 737 MAX 8 operators, and among the Europe’s largest operators are the Norwegian Airlines, the TUI Airways and the Ryanair. Turkish Airlines is considered also as a big operator of this model.
Lion Air Flight 610 crash in 2018, Indonesia
The Lion Air Flight JT 610 Boeing 737 Max 8 took off from Jakarta on Monaday, 29 October 2018, at 06.20 (23.20) with destination of city Pangkal Pinang. After 13 minutes, it crashed and all his 189 passengers and crew were killed. By the time it crashed, this Boeing had only 800 hours of flight time [18].
After the Lion Air crash, it was found that the aircraft had experienced problems with a sensor which calculates the angle of flight, or angle of attack. The Digital Fly Data Recorder (DFDR) of the airplane recorded a difference between left and right AOA (Angle of Attack) of about 20 degrees. Moreover, data recovered from the cockpit voice recorders has revealed that pilots were searching for a way to fix the aircraft’s nose of pitching down because of the faulty AOA sensors. Furthermore, pilots were struggling to deal with an automated safety system – known as the Manoeuvring Characteristics Augmentation System (MCAS). Practically, the pilots were constantly fighting against the MCAS system in order to maintain proper airspeed and altitude. Unfortunately, despite pilots’ trials to raise the nose of aircraft, the MCAS triggered more than 21 times and in the end, the aircraft collapsed in sea [16, 18, 19].
The MCAS system has been blamed for this Boeing 737 Max crash in Indonesia. The role of MCAS was to compensate for a design change to the twin engines from past 737 generations. The Max’s engines are larger and mounted farther forward on the wings and this brought issues about aerodynamics of aeroplane. MCAS is a special technology which automatically lowers the nose of airplane in order to head off an aerodynamic stall. MCAS can detect critical flight situations and can intervene in the event of an imminent stall. MCAS would automatically swivel up the horizontal stabilizer, so air flow would push the tail up and push nose down. This MCAS system has been designed to prevent the airplane stalling when making steep turns under manual control. However, if a sensor gives a false reading, MCAS may activate and push the nose down when nothing is wrong with the airplane [20]. A stall can occur when the plane flies at very steep angle and this can reduce the lift generated by the wings, and has the risk to make the airplane to drop. In order to recover from a stall, a pilot would normally push down the nose of airplane. For the Boeing 737 Max, MCAS does this automatically and moves the aircraft back to a normal flight position. MCAS repeatedly takes action if it detects the plane is still tilted at too high angle [16, 19].
On a previous day prior to the Lion Air Flight JT 610 crash, the same airplane encountered problems via Denpasar to Jakarta because of a broken AOA sensor. A pilot issued the secondhighest warning because of problems with MCAS system. But fortunately at that time, the flight was saved because of another pilot which was commuting to Jakarta and helped the crew to disable in time the MCAS system. The flight was continued with manual trim without auto-pilot until landing [19].
After the Lion Air Flight JT 610 Boeing 737 Max 8 crash, Boeing company issued guidance to pilots on how to manage MCAS system. As a note, prior to this disaster, information about MCAS was not included in pilot manual. However, a software fix in order to remedy the problem has been repeatedly delayed by Boeing until the next crash occurred six months later [16, 20].
Ethiopian Airlines 302 crash in 2019, Ethiopia
On 10 March 2019, the The Boeing 737 MAX 8, registered ET-AVJ with flight ET302 took off from Addis Ababa, Ethiopa, at 08:38 local time (05:38 GMT). The flight was supposed to be a two-hours flight and had as destination Nairobi, in Kenya. However, the Ethiopian Airlines Boeing 737 MAX 8 crashed soon in a field, just six minutes after take-off, at 08:44, almost 30 miles (about 50 kilometers) southeast of the airport, near the town of Bishoftu, south of Addis Ababa. All 149 passengers and 8 crew members on board lost their lives [16].
At the time of crash, the aircraft had only around 1,200 hours of flight time since Boeing delivered it to Ethiopia in November 2018. This airplane, underwent on 4 February 2019 a very rigorous first check maintenance and earlier on Sunday, had flown from Johannesburg, South Africa, without any incidents on board. The Boeing 737 Max which crashed in Ethiopia was part of 30 airplanes of this Boeing model which were ordered by the Ethiopian Airlines [16, 17].
Very shortly after taking-off, two sensors measuring the AOA began to record different readings. One of the sensors particularly was giving erroneous readings and this triggered the MCAS safety system which activated and forced the nose of airplane down. The pilots tried very hard to deal with the MCAS system and repeatedly disengaged it and manually tried to steady the airplane and to control the aircraft’s angle of flight. However, the MCAS prevailed in the end, and pushed down the nose of the aircraft until it crashed. The black box flight recorders, more precisely the flight data recorder and cockpit voice recorder of the Ethiopian aircraft have been under scrutiny of the investigators in Paris. It was announced that an official report will be issued within one year after this disaster [16].
Discussions
Analysis of the Lion Air Flight JT 610 and the Ethiopian Airlines Flight ET302 crashes identifies similarities among flight data. The vertical speed readings which indicates how fast a plane is going up and down, were erratic for both airplanes in Ethiopia and Indonesia and are a part of evidence that the pilots in both countries encountered big difficulties in maintaining a stable ascent of the aircraft very soon after taking-off. Furthermore, the US Federal Aviation Administration (FAA) declared that evidence which was collected from both satellite data and site brought to attention that both airplanes in Ethiopia and Indonesia behaved very similarly after taking-off until the crush moment. Moreover, in Ethiopia, a piece of wreckage from the tail of aircraft was found and showed that before the crush the horizontal stabilizers were set to point down the nose of aircraft. Both Lion Air Flight JT 610 and the Ethiopian Airlines Flight ET302 encountered difficulties with the MCAS system which automatically pushes the aircraft’s nose down, or automatically trims the aircraft if it detects a stall [21].
The older Boeing 737 models do not have the MCAS system which its role is to quickly fix the aerodynamic problems which emerged after testing in 2012. the MCAS system was allowed to be triggered on impulse of a single sensor [17, 20]. In the beginning of May 2019, Dennis Muilenburg, the Boeing CEO, acknowledged that the automatic flight control system played a role in both crashes in Indonesia and Ethiopia. He declared that “The full details of what happened in the two accidents will be issued by the government authorities in the final reports, but, with the release of the preliminary report of the Ethiopian Airlines Flight 302 accident investigation, it’s apparent that in both flights the Maneuvering Characteristics Augmentation System, known as MCAS, activated in response to erroneous angle of attack information” [22].
After the crash of Ethiopian Airline 737 Max airplane, the Boeing company and the FAA were heavily questioned by the US regulators and safety experts around the world, particularly, about evaluation of the anti-stall system, and about the training which was offered to pilots around the world. An important issue concerns the AOA signal reading and the MCAS’s dependence on a single sensor. As the case studies of the Boeing 737 MAX 8 crashes in Indonesia and Ethiopia have shown, the malfunctions of sensors has a negative impact on functioning of system and contributes to its collapse. Moreover the probability of hazardous MCAS malfunctions (catastrophic failure) was calculated as virtually inconceivable. Boeing came up with a probability for this MCAS failure of about once every 223 trillion hours of flight. In its first year in service, the MAX fleet logged just 118,000 flight hours. Furthermore, the failure analysis did not consider the possibility the MCAS could triggered repeatedly. In addition, Boeing never flight-tested a scenario in which a broken angle-of-attack sensor would triggered MCAS. During flight testing in 2016, and in order to compensate for other issues, the scope and power of MCAS were further extended [20].
Bekker [5] and DNV-GL [7] drew also attention that the collapse of sensors and data quality to the model are critical for the digital twin. Bekker [5] emphasized that with regards to digital twin, the technological challenges remain, particularly those ones related to the sensor technology. The increased communication capabilities and increased number of sensors brought security issues in order to safely enable the digital twins [5]. Furthermore, with regards to the digital twins and sensor monitoring, the DNV-GL highlighted that a crucial aspect is given by sensor data quality which is a key requirement among industry and authorities [7].
The measurement data are provided to the digital twin through the sensors, which are critical
for their inputs to the virtual model, see the digital twin steps presented in Figure 2. A malfunction of a sensor, wrong input to model and model associated with a high uncertainty and not designed to consider most of scenarios, all of these present a high risk and ultimately, it can lead to a disaster. According to Johansen and Nejad [8], a data driven model is often used in the marine industry rather than physical-based, primarily due to lack of information of the system parameters.
Another important aspect of digital-twin relates to the real-time digital twin and nearreal-time results. A slow processing of data or analysis models will bring a time-lag between measurements and the digital twin model [5]. The digital twin needs to have the ability to update dynamically in real time /near real-time as the state of the physical asset evolves and suffers changes over the time, or in the other words, the physical asset is getting older. The digital twin needs to experiment the same environmental conditions, to develop over the life-cycle like the physical asset and to get continuously updated.
Other critical aspect for the digital twin implementation concerns the role of experts. A digital twin requires various efforts and skills, and interpretation and utilization of its results needs different expertise and training. An interdisciplinary approach is highly required [5]. Moreover, as per DNV-GL [6], the digital twin brings together experts from various fields and requires long-term collaborations and integration of expertise. However, a world of experts requires learning, diversity, acceptance, collaboration, and interdisciplinary approach.
DNV-GL [6] proposed an extension of the Digital Twin concept, the so-called Probabilistic Twin. While the digital twin is a digital copy of a physical asset, the Probabilistic twin is a forecasting tool in order to support the risk management of asset’s operation. The Probabilistic twin couples the digital twin to risk models which are continuously updated based on existing knowledge and actual conditions. Nevertheless, before moving beyond digital twin, the implementation of the digital twin is associated with uncertainties.
The Boeing accidents highlight that the consequences of a faulty digital twin can be very severe and can contribute to major accidents and disasters. These Boeing accidents can be seen through different views or perspectives related to the digital twin: through the sensors malfunctions and software malfunctions, through the faulty verification of the new MCAS system through digital twin and simulators, faulty model, through wrong data-analysis, failure to predict risky scenarios and wrong decision making. These views indicates the importance of sensor reliability, importance of right input measurements, risk of decision making based on wrong input measurements, and significance of better modelling, simulations and verification. The Boeing accidents can be seen as examples of risks associated with digital twin main steps as presented by the Figure 2.
The MCAS system had been tested via digital twin and through simulations, but later on, after disasters, Boeing acknowledged the faults linked to the MCAS simulator and flaws in the 737 Max flight simulators [23, 20]. The Boeing officially announced the updates of MCAS system in May 2019:“MCAS is designed to activate in manual flight, with the airplane’s flaps up, at an elevated Angle of Attack (AOA). Boeing has developed an MCAS software update to provide additional layers of protection if the AOA sensors provide erroneous data. The software was put through hundreds of hours of analysis, laboratory testing, verification in a simulator and two test flights, including an in-flight certification test with Federal Aviation Administration (FAA) representatives on board as observers” [21]. The faulty software linked to MCAS system has been further confirmed through additional layers of protection not limited to: flight control system will compare inputs from both AOA sensors, and MCAS can never command more stabilizer input than it can be counteracted by the flight crew. The pilots will always have the ability to override MCAS system and to manually control the airplane [21].
Nevertheless, during simulator testing of the software changes, the Federal Aviation Administration had identified in June 2019, a new potential risk which is required to be settled

Figure 2. Examples of risk associated with digital twin main steps.
by Boeing. This matter relates to the runaway stabilizer trim, an uncommanded movement of the horizontal stabilizer, the little wing near the tail which moves the plane up or down. If the pilots are unable to correct it and a microprocessor will fail, then, there is the risk of plane crash
[24].
The aviation industry is very advanced in implementation of digital twins [2, 3, 4] and the digital twin has been used much earlier and for longer time in this industry in comparison with marine industry. However, it was identified that the aviation industry still confronts major challenges associated with implementation and development of digital twin. Nowadays, the technological trend of digital twin is growing fast in marine industry, however the implementation of the digital twin in marine industry is not so well developed like in the aviation industry.
The aviation and marine industry are different industries, however with regards to the risk of digital twin implementation, some common traits can be identified.
First, the essential steps of the digital twin as presented by Figure 1 are common steps for both digital twin in aviation industry and digital twin in marine industry. Second, both industries pose risk to people, properties, business and environment. Third, the structures in both aviation and marine industry are subject to dynamic environmental forces; for example, in aviation industry, there is wind, temperature, air density, and in marine industry, there is wind, waves, current load, tidal currents. Moreover, in both industries, there are dynamic electromechanical systems which are used to transfer and convert energy and control of structures such as airplanes, ships and other offshore or marine structures. Thus the lessons and learning from aviation industry with regards to risk posed by digital twin can have high value among other industries, particularly, in the marine industry. Moreover, a reactive learning from major accidents and disasters needs to be continuously supported by a proactive learning and a dynamic risk culture [25].
Concluding remarks
The digital twin presents many advantages, however the implementation of the digital twin is associated with high risk and high uncertainties which must be addressed, even within the industries like aviation industry where the digital twin is well established and at a more advanced level than in the marine industry.
An essential lesson from the aviation industry towards marine industry is that the implementation of the digital twin comes with its own risk, and requires a risk assessment.
The aim of employing digital twin is to reduce the risk in operations, and therefore, the digital twin itself should not pose or bring new risk. However, the recent failures in the aviation industry show the vulnerability of sensors, and cases where the digital twin was not able to identify the system faults - software in this case - during the design and this contributed to two disasters. The digital twin used for system verification of the Boeing 737 Max product encountered failures. The digital twin was ineffective as it failed to simulate and predict those operational scenario cases which could lead to deadly accidents.
Moreover, through the case studies of disasters in the aviation industry it was found that the digital twin may not be able to represent all the possible scenarios which a system may experience during its life time.
As a paradox, the purpose of implementation of the digital twin is to reduce the risk, but the digital twin itself brings its own risk as illustrated by case studies and disasters in the aviation industry. Various challenges are associated with implementation of the digital twin and these include and are not limited to sensor reliability, sensor data quality and their input to the virtual model, uncertainty associated with model, the real-time digital twin and near-real-time results, dynamic updates to the model, the role of experts, integration of expertise, multidisciplinary approach in engineering, and collaboration and learning among industries.
Towards learning from the aviation industry recommends that a risk associated with the digital twin itself shall be assessed before the implementation of the digital twin. The digital twin comes with high uncertainties and cannot be only seen as the technological solution which shall be implemented in order to solve all the problems within the industry. A high awareness indicates that the digital twin shall not be seen only in terms of potential benefits, but comes with its own challenges and risk which need to be addressed in time.
The marine industry, particularly the autonomous ships, needs to learn from and integrate the lessons from the aviation industry with reference to the digital twin. The aviation industry has employed the digital twin for longer time than the marine industry, but still the implementation and usage of digital twin is not risk free and a complete reliance on digital twin is not feasible.
In a digital era with many complicated and highly advanced technological systems, the digital twin solution presents its own related uncertainties which need to be continuously assessed and addressed.
"
Z7Y9W2V7;<pad> digital twin technology is defined as a digital representation of the state and behavior of a unique, real asset or process in almost real time. digital twin technology increases the value of operational data by allowing the measurement and analysis of extreme operational loadings. a digital sister-ship of this vessel is envisioned, where modelbased simulation, data analytics and visualization capabilities are connected in a cloud-based interface. the specific potential of digital twin technology for shipbased research and science is explored over and above the benefits for conventional stakeholders.</s>;"See discussions, stats, and author profiles for this publication at: 

Conference Paper · June 2018

CITATIONS	READS
3	418
1 author:


52 PUBLICATIONS   137 CITATIONS   

Some of the authors of this publication are also working on these related projects:
Dynamic Material Response 
An investigation of shaft line torsional vibration during ice impacts on PSRVs 
All content following this page was uploaded by  on 11 June 2018.
The user has requested enhancement of the downloaded file.

Exploring the blue skies potential of digital twin technology for a polar supply and research vessel 
A. Bekker 
Sound and Vibration Research Group, Department of Mechanical and Mechatronic Engineering, Stellenbosch University, South Africa 
 
ABSTRACT: The SA Agulhas II is a South African polar supply and research vessel, which offers crucial research access to the Antarctic and Southern Ocean. In order to advance the scientific basis for ice-going vessels and ship-based ergonomics this vessel has been the subject of full-scale engineering measurements since 2012. The sensor infrastructure and advanced data analytics that have resulted position this ship as an ideal platform from which to explore a definitive trend in the future marine industry: digital twin technology. This is a digital, real-time, in-context, operational mime of an asset, which connects the digital and real word representations towards actionable insights. The technology readiness of the SA Agulhas II platform, is considered against the conceptual architecture required to implement digital twin technology. Furthermore, the advantages of digital twin technology are explored for stakeholders including the marine industry, the vessel owner and potential applications for advancing Antarctic science. 

1 DIGITAL TWIN TECHNOLOGY 
1.1 Definition 
A digital twin is defined as a digital representation of  the state and behavior of a unique, real asset or process in almost real time (Erikstad, 2017; Parrott & Lane, 2017) within its operational context. Datta (2016) refers to a digital twin as a software avatar that mimics the operation of a real asset or process.  
Digital twin models are versatile and may be created in a wide variety of contexts to serve different objectives (Parrott & Lane, 2017).  These models possess an integrated, holistic, and iterative quality of the physical and digital world pairing (Parrott & Lane, 2017) and can comprise various digital models and collections of information and processes (Erikstad, 2017). 
 Data can be in the form of 3D CAD models, dynamic and discrete simulation models, virtualized control systems and communication networks, analytical models, data models , sensor data, relationship data, as well as digital information including documentation and reports (Smogeli, 2017). 
Erikstad (2017) defines five intrinsic characteristics of digital twins.  
Identity: A digital representation of a single, real, unique asset. 
Representation: The asset in its “as-is” state. This includes as-built models and all subsequent modifications. 
State: A close to real-time representation. 
Behavior: A digital representation of asset responses. 
Context: Describing the context in which the asset operates. 
The implementation of digital twin technology depends much on the asset or process type as well as the required accuracy, quality, availability and feasibility as governed by cost and technology readiness (Erikstad, 2017). At a minimum, a digital twin implementation must at least comprise: 
Edge capabilities for the real-time observation of asset or process response. These may include sensors, data, analytics and integration, whereby information is communicated to a digital platform. 
Digital twin core runtime, which is the twin model itself, using the input stream from the edge to render a (near) real-time representation of the asset. According to Deloitte the architecture of digital twins should be designed for flexibility and scalability. Conceptually, the enabling technology is created through seven steps, which have been adapted from Parrot and Lane (2017). 
Create: Outfitting the asset with sensors to obtain operational responses and environmental / context-specific data. Which are secured and potentially augmented with process-based information. 
Communicate: Seamless, real-time, bi-directional integration / connectivity between a physical process and digital platform, potentially through network communication. This comprises three primary components including edge processing, communication interfaces and edge security. 
Aggregate: Data is ingested into a data repository, processed and prepared from analytics. Aggregation may be performed on the premises or in the cloud. 
Analyze: Data is analyzed and visualized. 
Insight: Insights from data analytics are presented through dashboards with visualizations in real-time. 
Digital twin: This is the digital mime, which can be generated from operational data or through models with outputs, which result from operational data inputs (for example Finite Element Model (FEM) representation from which stress localization can be interrogated). 
Act: Actionable insights from previous steps are fed back to the physical asset or digital process to achieve the impact of the digital twin. 
 
1.2 Enabling technology trends 
Digital twins are relatively new to the maritime industry. Trends in the marine industry indicate that digital twin technology is likely a key competency, which will distinguish innovators in the future of this industry.  This technology, which was named as one of the top ten technology trends in 2017 (Panetta, 2016), promises the contribution which is most prominently that the performance of the marine asset is monitored in operational conditions, and that this information is digitally represented in virtually real-time. Trends that will accelerate the adoption of this technology include: 
The large scale of many marine structures challenges the ability to reproduce extreme thermal, mechanical, and acoustical loadings in a laboratory at anything more than the component scale. Therefore, computational simulation is required to identify and quantify of limit states (Glaessgen & Stargel, 2012). Digital twin technology increases the value of operational data by allowing the measurement and analysis of extreme operational loadings as if experiments were performed in an operational laboratory. As always, future generations of marine vehicles will require lighter mass while being subjected to higher loads and more extreme service conditions over longer time periods. This leads to increased demands on structural materials and requirements to decrease structural margins. Industries, with in-depth operational knowledge of their products will increasingly be able to shave off unnecessary structural margins whilst retaining safety-critical structural integrity. 
The simultaneous development of several digital technologies together are enabling immense potential in digital manipulation and visualization of data.  Specifically, 3D Laser Scanners have become more affordable and the resulting point cloud data can be processed with significantly less effort in software, which requires little training. Today, CAD software can handle enormously large point cloud datasets generated in shipbuilding at virtually little to no additional investment. The combination of these factors has recently led to the increased utilization of laser scanning in shipbuilding (Morais, Danese, & Waldie, 2016) and as such, powerful capabilities to progressively visualize ship structures in higher detail. The potential to maintain as-built models is increasingly feasible because of the shrinking additional costs and benefits to ship-owners who need to have a better understanding of their asset and insights, which will enable cost savings during operations. 
Mutually reinforcing technologies such as Big Data analysis techniques are enabled by increased processing power through cloud storage and analysis. Furthermore, the increasing support of hardware and software interfaces for the IoT (Internet of Things) is now a reality, which increases the amount of data available and spurs the development of machine learning algorithms. Conditions are ripe for the utilization of data as a result of several technologies which have improved rapidly at the same time (Morais et al., 2016). The combination of these will place new tools and creative solutions at the disposal of the marine industry. 
Advances in edge processing have eliminated bottlenecks, which increase the viability of digital twin technology. Edge interfaces connect sensors and process historians, processes signals and data from them near the source, and pass data along to the platform (Parrott & Lane, 2017). Edge processing translates proprietary protocols to more easily understood data formats whilst reducing network communication.  
1.3 Advantages of digital twins 
The ever-increasing ability to perform sophisticated data analytics and to visualize information presents industry with a data-rich layer, which is rife with potential. Deloitte (Parrott & Lane, 2017) emphasizes the real power of a digital twin is that it can provide a near-real-time comprehensive linkage between the physical and digital worlds. These are richer models that yield more realistic and holistic measurements of unpredictability 
Visualization support for an “as-is” asset can allow for remote inspection or inaccessible locations, thereby reducing some of the effort, frequency and risk of physical inspections (Erikstad, 2017). In a 3D digital twin, users can navigate, perform measurements, calculate values, and display, select, filter, localize and annotate objects.  
The accurate context of digital twins can be used for training – for example, training for inspection competence using a survey simulator or for real operations.  
Digital twins can be used to simulate specific complex deployed assets and to monitor and evaluate wear and tear and specific kinds of stress as the asset is used in the field (Parrott & Lane, 2017). Such a model could provide accurate information, which informs service intervals and safety-critical fatigue problems. 
A digital twin, fueled with sensor data, allows decision makers, to intervene or react, if not in realtime, then within a decision interval that enables actions that still have value (Smogeli, 2017).  
As empirical information from sensor data accumulates, digital models will likely increase in predictive value, which will enable more pro-active vessel management, risk avoidance and increased profitability in operations (Smogeli, 2017).  
Accurate hindsight and causality are enabled through the capture of operational data, coupled with simulation models. This delivers high quality design inputs for future builds or designs and can accelerate failure mitigation. 
Digital twins are used for forecasting purposes. These models are named the so called ‘probabilistic twins’ whereby digital twins are coupled to risk models, thereby providing foresight (Smogeli, 2017). 
1.4 Challenges for digital twin technology 
 
Failures in the implementation of digital twin technology have proven that the successful implementation is not a one-company effort. A consortium comprising of DNV GL, the Norwegian University of Science and Technology, Rolls-Royce and SINTEF Ocean have initiated a collaborative effort to develop a new standard in marine digital twins. This standard will be an open-source platform, which is envisioned to include a digital library of generic product models, which can be accessed by any user in industry. Datta (2016) concurs that the rapid diffusion of digital twins calls for open source entity level models of subcomponents. It is envisioned that the model owners of subcomponents, to create and contribute models to a common repository as the vast majority of users cannot deploy an army of engineers to create custom digital twins for their exclusive experiments. 
The quality and reliability of data (veracity), especially when collected in large volumes, from a variety of platforms is challenging to maintain. The users of digital twin technology should further be cognizant of the applicability of digital models in specific situations. For instance, keeping in mind the fact that machine learning algorithms have decreased predictive capabilities when used outside the bound of the training data set (Erikstad, 2017). The emergence of complexly connected trends may be difficult to validate and interpret accurately. 
Technical challenges exist especially related to sensor technology, decision-making intelligence, and system robustness (Heikkilä, Tuominen, Tiusanen, Montewka, & Kujala, 2017). 
Increased sensor and communication capabilities have created new security issues. Common security approaches utilize firewalls, application keys, encryption, and device certificates. Contemporary security solutions are yet under development and solutions to safely enable digital twins will likely become increasingly pressing as assets are progressively IP enabled (Parrott & Lane, 2017). 
 
This article explores the potential advantages of digital twin technology for a polar supply and research vessel, the SA Agulhas II (SAA II). A digital sister-ship of this vessel is envisioned, where modelbased simulation, data analytics and visualization capabilities are connected in a cloud-based interface, with data from full-scale measurement sensors. The specific potential of digital twin technology for shipbased research and science is explored over and above the benefits for conventional stakeholders such as the vessel owner and marine industry. 
2 THE SA AGULHAS II 
2.1 Vessel background 
The SA Agulhas II (SAA II) is a polar supply and research vessel owned by the South African Department of Environmental Affairs. She was manufactured by STX Finland in Rauma shipyard and measures 121.3 m between perpendiculars and is 21.7 m wide. She is propelled by four Wärtsilä 3 MW diesel generators that power two Conver Team electric motors, which are each, connected to a shaft with a variable pitch propeller. Accommodations are available for 44 crew and 100 passengers on annual research and re-supply voyages to South African research bases in Antarctica and the Southern Ocean. 
In terms of research facilities, the SAA II offers eight permanent laboratories for marine, environmental, biological and climate research totaling 800 m2. She is equipped with launching infrastructure for deep-water probes through an environmental hanger door on the starboard side. If the ship is operating in icy waters, an alternative launch porthole is provided through a 2.4x2.4 moon pool. A drop keel with transducers for the measurement of plankton density and ocean currents can be lowered 3 m below the hull surface. Furthermore, a hydraulic A-frame in the stern of the ship is provided to tow sampling nets and dredges.  
The drive towards understanding and exploration of the oceans in globally perplexing matters such as climate change, places the SAA II in a strategic position for prominent research support (Moedas, Pandor, & Kassab, 2017). This is attributed to her annual research voyages to Marion Island, Gough Island, Antarctica and the Southern ocean, which are areas of immense interest for climatologists, oceanographers and marine biologists who are researching food security, global ice-cover and tipping points in the earth’s ecosystem. The global importance of these questions leads to the likelihood that the SAA II may well be funded for international research expeditions beyond her current utilization of 120 days per year for voyages to new geographical stations of interest, some of which will be in ice-covered waters.  An international research consortium comprising the Stellenbosch University, Aalto University, Aker Arctic, DNV GL, Rolls-Royce, STX Finland, University of Oulu, Wärtsilä and the Department of Environmental Affairs South Africa initiated a full-scale measurement program on the SAA II for her ice-trails in the Baltic Sea in March 2012 (Suominen et al., 2013). 

Table 1. A summary of full-scale measurements on the SA Agulhas II. 
 	 
 	 
Table 2. A summary of data products from full-scale measurements on the SAA II.  

These measurements included ice loads on the ship hull and propulsion system, ice-induced structural vibrations and noise, whole-body vibration comfort, ship dynamics in ice, global ice loads, underwater noise and mechanical and physical sea-ice properties.  
The original aim of this work was to contribute to the scientific basis of ice-going vessels by providing operational data and performance analyses during a three day ice trail in the Baltic Ocean (M. Suominen et al., 2013). Since then, Stellenbosch University and Aalto University have continued with a full-scale measurement campaign with the focus on human factors, structural dynamics and environmental conditions, which are experienced on research voyages with durations ranging between 14 and 78 days. A summary of full-scale measurement parameters is presented in Table 1.  
Several research studies have focused on the analysis of the recorded full-scale data. Table 2 summarizes the data products and analysis algorithms, which have been developed through these efforts SAA II. These products relate to human factors, dynamic responses of the hull-structure and shaft-line and environmental conditions in which the vessel currently operates. Several measurement efforts since 2012 have resulted in an increased number of sensors and increased levels of expertise in determining the reliability of captured data. 
An example of such growing expertise is the daily determination of data quality on research voyages. Accelerometer data was traditionally recorded by onboard accelerometers and stored in 5-minute data records. An LMS.Turbine Testing software and hardware combination is used to ensure truly continuous data recordings. Data was stored in LDSF format and extracted / converted to a processible format (.mat / .txt / .csv) about 15 days into a voyage. This required the start-stop of the measurement system. An onboard engineer manually checked the cable connections and measurement equipment on a daily basis to ensure system functionality. With experience, the measurement approach has changed to incorporate the start-stop of the system on a daily basis. The recorded data for this day can now be converted on a daily basis and run through a program to interrogate its veracity. The software evaluates the statistical moments and distribution of the data channels as shown in Figure 1. This enables the identification of data which lies outside of what experience has taught to be normal levels. Researchers onboard thereby have an additional toolset with which to identify faulty or interesting measurements. 
2.2 Readiness of the SAA II for digital twin technology 
The full-scale measurement project on the SAA II is considered against the backdrop of Section 1.1 to assess its status against the ideals, software and hardware requirements to realize digital twin technology. The ‘implemented’ and ‘required’ elements are segregated and summarized in terms of ‘haves’ and ‘have nots’ in Table 3. From this analysis it is apparent that some promising architecture such as sensing and data analysis elements are already in place and that the promising applications, insights and associated requirements of digital twin technology remain to be defined and explored.  
The realization of a digital twin concept for the SAA II will entail the on-board processing of sensor data from a variety of sources, in different formats and at different data rates and levels of reliability. Aside from overcoming these big data challenges, digital twin technology will require the transmission 

of reduced data and therefore on-board processing.
 	 

Figure 1. An illustration of the results from the object-orientated data algorithm to interrogate the veracity of data from 25 acceleration measurements. The minimum-maximum, root-mean-square (RMS), mean value, variance, skewness and kurtosis of the acceleration. 

 
Table 3. An analysis of the readiness of conceptual architecture of the SAA II for the implementation of digital twin technology. 

 This potentially implies the requirement for data acquisition or sensing units that enable real-time data transmission and infrastructure for the aggregation and analysis of data prior to communication. Furthermore, support will be needed for for cheap and abundant data storage on the vessel and algorithms for real-time signal processing on the continuous data stream. It is clear that the demands of digital twin technology with regard to connectivity in polar environments will be an important factor, which governs the required data cost.  
3 EXPLORING THE ADVANTAGES OF 
DIGITAL TWIN TECHNOLOGY FOR 
STAKEHOLDERS OF THE SAA II 
The legacy of closely scrutinized, high quality data from the SAA II ideally positions this platform to prototype future trends in data technology. The question is: What advantages can be gained by embracing realtime digital twin technology? Despite obvious challenges, the proverbial blue skies benefits of digital twin technologies are conceptually explored for the SAA II. The perspectives of different stakeholders including the vessel owner, maritime industry and broader research community are considered. 
3.1 Benefits for the maritime industry 
An actual lifetime loading cycle can be accrued to determine the impact of Southern hemisphere storms and Antarctic ice on the structural health and propulsion systems, which could benefit future ship designs and cause-effect analyses. 
Arctic Marine Transportation System remains demanding, dynamic and complex due to challenging hydro-meteorological conditions, poorly charted waters and remoteness of the area resulting in lack of appropriate response capacity in case of emergency (Heikkilä et al., 2017). This also applies to the Antarctic. The vast majority of marine casualties have their origins in human-related errors. The added advantage of digital twin technology includes safety benefits such as expert remote assistance, which could be offered in real time with the best available visualization and technical information. 
Insight into the accrual of fatigue loading of the hull and propulsion structures will assist in the development of long-term strategies to operate and navigate ships for a prolonged service life. 
In a more general sense, this pursuit can be motivated by the industrial drive towards the reduction of ship crew and new technologies for the remote sensing and control of ships. The ideal of automated ships, cannot be accomplished without sufficient high quality operational data to train and condition machine learning algorithms and prototype observations of ship responses to control algorithms. Digital twin technology creates the ideal environment for prototyping these ideas. The autonomous operation of ships with harsh voyage profiles will likely prove a significant engineering challenge. As such, the study of the voyages of the SAA II in harsh waves and ice could contribute to the training data required for autonomous operation algorithms. 
3.2 Benefits to the vessel owner 
Challenging ice conditions and rough open water are the factors that most degrade operational efficiency. The real-time processing of operational ship data will assist in aiding tactical judgments in ship handling to improve safe navigation and cost savings. This could result in direct economic benefits through fuel savings and increased scope for research with more efficient ship operations becoming possible for a given budget and available ship time. 
Maintenance intervals could be optimized according to the factual requirements of the aging vessel structure. Accurate assessment of the fatigue life of the vessel as she nears the end of her service life would result on decommissioning based on facts which is sure to offer operational, safety and costing benefits over estimations based on the best engineering logic available. 
The imperfections in engineering practice for icegoing vessels in the Southern hemisphere is emphasized by the fact that the SAA II is pre-disposed to prevalent stern slamming in even mild following sea states with 1 m swells, (Omer and Bekker, 2016). The pursuit of mitigating measures for slamming is governed by the underlying question: Will slamming vibration crucially reduce the operational lifecycle of the vessel? Public literature is largely devoid of recommendations to eliminate wave slamming through navigational strategies and full-scale measurements that elude to possible loading conditions and risks. The reason for this is the unpredictable and non-linear nature of slamming on full-scale ships, which implies that measurement campaigns can run for years without successfully capturing a slamming event.  
Safer navigation in remote Antarctic ice is greatly benefited by a hull monitoring system because of the lack of ship response feedback to vessel operators. It is difficult for the crew to estimate the magnitude of the loads in harsh environmental conditions. Real-time, quantative measurements could provide information to avoid unintentional damage (Wang et al., 2001). The added advantage of digital twin technology includes safety benefits 
in that expert remote assistance could be offered in real time. 
Digital twin technology could further be used as a ship “flight recorder” (Wang et al., 2001) to correlate accidents with the most recent measurement and simulated data available. 
3.3 Benefits to ship-based polar research 
Remote polar research could become a possibility through correct data reduction and transmission. This could reduce the pressure on berth space requirements. Participating researchers will not be required to be in situ, therefore also enabling the real-time involvement of researchers who are not healthy, or able to be away from shore for extended periods.  
Digital twin technology would enable a landbased presence in Antarctic waters, which could reduce human impact in this sensitive natural environment, whilst enabling increased participation from academic supervisors and research collaborators. 
The real-time availability of research data will result in the quicker delivery of results to landbased collaborators and shorter lead-times to research publications.  
Adjustments or additional measurements could be requested by land-based scientists before the vessel progresses too far from her remote location which will increase scientific work quality and furthers the agenda of opportunistic science. 
By embracing a digital, data-driven mindset the recording, modelling and reduction of various data will lead to cross-pollination of disciplines – observation of inter-relationships between more variables and new insights. South African research efforts will furthermore remain contemporary enough to contribute to international research efforts in developed nations who will embrace emerging technologies and rapid change through cloud-based digital technology platforms of their own. 
The drive towards understanding and exploration of the oceans in globally perplexing matters such as climate change, places the SAA II and her operations in the spotlight (Moedas et al., 2017). This is furthered by the imperative of the South 
African government to build the South African Blue Economy. From this perspective, a digital twin of the SAA II has the potential to have immense showcasing potential whereby real-time vessel loads / oceanographic information could create a cloud-based museum. This could be show cased for the purposes of school tours or targeted events at the Department of Environmental Affairs offices at the iconic Victoria and Alfred Waterfront in Cape Town. 
Research teams typically suffer from high turnover rates as post-graduate students graduate on a 2 to 5 year cycle. A digital twin of the SAA II offers immense training potential because of its remote accessibility. The consolidation of all available models and information could benefit knowledge retention and continuation of research efforts, which benefits long-term observations and trending which are essential to answer global research questions. 
Real-time measurement and modelling places the vessel in an environmental laboratory, whereby the fidelity of digital response models can be interrogated and improved in their operational context in real-time. As such, operational data is truly utilized for the asset that it can be. High fidelity models could be of high value when considering the feasibility of chartered voyages to new, remote stations to support emerging research agendas. 
3.4 Challenges 
The successful implementation of digital twin technology will depend on the aspect of the vessel which is to be represented, and the associated requirements in terms of accuracy, quality, and time resolution. The ability to deliver such models will depend on technology readiness and cost of implementation. In the light of the foregoing discussion, some challenges / next steps can be highlighted towards the realization of a digital twin technology for the SAA II. It can be a daunting task to create a digital twin, which aims to deliver the blue skies advantages all at once. Deloitte (Parrott & Lane, 2017) recommends to start in one area, deliver value there, and continue to develop. Foreseen challenges include: 
The versatile uses and many benefits of digital twin technology are numerous. It is important to focus and identify large benefits, which may be acquired through cost-effective means and little additional digital twin architecture. 
Presently full-scale measurements on the SAA II are not obtained or aggregated through a centralized data acquisition network. A central information system is required and will enable concurrent data acquisition synchronized by a single time stamp. A robust sensing network with edgeprocessing (Mao, Hou, & Wu, 2008) capabilities is likely required to achieve the required data sensing capabilities (Erikstad, 2017). 
Poor internet connections in remote Southern ocean environments will inhibit the streaming of live data and will require significant data reduction. 
Utilizing the full potential of digital twin technology requires insight into the inter-disciplinary research applications of the ship and her laboratories outside the ambit of engineering technology, science and research. 
Some technology readiness challenges remain, especially related to sensor technology, decisionmaking intelligence, and system robustness in terms of ship-based measurements (Heikkilä et al., 2017). 
To date the full-scale measurement project on the SAA II has not involved the development of simulation models. Insight with regard to stress concentrations and accrual of fatigue loadings will require access or creation of as-built digital models and vital partnerships with appropriate expertise in simulation and visualization. 
In order to realize near-real-time results the processing of data or analysis models must be completed within the measurement time. Slow processing will introduce an ever-growing time lag between measurement and the digital twin model that at best is constant, and at worst continues to grow. 
Not only does it require effort and skill to create a digital twin, but also to interpret and utilize the results it produces. Initial prototype systems may not be robust and will require expertise to debug. This implies that the successful implementation of this technology relies also on user readiness and training. 
8) Data security concerns and secure data encryption are undeniable challenges of the future, which have not been addressed, nor explored in the contect of the SAA II. 
4 CONCLUSIONS 
Literature is rife with the potential applications of digital twin technology, which are clearly versatile. Implementations of digital twin platforms on ships have not yet been reported, however current trends indicate that such technology is best implemented in open source consortia and will be a distinguishing factor in the marine industry of the future. The SAA II polar supply and research vessel is a promising platform, which offers a mature sensing network and advanced data analytics, which can contribute immense value to the marine industry, the vessel owner and engineering and earth sciences if digital twin technology can be sufficiently harnessed. 
ACKNOWLEDGEMENTS 
The financial assistance of the National Research Foundation (NRF) towards this research is hereby acknowledged. The collaboration of Aalto University, Pretoria University and the Department of Environmental Affairs of South Africa is gratefully acknowledged. 
"
QQY7CVN2;<pad> the goal of developing and implementing the digital twin technology is to increase business efficiency throughout the life cycle of a product or process. driven by data collected from sensors in real time, these sophisticated computer models mirror almost every facet of a product, process or service. a digital twin can be defined as a constantly changing digital profile containing both previous and most relevant data about a physical object or process.</s>;"Corresponding Author:
Zakharov L.A. leonidzakharov92@gmail.com
Received: 5 March 2020
Accepted: 18 March 2020
Published: 8 April 2020
Publishing services provided by Knowledge E
 Zakharov L.A. and Derksen L.A.. This article is distributed under the terms of the , which permits unrestricted use and redistribution provided that the original author and source are credited.
Selection and Peer-review under the responsibility of the SEC 2019 Conference Committee.

Conference Paper
Hardware and Software Infrastructure of Digital Twin Technology
Zakharov L.A. and Derksen L.A.
Ural Federal University named after the first Russian President B.N. Yeltsin, Russia, 620002,
Ekaterinburg, street Mira, 19
Abstract
This article describes of hardware and software infrastructure that provides the implementation of digital double technology. The basic approaches to determining the technologies that make up the infrastructure for the implementation of the digital twin, as well as the benefits of implementing this technology are considered. The need for processing and storing big data, as well as the benefits of implementing this technology, is substantiated.
Keywords: digital twin, digital model, big data, product lifecycle, cyber-physical system, automation, machine learning, smart maintenance.

Introduction
Digital twins — precise, virtual copies of machines or systems — are revolutionizing industry. Driven by data collected from sensors in real time, these sophisticated computer models mirror almost every facet of a product, process or service. Many major companies already use digital twins to spot problems and increase efficiency [1].
The goal of developing and implementing the digital twin technology is to increase business efficiency throughout the life cycle of a product or process (Fig.1).

Figure 1: Product lifecycle model
A digital twin is a model with a low level of abstraction, a computer form of a specific physical product. It may include its geometry, manufacturing method, and other information. A digital double can be very detailed, reflecting a wide range of product characteristics. It may contain:

How to cite this article: Zakharov L.A. and Derksen L.A., (2020), “Hardware and Software Infrastructure of Digital Twin Technology” in III Annual International Conference ”System Engineering”, KnE Engineering, pages 29–35. DOI 10.18502/keg.v5i3.6754
digital model of the product (shape, size);
specification of materials;
manuals and product maintenance data;
information on the behavior of the product in various conditions.
information on manufacturing methods and supply chain
In industrial and scientific sources, the definitions of ”digital twin” are different. According to some of them, the digital double is an integrated model of a product already built, which is designed to contain information about all parameters and defects of the product and is regularly updated during physical use - sometimes such an object is called a ”digital shadow” of the product. Another common definition is a digital model, obtained based on information from sensors installed on a physical object, which allows you to simulate the behavior of an object in the real world. None of these definitions, however, give enough attention to processes as an important aspect of the digital double.[2-4]
Some experts identify three types of twins: digital twin prototypes (DTP), digital twin instances (DTI) and aggregated twins (Digital Twin Aggregate, DTA).
DTP (prototype) is a virtual analogue of a physical object existing. It includes data for a comprehensive description of the model, including information on its creation in real conditions. These are production requirements, a three-dimensional model of an object, a description of technological processes and services, and requirements for disposal.
DTI (instances) - data on the description of a physical object. Most often they contain an annotated three-dimensional model, data on materials used in the past and present time, and components, information on the processes performed in all time periods, test results, records of repairs, operational data received from sensors, monitoring parameters.
DTA (aggregated twin) is a computing system that combines all digital twins and their real prototypes and allows you to collect data and exchange them. [5]
Fundamentally, a digital twin can be defined as a constantly changing digital profile containing both previous and most relevant data about a physical object or process, which allows optimizing business efficiency. It is based on a large amount of accumulated data obtained during measurements of several indicators of an object in the real world. The analysis of the accumulated data allows you to obtain accurate information about the performance of the process, as well as lead to conclusions about the need to make changes both in the manufactured product and in the production process itself.
Infrastructure of Digital Twin technology
Infrastructure is a complex of interconnected service structures or objects that include and provide the basis for the functioning of the system. The modern infrastructure for supporting production processes (including digital twin technology) must be decentralized - the user must be able to access resources from anywhere, regardless of their location.
Any object or process is in an operational environment that changes throughout the entire life cycle, while under the influence of external factors the characteristics of the object will change. At the stage of development of the concept, the factors are the needs of stakeholders and regulatory documentation, at the stage of development technical solutions of designers, at the stage of production of the product - machines and tools used to manufacture the product (or implementation of the process), at the stage of operation - various external influences arising during operation or maintenance. For the successful implementation of this technology, it is necessary to introduce an infrastructure that ensures the collection and processing of incoming information.
The basic component of such an infrastructure is a life cycle management system a software package, which in turn consists of several subsystems that implement all or some of the functions at the enterprise:
Manage system descriptions
Requirements Management
Project management
Management of design processes
Product composition management
Product line management
Compliance Management
Content and document management
Recipe, packaging and brand management
Supply chain management
Electromechanical data management
Process control of technological preparation of production
Settlement data management
Operation, maintenance and repair
Reports and analytics
Collaboration Tools
Visualization (including VR and AR)
Platform extension services
Integration Services
Enterprise knowledge management platform
To collect data from a real physical object, several sensors are used that convert external influences into a form convenient for processing. Due to the large number of necessary parameters (acceleration, orientation, temperature, stress) and the large number of points for taking these parameters, a trend has arisen for ”smart sensors” - compact sensors that can measure and transmit many parameters simultaneously. Typically, these sensors are equipped with wireless interfaces for easier integration into the product design and data acquisition system. An important problem that arises in the process of implementing the technology is to ensure the reliability of the data received from the sensors. In the process of testing and operation, data from the object can be falsified - as a result of a sensor malfunction or malicious intent. Blockchain is a promising technology for data tracking - for example, now this technology, in combination with electronic identifiers (RFID and NFC), is being successfully implemented to ensure the security of supply chains and ensure product quality, and to counterfeiting control.
An important condition for the implementation of this technology is the integration of computing power directly into the product or equipment that ensures the production process. Such equipment belongs to the category of cyber-physical systems and allows you to create a process control system with a decentralized structure, with the properties of self-organization. Main components of typical cyber-physical system show on fig.2
[5]
Processing incoming information is impossible without the availability of a data acquisition system and computing clusters. Throughout the product life cycle, a large amount of data and information is generated, for the storage and processing of which storage servers and computing clusters are used. An important requirement for these subsystems is scalability - the ability to quickly increase the amount of computing power and information capacity in accordance with the growth of incoming data.

Figure 2: main components of cyber-physical system
Results and Discussions
The advantages arising from the implementation of Digital Twin technology depend on the level of abstraction of the digital model, as well as the complexity of the infrastructure used. When developing a digital product double, data management is simplified, which leads to an increase in labor productivity at the stages of development and production. Processing of large data generated by cyber-physical systems in production allows you to quickly identify the relationship between the change in technology and the quality of the resulting products, which leads to an increase in the effectiveness of the quality control system in the enterprise. The use of digital twins in RnD allows you to quickly verify and validate the product or discover non-obvious relationships between various parameters of a complex system.
Registration and processing of external factors acting on the facility allows the use of the technology of “predictive maintenance” - the development of a technical plan in accordance with the actual level of impact on the facility. The use of this technology allows to reduce costs arising from equipment wear. The principle of scheduled maintenance considers the estimated impact on the product without considering real, unplanned impacts. With such an approach to servicing, both cost overruns are possible (if the product’s real resource is more than expected), and emergencies arise due to unforeseen effects. In the case of applying the predictive approach, maintenance is carried out in accordance with the principle of necessity and sufficiency for restoring the product’s resource, which eliminates cost overruns, and allows you to prepare in advance for the maintenance process. An example is the steam turbine service approach first introduced by Duke Energy - a vibration-sensing sensor has been integrated into the product. As soon as the vibration level began to exceed normal values, it was concluded that an early failure would occur, which served as the reason for the start of maintenance, while early detection of the problem (turbine blade defect) allowed the company to save more than $4 million. The maintenance process itself can be improved with the help of information support - for example, interactive instructions using augmented reality technology can reduce personnel qualification requirements and significantly reduce maintenance time. In combination, all modern technologies used to increase the efficiency of maintenance are called “Smartenance” (Smart Maintenance)
The most significant advantage provided by the technology of ”digital twin” is the ability to create process control systems of a fundamentally new type. The most significant obstacles to the automation of complex systems are:
Many possible states of the process, while the number of undesirable states far exceeds the number of desired
High state transition rate
At the same time, modern technical systems generate a large amount of operational information that exceeds the ability to process this information by a person and classical (Kalman) control systems. To reduce such risks, new management systems based on machine learning and knowledge management are being developed. The creation of such systems is impossible without a large amount of data on the behavior of the object in various conditions, because they are necessary for the machine learning process. The introduction of such control systems is most important for complex technical objects and processes, in the event of failure of which catastrophic consequences occur (nuclear reactors, hydropower plants, etc.). It should be noted that in the development of such control systems, the presence of errors in the training data set can lead to a deterioration in the quality of management; therefore, increased attention should be paid to cleaning this data, as well as paying attention to anomalous parameters in datasets.
Conclusion
The features of the «digital twin» technology considered in the article can be used to develop a concept of application in the enterprise’s business processes. Successful implementation of these technologies is impossible without the use of hardware and software infrastructures that provide the collection and processing of a large amount of data received from a product or production system. The data obtained can be used to iteratively improve the product. The introduction of this technology improves the efficiency of both individual business processes and the enterprise as a whole.
"
R8BN99ID;<pad> the digital twin paradigm can be used to optimize operational parameters of a system or mission. probabilistic diagnosis, prognosis, and mission planning can be used to optimize crack growth. the fusion of information gained from multi-physics, multi-fidelity, computational models as well as sensor data is critical for accurate, efficient health diagnosis.</s>;"Journal Pre-proofs

Digital twin approach for damage-tolerant mission planning under uncertainty

Pranav M. Karve, Yulin Guo, Berkcan Kapusuzoglu, Sankaran Mahadevan, Mulugeta A. Haile
PII:	S0013-7944(19)30649-6
DOI:	
Reference:	EFM 106766

To appear in:	Engineering Fracture Mechanics

Received Date:	18 May 2019
Revised Date:	30 October 2019
Accepted Date:	5 November 2019

Please cite this article as: Karve, P.M., Guo, Y., Kapusuzoglu, B., Mahadevan, S., Haile, M.A., Digital twin approach for damage-tolerant mission planning under uncertainty, Engineering Fracture Mechanics (2019), doi: 


This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.


© 2019 Published by Elsevier Ltd.

Digital twin approach for damage-tolerant mission planning under uncertainty

Pranav M. Karve, Yulin Guo, Berkcan Kapusuzoglu, Sankaran Mahadevan1
Department of Civil and Environmental Engineering, Vanderbilt University, Nashville, TN, USA

Mulugeta  A. Haile
U.S. Army Research Laboratory, Aberdeen, MD, USA





Abstract

The digital twin paradigm that integrates the information obtained from sensor data, physics models, and operational and inspection/maintenance/repair history of the system or component of interest, can po- tentially be used to optimize operational parameters of a system or mission in order to achieve a desired performance or reliability goal. In this article, we develop a methodology for intelligent mission planning using the digital twin approach, with the objective of performing the required work while meeting the damage tolerance requirement. The proposed approach has three components: damage diagnosis, damage prognosis, and mission optimization. All three components are affected by uncertainty regarding system properties, op- erational parameters, loading and environment, as well as uncertainties in sensor data and prediction models. Therefore the proposed methodology includes the quantification of the uncertainty in diagnosis, prognosis, and optimization, considering both aleatory and epistemic uncertainty sources. We discuss an illustrative fatigue crack growth experiment to demonstrate the methodology for a simple mechanical component, and build a digital twin for the component. Using a laboratory experiment that utilizes the digital twin, we show how the trio of probabilistic diagnosis, prognosis, and mission planning can be used in conjunction with the digital twin of the component of interest to optimize the crack growth over single or multiple missions of fatigue loading, thus optimizing the interval between successive inspection, maintenance, and repair actions.
Keywords: fatigue crack growth, digital twin, diagnosis, prognosis, Bayesian estimation, information fusion, optimization, uncertainty quantification.



Nomenclature

θ	Vector of model parameters

∆K	Range of stress intensity factor

sm	Measurement error

x	Vector of deterministic decision variables

xlb	Lower bounds for the deterministic decision variables

1Corresponding author, Email:  


Preprint submitted to Engineering fracture mechanics (Special issue on Digital Twin)	October 30, 2019

xub	Upper bounds for the deterministic decision variables

σm	Standard deviation of measurement error

a	Crack length

aN	Crack length after N fatigue loading cycles

acrit	Critical crack length
ppost	Posterior distribution
pprior	Prior distribution

R	Stress Ratio

ui	Displacement at i-th loaded node in finite element model

W	Work performed for a given load and crack length

Wcycle The work done in a fatigue loading cycle
WGP	Gaussian process surrogate model that estimates the work done in the loading phase of a cycle
Wmin	Minimum amount of work that needs to be performed in each mission
ydata	Diagnostic data obtained from pitch-catch tests

νa	Poisson’s ratio of adhesive

νp	Poisson’s ratio of Aluminum 7075-T6 plate

νxy, νyz, νxz Poisson’s ratios of orthotropic piezoelectric transducers

ρa	Density of adhesive

ρp	Density of Aluminum 7075-T6 plate

ρpzt	Density of orthotropic piezoelectric transducers dij	Dielectric coefficients of piezoelectric transducers Ea	Young’s modulus of adhesive
Ep	Young’s modulus of Aluminum 7075-T6 plate

eij	Dielectric permittivity of piezoelectric transducers

Exx, Eyy, Ezz Young’s moduli of orthotropic piezoelectric transducers S0e	Scatter energy in S0 mode wave packet for a given crack length S0o	Scatter energy in S0 mode wave packet for for the initial flaw
T	Nodal force in finite element model


Introduction

Modern aerospace systems often work in dynamic environments with significant variability in loads, operational requirements, and environmental conditions. Additionally, they need to cope with degradation and failures of the physical components due to aging, operational stress, and environmental conditions. There
5 is sometimes a need for aerospace vehicles and equipment to operate for long periods of time without the opportunity for maintenance or repair, for example, during extended missions; thus strategies for extending the maintenance-free operation window become important. These may include mission planning before a mission, or adaptive actions during the mission (such as changing the maneuver of the vehicle to reduce  or redistribute the stress) in order to slow down the damage progression.   The execution of such strategies
10 depends on the diagnosis of the current health state of the system, and the prediction of how the damage will grow during a desired mission. In this work, we investigate a new risk management paradigm for achieving robust and reliable system operation, through the investigation and integration of several ideas: information fusion, probabilistic damage diagnosis, probabilistic damage prognosis, and mission planning optimization. To this end, we tackle three key aspects of the problem of interest: a) fusion of heterogeneous information from

15 sensors, models, and other sources in order to achieve efficient probabilistic diagnosis and evaluate current system health; b) development of efficient probabilistic prognosis and uncertainty quantification algorithms to predict future health, capability, and reliability; and c) investigation of decision-making algorithms for mission planning, in order to ensure reliability and safety in the completion of a future mission.


Figure 1: Components of the proposed digital twin approach

The digital twin paradigm is well-suited for performing the aforementioned tasks (Figure 1). As defined by
20 Glaessen and Stargel [1], “a digital twin is an integrated multi-physics, multi-scale, probabilistic simulation of an as-built vehicle or system that uses the best available physical models, sensor updates, fleet history, etc., to mirror the life of its corresponding flying twin”. Continuous learning from sensor data obtained from the flying twin that enables decision making with up-to-date information is a key advantage afforded by this paradigm. The digital twin concept has previously been studied for manufacturing, intelligent system maintenance, and
25 asset sustainment [2, 3, 4, 5, 6, 7, 8, 9, 10]. We seek to utilize the digital twin of a mechanical component to perform intelligent operational planning that ensures reliable operation of the system and/or the component. The fusion of information gained from multi-physics, multi-fidelity, stochastic computational models as well as sensor data is critical for performing accurate, efficient health diagnosis as well as reliable damage growth predictions.  Computationally efficient and accurate digital replicas of real-world mechanical systems and
30 components are necessary to tackle optimization problems concerned with mission planning. Furthermore, the quantification of uncertainty in the current estimate of the system state and in the prediction of system health and performance in a future mission, and treatment of the quantified uncertainty in the mission planning/optimization algorithm are crucial for ensuring reliable system performance in the future mission. In this article, we develop the digital twin paradigm that addresses the above needs, and perform experiments
35 that illustrate how probabilistic damage diagnosis, damage prognosis, and mission planning under uncertainty can be integrated to increase the maintenance-free operation period of mechanical components. We provide a brief overview of these three aspects of the problem in what follows.

Probabilistic damage diagnosis
Intelligent operational planning for the mechanical component of interest to achieve a performance goal
40 (for example, extension of maintenance-free operation period, or enhancing the resilience in completing a mission) requires quantification of the current state of damage. The digital twin approach relies on the system health assessment history to provide up-to-date information for effective decision making. To this end, both the severity of damage and the (aleatory and epistemic) uncertainty in the diagnosis need to be quantified throughout the life of a system or a component. Damage diagnosis is an inverse problem that can be
45 tackled using data-based, physics-based, or hybrid approaches. These approaches rely on a forward prediction model (either mechanistic or empirical), that predicts a damage-sensitive response quantity of the system to a known mechanical, electromagnetic, optical or other type of excitation. The inverse problem seeks to identify the damage presence/location/severity, using the model of choice, and the measured, damage-sensitive system response (data) to a known excitation of choice. Thus, variability of inputs and parameters used in the model,
50 noisy and erroneous data from faulty or damaged sensors, as well as the epistemic uncertainty regarding the model are the key sources of uncertainty in damage diagnosis.
The probabilistic damage diagnosis algorithm needs to have the capability to quantify the uncertainty in the estimate of the damage resulting from the aforementioned uncertainty sources. The physics model- based, Bayesian damage diagnosis approach proposed in this article naturally quantifies the uncertainty
55 in diagnosis [11], and can leverage reliability analysis methods well developed in the literature. In the consideration of sensor data, it can include the following cases: a) damage not detected, b) damage detected but not measured, and c) damage detected and measured [12]. In the case of on-board sensing, the Bayesian methodology can also include different scenarios of data availability and missing data. Information from inspection, previous mission records, and structural repairs is likely to be heterogeneous, available in different
60 formats and different levels of fidelity. The Bayesian methodology is well suited to fuse the information from such heterogeneous sources of data.
In this work, we perform damage diagnosis by fusing homogeneous sources of data, that is, data obtained from multiple combinations of actuators and sensors in pitch-catch sensing. The same approach can be used with heterogeneous sensing data, i.e., when different types of sensors are used. However, we use two
65 different sources of information in this work, where the physics-based model is first corrected using preliminary diagnostic experiments to obtain the final model that is to be used in the Bayesian damage diagnosis. To this end, we first build a high-fidelity numerical model of the governing physics for the component in question. We obtain the mean values of damage sensitive features corresponding to known damage severity using this model. We conduct preliminary diagnostic experiments on components similar to the one used to demonstrate
70 the methodology. The training data obtained from these preliminary tests consists of the values of damage sensitive data features for known damage severity. We used this data to update the physics-based diagnostic model. This updated model is used in Bayesian damage diagnosis and information fusion.
Probabilistic damage prognosis
Once the state of damage in a component of interest is diagnosed (along with an estimate of the diagnosis
75 uncertainty), intelligent mission planning can be performed if the propagation of damage under various candidate mission profiles can be estimated. This can be achieved by means of probabilistic damage prognosis. Typically, models of different level of fidelity are available for performing damage prognosis. For example, if sub-critical fatigue crack growth is the damage type of interest, then various models that employ either   a global fracture criterion (linear elastic fracture mechanics or elastic-plastic fracture mechanics), or local



80












85












90












95














100












105












110












115












120

fracture criterion are available. These models involve inputs and parameters that are uncertain due to natural variability; experimentally obtained parameters that suffer from data uncertainty; and model errors. Thus, the sources of uncertainty that need to be considered for probabilistic damage prognosis are: a) uncertainty in the estimate of the current state of damage (diagnosis uncertainty), b) natural variability in model inputs (loads, etc.) and model parameters, c) epistemic uncertainty in model parameters calibrated using experimental data, and d) epistemic uncertainty due to model errors (model form error, numerical discretization error, surrogate model error, etc.).
Methodologies for probabilistic fatigue damage prognosis have been studied for several decades. However, these methods have mostly considered aleatory uncertainty (natural variability) but not epistemic uncertainty (lack of knowledge due to data and modeling inadequacies). Here, the Bayesian approach will be used to fuse both aleatory and epistemic uncertainty from various sources to quantify the overall uncertainty in prognosis. We use a linear elastic fracture mechanics-based model to perform fatigue damage prognosis. The model requires stress intensity factors (SIFs) for various loads and damage severity levels as an input. For non- trivial loading conditions and component geometry, SIFs need to be obtained by performing computationally expensive finite element analysis. Here, we build a surrogate model that outputs the SIF given the load and damage severity as inputs to alleviate the computational burden. Furthermore, we calibrate the key model parameters using laboratory test data. Thus, in the proposed probabilistic damage prognosis methodology, we consider: a) (epistemic) diagnosis uncertainty, b) (epistemic) model uncertainty, and c) (aleatory) uncertainty in model parameters.
Load profile optimization under uncertainty
Contemporary aerospace systems often undergo calendar-based maintenance, which relies on a predeter- mined schedule, and may result in increased costs and unknown risk. Condition-based maintenance (CBM) is an efficient, cost-effective maintenance paradigm that ensures safe operation of aerospace systems. Various aspects of the CBM philosophy can be effectively realized, and even enhanced, using the digital twin concept. For example, the CBM philosophy can be extended by pursuing condition-based operations of the mechanical system, i.e., making operational decisions (such as mission profile) based on the current condition of the system. If successfully implemented in practice, this approach can further improve the cost-effectiveness. If the damage severity (and the associated uncertainty) is known, and a well-calibrated model for probabilistic damage prognosis is available, then mission planning or system reconfiguration can be performed to extend the maintenance-free operation window. In this case, an optimization problem can be formulated, wherein mission parameters that optimize a suitable metric of system performance or damage growth are sought while ensuring that some constraints regarding system performance, safety, and operation time are satisfied. The probabilistic diagnosis can be performed offline (after the completion of a mission) or online (during the mission). The optimization problem needs to consider the following important sources of uncertainty:
uncertainty in the current state of damage (available from probabilistic diagnosis), and b) aleatory and epistemic uncertainty in the probabilistic damage prognosis.
For the proposed intelligent operational planning, we consider a damage growth minimization problem, and set the loading history parameters applied to the mechanical component of interest as the decision variables. The optimization problem is solved subject to a minimum mechanical work requirement and maximum allowable duration to complete the task. At least two strategies for defining the objective function for load profile optimization under uncertainty are available: a) minimization of the expected damage growth, and b) a reliability-based approach where the probability of damage growth exceeding a critical size is minimized (minimization of the probability of need for maintenance). The first approach is similar to robust








125













130














135














140












145
















150














155












160

design optimization [13, 14]; whereas the second approach can be described as reliability-based design optimization [15, 16, 17]. We consider both approaches depending on their suitability at different stages of life of the component.
The the key characteristics of the work discussed in this article are summarized below:
Development of a probabilistic damage diagnosis methodology that is capable of tackling physical vari- ability, data uncertainty, and physics model uncertainty. The methodology utilizes Bayesian estimation to perform information fusion, and quantifies the damage severity as well as the associated (diagnosis) uncertainty.
Development of a probabilistic damage prognosis methodology that considers various important sources of epistemic and aleatory uncertainty in the damage growth prediction, and quantifies the uncertainty in damage prognosis.
Development of a digital twin that fuses the information gained from probabilistic damage diagnosis and prognosis. The digital twin thus supports intelligent decision making (mission planning) using up-to-date information, and quantified uncertainty.
Formulation and solution of the (probabilistic) optimization problems concerned with intelligent mission planning.

The remainder of this article is organized as follows. In Section 2, we develop the methodological as- pects of the three main tasks: probabilistic damage diagnosis (Section 2.1), probabilistic damage prognosis (Section 2.2), and load profile optimization (Section 2.3). In Section 3, we discuss how we build a digital twin for damage diagnosis, prognosis, and load profile optimization for the illustrative example. These in- volve discussions on numerical models, model calibration, surrogate model building, and formulation of the optimization problems. In Section 4, we first discuss results pertaining to the preliminary steps required for building the digital twin for the component of interest. We then discuss results of laboratory experiments that use the digital twin to optimize the cyclic load profile (Section 4.4). Lastly, in Section 5, we provide concluding remarks and discuss future work for achieving extended maintenance-free operation of mechanical components and systems.

Methodology

This section develops the digital twin approach for fatigue crack growth diagnosis, prognosis, and load profile optimization in order to achieve damage-tolerant fatigue crack growth in the component of interest while meeting required system performance over single or multiple missions.

Probabilistic damage diagnosis
Various methodologies based on visual inspection, passive sensing of mechanical waves, active sensing of mechanical waves, nonlinear ultrasonic wave modulation, time reversal, etc. have been investigated for the diagnosis of cracks in mechanical components [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]. In this work, we utilize two methods: a) high-resolution imaging, and b) ultrasonic-guided-wave-pitch-catch-based approach, for monitoring the fatigue crack growth in a thin metallic component [22, 23, 24, 25, 26, 27, 20, 28, 29, 30]. High-resolution imaging is used to obtain the true value of the crack length, whereas the ultrasonic pith-catch method is used for probabilistic damage diagnosis. To perform the guided-wave pitch-catch, we use a network of monolithic PZT (Pb(Zr − Ti)O3) sensors and actuators in the neighborhood of the crack.

Bayesian estimation and information fusion for damage diagnosis
A metallic component and the actuator-sensor network are depicted in Figure 2. The goal of the proba-


Figure 2: A thin metallic component and actuator-sensor network

bilistic diagnosis is to estimate the crack length using a physics-based, damage-sensitive feature of the sensed (voltage) signal. The methodology needs to be able to quantify the uncertainty in diagnosis due to various sources such as: physical variability, data uncertainty, and model uncertainty. Since Bayesian estimation methodology is well-suited for this purpose, we cast the problem of probabilistic diagnosis as a problem of Bayesian estimation of crack length. We express the uncertainty in our knowledge of the crack length by means of a probability distribution function. We assume a prior distribution (pprior) of the crack length based on intuition, experience, model prediction, etc., and update the knowledge using the data by computing the likelihood function (plhd) as:













165












170












175












180

ppost(a|ydata) ∝ plhd(ydata|a) ∗ pprior(a),	(1)
where a denotes the crack length, and ydata is the scalar, physics-based damage metric obtained from analyzing the sensed signal. In general, the data used for Bayesian estimation can come from different types and number of sensors. Different types of sensors involve measurement of different physical quantities as well as damage- sensitive data features (heterogeneous sources data), whereas multiple sensors of the same type involve measurement of the same physical quantity at different locations, or at different times (homogeneous sources of data).   We  fuse the information obtained from these data sources using sequential Bayesian updates   for different sensors, where the posterior of (say) damage severity obtained from all sensors used for the previous update is taken as the prior for the current update (using the measurements from the current sensor). Thus, the estimate of the current state of damage is obtained by fusing the information from multiple homogeneous data sources, and provides an estimate of the diagnosis uncertainty. We remark that the proposed methodology can be used to fuse information from heterogeneous sources of data as well, using the same Bayesian computational technique. We compute the posterior distribution of the crack length using a Markov chain Monte Carlo method (Metropolis-Hastings algorithm [31]). This method requires many evaluations of the likelihood function. Since the underlying actuation-wave propagation-sensing problem is a multi-physics problem, whose computational model consists of a large number of inputs and parameters, repeated evaluations of the likelihood function using high-fidelity physics models become computationally unaffordable. Next we first describe the computational model of the governing physics for damage diagnosis using Lamb wave pitch-catch sensing.

Numerical model of the governing physics
The governing physics for the problem of interest involves piezoelectric effect (Gauss’ law for electric- ity) and elastic wave propagation in isotropic, thin metallic components (balance of momentum). Multiple high-fidelity physics model runs to compute the likelihood remain time consuming for the proposed Bayesian diagnosis methodology. We thus replace them with inexpensive surrogate models to facilitate rapid computa- tion. A variety of techniques (e.g., neural networks, chaos polynomials, Gaussian process regression, etc.) are available to train a parametric relationship between the inputs and the output using a basic mathematical form (neural network, polynomials, random processes) and a set of outputs corresponding to known inputs (training data). Here, we use a Gaussian process (GP) surrogate model [32]. In order to generate training data for the GP model, we, first build a finite element model for the governing physics using a commercial finite element program (Abaqus [33]). We simulate piezoelectric actuation, Lamb wave propagation and piezoelectric sensing in an Aluminum plate. The plate contains a hole in the center to represent the initial damage, and the Lamb wave pitch-catch simulations are performed for all actuator-sensor paths for multiple cracks radiating outward from the hole. We record the electric potential (voltage) signal for all sensor loca- tions. We compute the spectrogram of the recorded signals and extract the variation of the spectrogram in time for a few (central) frequencies. Using dispersion analysis for the Lamb wave propagation in the plate, we identify the part of the spectrogram corresponding to the first symmetric (S0) mode wave packet. We choose the ratio of the (scatter) energy for the first symmetric (S0) mode wavelet in the sensed signal as the damage index (DI). That is,

DI(a) = S0e(a) ,	(2)
S0o

where S0e is the scatter energy in S0 mode for a given crack length (a),  and S0o is the scatter energy in  S0 mode for the initial flaw (hole). The values of the damage index for different crack lengths (DIfem(a)),  for all actuator-sensor paths are computed using the finite element model and mean values of the model
185 parameters. These do not include the effects of model parameter variability, (physics) model form error, and measurement noise. The mean estimates are then corrected by collecting experimental data that yields an estimate of the overall model error.

Estimation of model errors
To estimate the overall model errors (described above), we conduct two tests where an aluminum plate seeded with damage (a hole in the center) is subjected to cyclic loading at a constant stress ratio.  After       a fixed number of loading cycles, a high-resolution photograph of the region around the hole is taken to estimate the crack length, and a Lamb-wave pitch-catch test is performed for all actuator-sensor paths. The data obtained from the pitch-catch tests is processed to obtain measured damage index values for different frequencies and known crack lengths. The difference between the mean value of damage index predicted using the finite element model and its value measured in a laboratory test provides an estimate of the model error for a given crack length and the given actuator-sensor path. The model error for each of the above cases (crack lengths, actuator-sensor paths) is computed, and a GP surrogate model is built to express the dependence of the model error on the crack length a. This GP surrogate model (f corr(a)),  trained using  data obtained from the two preliminary laboratory tests, now includes the correction for the combined effect of model parameter variability, model form error, and measurement noise. It is able to predict the value of the damage sensitive metric (DIcorr(a)) and the associated uncertainty for a given value of damage (crack

length, a), that is,

DIcorr(a) = DIfem(a) + f corr(a),	(3)






190














195

for each actuator-sensor path, and all (central) frequencies of interest. This model (DIcorr(a)) is used in Bayesian parameter estimation (to estimate crack length, a), and to fuse the diagnostic information obtained from multiple actuator-sensor paths.

Probabilistic damage prognosis
The damage evolution phenomenon of interest is fatigue crack growth under uni-axial, cyclic loading. We use an analytical damage evolution model based on the assumptions of linear elastic fracture mechanics (LEFM) with small-scale plasticity, where the sub-critical fatigue crack growth due to the applied cyclic loading is estimated by calculating the crack growth rate as a function of stress-intensity factors (SIFs), and other model parameters.

Crack growth model
Many empirical formulas for fatigue crack growth prediction are proposed in the context of LEFM with small-scale plasticity, for example, Paris’ law [34], modified Paris’ law [35], Forman’s equation [36], the NASGRO model [37], etc. These models predict crack growth rate (da/dN ) as a function of the stress intensity factor range (∆K) and other model parameters. In this work, we use the Forman’s equation, which takes into the consideration the effects of the stress ratio (R) and the fracture toughness (Kc) on the crack growth rate. Thus, we model the crack growth rate using:

da	C (∆K)m
=
dN	(1 − R) · Kc − ∆K


,	(4)


where m and C are the model parameters that can be calibrated using data from experiments, and Kc can
200 be obtained from the experimental data reported in the literature.














205














210












215

Sources of uncertainty
The probabilistic prognosis methodology needs to incorporate the effect of following sources of uncertainty:
a) natural variability in loads, material properties, etc., b) measurement error in experimental data (used for the calibration of parameters C, m), c) diagnosis uncertainty in current crack size, and d) model uncertainty in fatigue crack propagation law, in SIF computation, discretization error in the finite element model, and surrogate model error (Figure 3). Note that, in the work discussed in this article, the natural variability in material properties, measurement error, crack length diagnosis uncertainty, as well as the surrogate model uncertainty in SIF computation are considered.

Physics model and surrogate model building
The underlying physics for damage evolution requires modeling the static equilibrium and stress concen- tration in a cracked mechanical component. We first build a finite element model of the component of interest with known initial flaw. Information regarding damage evolution, as well as mechanical work done by the applied cyclic loading can be extracted from this model. We use the finite element model to compute a) the SIFs corresponding to different load levels and different amounts of damage (i.e., crack size), and b) the work done by the applied loading history. We use the data to build GP surrogate models that estimate a) stress




Figure 3: Sources of uncertainty in probabilistic fatigue crack prognosis












220







































225

intensity factors, and b) work done for a given combination of load and crack size. The output of the GP models feed into the probabilistic damage prognosis and system optimization calculations, and ensure that sampling-based probabilistic analyses are performed in a computationally efficient manner. We remark that before performing probabilistic damage prognosis, we also use the GP surrogate model and laboratory test data to calibrate the parameters for the LEFM-based fatigue crack growth model. We utilize the calibrated model parameters and trained surrogate models for probabilistic prognosis and load profile optimization.

Estimation of physics model parameters
Before performing probabilistic damage prognosis, we conduct laboratory tests to estimate parameters that define the physics model. In our laboratory experiments, the component of interest is subjected to uni-axial, tension-tension loading for known R values and loads. The crack growth after a specified number of cycles is periodically measured using high-resolution imaging. We assume that the measurement error  is a Gaussian random variable with zero mean and known standard deviation.   If θ denotes the vector      of parameters to be calibrated, and aN denotes the crack length after N loading cycles obtained from a laboratory test, then the Bayesian update formula is given by:

ppost(θ|aN ) ∝ plhd(aN |θ) ∗ pprior(θ),	(5)
where we assume a prior distribution (pprior) based on intuition, experience, etc.; and update the knowledge using the data by computing the likelihood function (plhd). The calibrated fatigue crack growth model is used for probabilistic damage prognosis.


Bayesian network for probabilistic crack growth prognosis
The GP surrogate model, and the crack growth model (Forman’s equation) are used for cycle-by-cycle crack growth prediction. At each cycle, the minimum and maximum loads are known and the current crack size is inherited from the crack growth analysis of the previous cycle.  We use the loads and the current
230      crack size as the input of the GP model to predict the range of SIF (∆K).  We then use this value of ∆K
in Forman’s equation to predict the crack growth rate (da/dN ) and the crack size increment for the current cycle. The Bayesian network for probabilistic crack prognosis is illustrated in Figure 4.




Figure 4: Bayesian network for probabilistic crack growth prognosis







235














240












245

In summary, we first build a finite element model for the component of interest that calculates SIFs for different loading intensities and crack sizes. We build a GP surrogate model that estimates the SIF for given loading intensity and damage severity using the training data obtained from finite element analysis. We use an LEFM-based fatigue crack growth model (that uses the SIF estimated using the GP surrogate), and estimate its model parameters by conducting separate fatigue crack growth experiments on specimens similar to the component of interest. We use the calibrated model parameters and trained surrogate models for probabilistic prognosis and load profile optimization.

Load profile optimization under uncertainty
Many mechanical systems currently undergo calendar-based preventive maintenance (also known as planned or scheduled maintenance) [38, 39], i.e., the components in the system undergo maintenance based on a predetermined, fixed schedule. The alternative, condition-based maintenance (CBM) philosophy is more attractive due to its ability to consider the system state (inferred from diagnostic information) in deciding the requirement of maintenance operations. The information about the current system state (i.e., health and capability) can also be used to support intelligent mission planning as discussed in this article. To this end, we discuss load profile optimization aimed at ensuring that the damage growth during the next mission does not exceed a critical value, while ensuring that a required amount of work is performed by the system during the mission.
We consider block loading in this discussion. The optimization problem considers the (constant) ampli- tudes and durations of the blocks as the decision variables. They represent the intensity and duration for which a particular action is performed. Three different optimization strategies are possible. The first strategy aims to minimize the expected final crack size after each mission. This approach is similar to robustness-based optimization, and can be formulated as:



minimize
x∈Rnx

E[af (x, θ)],

subject to    E[g(x)] ≥ Wmin,
xlb ≤ x ≤ xub,

(6)

where x is the vector of deterministic decision variables, and θ is the vector of damage prognosis model param- eters, g(x) is the performance function that denotes the work done (described in Section 3.2.2), E[af (x, θ)] is the expected value of the crack size after each mission, and E[g(x)] is the expected value of the non-linear function that represents the required performance in the mission (e.g., the total amount of work done during the mission). Wmin represents the minimum amount of work that needs to be completed in each mission, whereas xlb and xub represent the lower and upper bounds, respectively, for the decision variables. The corresponding reliability-based design optimization seeks to minimize the probability of exceeding a (prede- fined) critical crack size (acrit). A crack size that necessitates repair can be chosen for this purpose. The optimization problem in this case can be cast as:


minimize
x∈Rnx

P [af (x, θ) ≥ acrit],

subject to    E[g(x)] ≥ Wmin,
xlb ≤ x ≤ xub.

(7)


We remark that the probability of failure Pf = P [af (x, θ) ≥ acrit] can be very low in the first few missions, and the probabilistic optimization process that uses Monte-Carlo sampling for computation of the failure probability may yield inaccurate results for this case. We thus employ a hybrid strategy where the first few missions aim to minimize the expected value of final damage, and the latter missions use reliability-based design optimization. The optimization problem for this third case can be stated as:


Earlier Missions
minimize
x∈Rnx ,θ∈Rnθ



E[af (x, θ)],

Latter Missions
minimize
x∈Rnx ,θ∈Rnθ


P [af (x, θ) ≥ acrit],



(8)

subject to	E[g(x)] ≥ Wmin,
xlb ≤ x ≤ xub.

subject to	E[g(x)] ≥ Wmin,
xlb ≤ x ≤ xub.


The transition from “earlier” to “latter” missions can be decided based on when the probability of failure (based on an approximate first-order calculation [40]) starts showing values that can be accurately captured by the limited number of samples employed in the prognosis and optimization. For example, in basic Monte Carlo sampling, the error in computation of the failure probability estimate can be obtained as [40]



sPf % = 200% ×

f
NMC × PT

,	(9)




250












255

where PT is the true probability of failure and NMC is the number of Monte Carlo samples. The above formula shows that if the failure probability is 0.01 and an error of 10% is desired, we need 39,600 samples. Based on the affordable number of samples and desired error, one can determine the failure probability threshold for transitioning from the first to the second optimization formulation. We remark that an objective function based on the probability of failure can also be used for the earlier missions. However, this may necessitate a large number of Monte Carlo samples resulting in high computational cost due to the usually low failure probability in earlier missions. Alternative reliability computation approaches like the first order reliability method [40] can be explored in this case.

Illustrative experiment






260














265
























































270

We demonstrate the proposed methodology by conducting laboratory experiments on an AL7075-T6 plate specimen.  The plate is seeded with damage (by means of a small hole and notch in the center) and   is subjected to uni-axial cyclic loading. The goal of the load profile optimization is to restrict the fatigue crack growth to be below a critical value while ensuring that a minimum amount of work is performed by the applied traction (in the loading phase). In the following sections, we discuss specific aspects of probabilistic diagnosis, prognosis and system optimization in relation to this illustrative experiment.

Probabilistic damage diagnosis
In this experiment, we perform fatigue crack diagnosis in the specimen using ultrasonic guided-wave pitch-catch technique.  The details of the aluminum plate and PZT actuator-sensor network are depicted  in Figure 5. We use a physics-based damage index calculated using features of the time-varying electric


Figure 5: AL7075-T6 plate (tplate = 0.81mm) and actuator-sensor network

potential measured by the sensors for damage diagnosis.

Numerical model of the governing physics and model errors


Figure 6: The finite element model for Lamb wave actuation, propagation, and sensing











275












280














285












290












295














300












305












310

The governing physics of the problem of interest requires multi-physics (piezoelectric effect, Lamb wave propagation in a plate) modelling. We use a commercial finite element program (Abaqus [33]) to carry out the numerical simulations. Figure 6 shows the basic set up of the model. We use three-dimensional continuum finite elements (C3D20) to model the plate and the adhesive used to bond the piezoelectric transducers to the plate. We utilize three-dimensional piezoelectric finite elements (C3D20E) to model the transducers. We intend to use a sampling-based algorithm to perform Bayesian diagnosis and information fusion. Thus, an efficient computational model that can provide the quantity of interest for a given sample of the parameters is needed. This is typically achieved by training a surrogate model (or a response surface) using the physics model. In this work, we use a finite element model to compute mean values of the damage indices for multiple actuator-sensor paths and for a range of crack lengths. The model errors corresponding to each actuator- sensor path are accounted for by a) obtaining laboratory test data that provides values of damage indices for different crack lengths, b) building a GP surrogate model for the error between damage indices predicted by the finite element model and laboratory tests. The GP model captures the combined contribution due to model parameter variability, model form error, and measurement noise for a given crack length.

Bayesian estimation and information fusion
We fuse the information obtained from different actuator-sensor paths in our Bayesian estimation algo- rithm. For example, if the estimation (equation 1) is performed for path A2S2 (Figure 5), then the updated posterior for the crack length can be used as a prior for the Bayesian estimation for the next path. The result at the end of the second Bayesian update is the result of the fusion of information contained in the data obtained from the two actuator-sensor paths. In our illustrative example, we perform the fusion of in- formation obtained from Lamb-wave pitch-catch performed along seven different actuator-sensor paths, viz. A2S2, A2S3, A2S1, A3S2, A1S2, A3S1 and A1S3 (see Figure 5). Thus, the posterior of crack length for our experiment contains the fusion of information from these seven actuator-sensor paths. We remark that in this work we used sequential information fusion. Our results show satisfactory performance for the experiments conducted in this work. Simultaneous information fusion can also be performed, if desired. In this technique, likelihoods for a candidate crack length for all paths can be considered simultaneously, to obtain a combined posterior. The estimation process is well suited to perform the fusion of information obtained from different sensors.

Probabilistic damage prognosis
A horizontal crack growing out of the hole in the center of the plate represents the initial damage, and increase in the crack length due to the applied cyclic loading represents damage evolution. Thus, we are concerned with crack propagation in an aluminum plate with an initial flaw under uni-axial, cyclic loading. The specimen is a 0.38m × 0.15m × 0.81mm, AL7075-T6 plate with a hole in the center and an initial notch parallel to the 0.15-m-edge, as shown in Figure 7(a). For probabilistic crack prognosis, mode I crack propagation is considered under the uni-axial, tension-tension loading. We create a two-dimensional finite element model in Abaqus [33] (assuming plane stress conditions) to reflect the laboratory test conditions: one edge fixed and the other edge loaded along the 0.075-m-wide region at the center (Figure 7(b)), and compute SIFs for different crack lengths and loads using the contour integration technique. As demonstrated by the partition lines in Figure 7(b), a structured mesh is enforced within the contour integration areas and outside the central region, and an unstructured mesh is defined elsewhere (Figure 7(c)). Quadrilateral elements with eight nodes (CPS8) are used in the finite element model.  Obtaining the SIF using a finite element model  at each cycle in the cycle-by-cycle analysis is computationally expensive for sampling-based, probabilistic


		
(a) AL 7075-T6 specimen showing the hole in the (b) Basic geometry and boundary (c) Finite element mesh near the

center and the notches made with a sharp tool

conditions for the finite element initial flaw and crack
model


Figure 7: AL 7075-T6 specimen and the finite element model used to estimate SIFs and work performance constraint







315














320

fatigue crack growth prognosis. To expedite the process of SIF computation, we use a GP surrogate model that accepts the load and the current crack length as inputs,  and provides the SIF as the output.  Thus,   the training points for the GP model consist of a two-dimensional (load, crack length) vector and one- dimensional-response (SIF). We perform a series of finite element model runs with different combinations of crack sizes and loads to obtain the training and testing data sets.

Model parameter estimation
We use Bayesian calibration to infer the probability distributions of the model parameters using exper- imental data. As stated in Section 2.2.4, the uncertainty in the Forman’s equation is represented by the probability distributions of the model parameters, C and m. The vector of parameters to be calibrated, θ,
in this case is [C, m].  Based on the thickness of the plate, Kc = 67 kPa√m [41].  We use the test data (with
measurement uncertainty) to calibrate this set of parameters as described by Equation 5 in Section 2.2.4.

Work performed by the applied loading


Figure 8: Nodal force and displacement on the top edge of the finite element model

Ensuring that the maintenance-free operation period for the mechanical component is extended while the

component completes the required operational tasks is a crucial part of a successful system reconfiguration methodology. Thus, we need a metric to measure the performance of the component in question (the alu- minum plate under cyclic loading). Without loss of generality, we choose the work done during the loading phase of the cyclic loading as the required performance metric. The work done by the applied tensile load F is calculated as follows. The nodal forces T on the top edge (Figure 8) can be approximated using the applied load, as:

F
T =
(Nnodes − 1)

,	(10)

where Nnodes denotes the number of loaded nodes in the finite element model. For the given load F and crack length a, the vertical displacement at the i-th node on the top edge of the finite element model is estimated as ui(F, a). Using the nodal displacement, the work done can be computed as:

 	F	Σ



Nnodes −1	Σ

 


Note that in equation 11, the dependence of the displacement on material properties of the plate and other model parameters is suppressed for brevity. In order to reduce the (computational) cost for calculating the work done for a given combination of the load (F ) and crack length (a), we train a GP surrogate model (WGP(F, a)) that outputs the work done given F and a (using data obtained from finite element simulations with different load levels and crack lengths). For a fatigue loading cycle with load ranging from F1 to F2, assuming the crack length remains constant within the cycle, the work done during the loading phase of a cycle can be obtained using this GP surrogate as:

Wcycle = WGP(F2, a) − WGP(F1, a).	(12)



325














330












335

For a given mission, the work output is calculated for each cycle using the GP model, and the (known) crack length as well as the load during that cycle. The work done in all cycles is added to obtain the amount of work done during the mission. In this manner we use the GP model to evaluate the performance constraint in the load profile optimization problem.

Load profile optimization
We assume that a given vehicle action or maneuver is associated with a characteristic (cyclic) load level range, and we seek the optimal magnitude as well as the optimal duration of the load (intensity and duration of the action) to ensure: a) that the damage growth is below a specified threshold, and b) work performed is above a required minimum. We perform the optimization offline, and to simulate vehicle usage, we conduct laboratory tests on aluminum plates using the cyclic loading specified by the optimizer. The key assumptions of the load profile optimization problem are listed below:

We aim to ensure maintenance-free execution of a fixed number of missions of the system. (This is particularly important in situations where maintenance resources may not be available until after one or more missions).
We define the missions through block loadings; thus each mission is divided into a set of load blocks,
340	which might represent a corresponding set of actions maneuver during the mission. Each block is characterized by specified minimum and maximum load levels (Figure 9). We also set the minimum and maximum duration for each maneuver for each mission. For all load blocks, the stress ratio, R =

0.5. The methodology is capable of considering variability in applied loading, however for the illustrative example discussed in this article, the variability in loading is not considered.
345	3. We assume that repair is required when the damage in the component exceeds the critical crack length (acrit).

Cycles

Figure 9: Block loading pattern assumed for each mission









350












355












360












365
















370

In our laboratory tests, we initiate a damage (crack growth) in the aluminum plate by subjecting it to a uni-axial, cyclic loading at fixed minimum and maximum tensile loads. We assume that the component (plate) has to complete four missions, and each mission has three loading blocks. (For example, considering a component in a flight vehicle, these three blocks could represent traveling to a desired location, then performing the required action, then traveling back to the base). Each block is characterized by the limits (minimum and maximum) on the tensile load, and duration for which the load acts. The bounds on load magnitudes and duration corresponding to each maneuver, as well as the work done are used to define suitable inequality constraints in the optimization problem. The assumed block loading pattern thus defines a family of load histories for a given mission (task) using a few parameters. This is an important feature that allows extension of this approach to more general (fully variable load) scenarios. The general scenario will necessitate a predictive, parametric model that is able to map missions/tasks to (a family of) loading histories. The parameters of this model can be optimized in lieu of the parameters that define the simple block loading used in this work. Thus, the assumed (simple) load profile, retains a key feature (parametric representation) of a more general (fully variable) load case. In the case of a more complicated load history, time series modeling techniques such as auto-regressive moving average (ARMA) modeling can be used to build a parametric model of the load history for different operational missions (tasks).
The goal of the load profile optimization is to minimize the probability of exceeding the critical crack size
at the end of the fourth mission while satisfying other constraints. We use a surrogate-based optimization framework [42] to perform the optimization. The surrogate model for the optimization can be regarded as an approximation model for the expensive objective function computation that requires sampling.

Results and discussion

In this section, we first discuss results of surrogate model training and model parameter estimation required for probabilistic diagnosis and prognosis. Next we discuss the results of load profile optimization for a laboratory experiment. In the experiment, the probabilistic diagnosis is performed using ultrasonic guided- wave pitch-catch data at the beginning of each mission. The value of crack size obtained from probabilistic

diagnosis, and the associated uncertainty are passed on to the load profile optimizer to design the optimal loading profile for the mission. The optimal loading is applied to the component in a universal testing machine (UTM). This process is repeated for all four missions. In this manner, the experiment is used to illustrate
375 the integration of probabilistic diagnosis, prognosis, and load profile optimization.

GP surrogate models for probabilistic diagnosis

Component	Properties	Component	Properties

AL 7075-T6	Ep = 71.7	109 Pa		Piezoelectric	Exx = Eyy = 54	109 Pa Plate	νp = 0.3327		actuators and	Ezz = 74	109 Pa
ρp = 2810 kg/m3	sensors	ρ = 7800 kg/m3 Adhesive	Ea = 2.6 × 109 Pa		νxy = νxz = νyz = 0.28
νa = 0.3	Gxy = Gxz = Gyz = 21 × 109 Pa
ρa = 1100 kg/m3	d14 = d36 =  670  × 10−10  m/V d21 = d23 = −210 × 10−10  m/V d22 = 500 × 10−10 m/V
e11 = e22 = e33 = 1.8593 × 10−8 F/m
Table 1: Material properties used in the (high-fidelity) finite element model









380












385












390














395












400

We build a three-dimensional, finite element model for a 0.81 mm thick aluminum plate (Figure 6) with PZT-5J transducers as actuators and sensors. We use a Hanning-modulated, three-cycle long sine pulse (central frequency 200 kHz) to define the voltage signal that excites the actuator. We run the model using mean values of model parameters depicted in Table 1, and for six different values of the length of the crack growing out of the hole in the center (a = {1.8, 5, 10, 15, 20, 25} mm). We record the output voltage time series for each of the actuator-sensor paths, for all crack lengths. We then compute the spectrogram-based energy metric [25] for S0 wavelet at 300 and 250 kHz. We treat the ratio of S0 wave energy (S0e) at damaged and undamaged state as the damage index (equation 2). We build a simple regression model to represent damage index vs. crack length (based on FE results) as shown in Figure 10. We correct this regression model with experimental diagnosis data (separate from the mission optimization experiments and the Forman equation parameter estimation experiments); and that correction term is represented by a GP model. A squared exponential covariance function (with two hyperparameters) is used for the GP model. The performance of the correction GP model is tested using data from a third diagnostic test (wherein the damage index and the crack lengths are known). The results of the numerical simulations, calibration of model error GP model, and the results of validation tests for (central) frequency 300 kHz and path A2S2 are shown in Figure 10. The validation tests show that the corrected GP model is able to estimate the damage index for a given crack length with sufficient accuracy, and can be used for damage diagnosis.

GP surrogate models for probabilistic prognosis (SIF and work performance computation)
We perform 144 different finite element model simulations, which cover the combinations of loads from 1000 lbs to 8000 lbs with a 1000-lb increment, and crack sizes from 5 mm to 90 mm with a 5 mm increment. The combinations cover the whole range of the laboratory test conditions.  Before training the GP model  to be used for prognosis,  we  investigate the effect of the size of the training data on the accuracy of the  GP surrogate models. Using 100 or 130 points for training, the mean absolute errors of all 44 or 14 testing points are found to be less than 2%. Thus, the number of training points appears to provide high, converged accuracy for the surrogate model. We choose the training data consisting of results from 130 finite element simulations (130 training points) to build the GP model with a squared exponential covariance function (with


1.6


1.4


1.2


1


0.8


0.6

4	6	8	10	12	14	16
a [mm]

Figure 10: Damage index values computed using the finite element model (physics model), the damage index data obtained using experiments (tests 1 and 2), the corrected damage index model (including the GP for model error), and validation test data for (central) frequency 300 kHz and path A2S2. It can be seen that the corrected model matches the validation test data fairly well. Similar procedure is followed for both frequencies for all actuator-sensor paths.







405












410














415












420












425

three hyperparameters, two of which are the separate length scale parameters for the force and crack length input). The trained GP model is used to predict the SIF for different crack sizes and loading cases. For the chosen GP model (130 training points), the average (absolute) error between the true SIF values and the mean of the estimated SIF values for the testing data is 1.19%, indicating the surrogate model predictions are sufficiently accurate. We train another GP surrogate model (using data from 130 training points) that outputs the work done given the loading and the crack length using the results from the same set of finite element simulations. Again, we choose a squared exponential covariance function with three hyperparameters for this model. For a given mission, the work output is calculated during the loading phase for each cycle using this GP model, and the work done in all cycles is added to estimate the amount of work done during the mission. We use this GP model to evaluate the performance constraint in the load profile optimization problem.

Crack growth model parameter estimation
The specimens used for fatigue tests were seeded with damage by punching a hole in the center of the plate and by notching the periphery of the hole at two diametrically opposite locations using a sharp metal tool (see Figure 7(a)). An effort was made to notch two diametrically opposite points on the periphery of the hole such that the diameter connecting them ran parallel to the width of the specimen. Due to the variability in this process (of notching the specimen manually), the number of loading cycles needed to reach the crack length of about 5 mm for identical loading histories were different for different tests (by a about a few thousand cycles). In order to minimize the effect of this variability on parameter estimation and load profile optimization, we use the data for fatigue crack growth beyond about 5 mm. That is, we assume that the specimen has an initial flaw in the form of a 5-mm-long crack growing out of a hole (diameter about 1.8 mm). The growth of the fatigue crack beyond this initial crack size is used for parameter estimation. The data for parameter estimation is obtained from three constant-amplitude uni-axial, tension-tension, cyclic loading tests with Fmax = 5000 lbs, Fmin = 2500 lbs. Initially, the cyclic (tension-tension) loading is applied


30


25


20


15


10


5


0
0	1000	2000	3000	4000	5000	6000	7000	8000	9000	10000
Numbers of Cycles, N

Figure 11: Laboratory test data used for calibration of parameters C, m











430












435












440












445

till the crack length grew to about 5 mm. The initial crack length is recorded (using high-resolution imaging) and loading is continued in sets of 2500 cycles. Crack sizes are measured after every 2500 cycles of the loading. The resulting crack growth vs. loading cycles plots for the three tests are shown in Figure 11.
We use the Metropolis-Hastings algorithm [31] of Markov chain Monte Carlo (MCMC) sampling to com- pute the posterior distribution (ppost) of θ = [C, m]. As discussed in Section 3.2.1, we choose a uniform distri-
bution as the prior distributions for C (C ∼ Unif(9×9−9, 1.7×10−8) m/cycle) and m (m ∼ Unif(3.05, 3.21)).
The proposal distribution for C is chosen to be log-normal distributions centered at the current point of C with a standard deviation of 50% of the current C value.   This particular proposal has two  advantages:     it ensures that negative values are not proposed, and the large standard deviation ensures that the gener- ated samples cover  as much sample space as possible.  The proposal for m is a uniform distribution that  is independent of the current point. During MCMC calibration, for each proposed θ, a crack growth curve (crack length vs. number of fatigue loading cycles) is calculated using Forman’s equation.  The likelihood  of the difference between the predicted crack lengths and the actual crack lengths (laboratory tests) at the recorded cycle counts is calculated using a zero-mean normal random variable with 0.008 m standard devia-
tion (sobs ∼ N (0, 0.008)) m as the measurement error. Twenty thousand posterior samples were drawn using
MCMC. After rejecting the first few samples as burn-in samples, the last 10,000 samples yield a mean value of 1.15 × 10−8 m/cycle, and a coefficient of variation of 0.1154 for C, a mean value of 3.17, and a coefficient of variation of 0.0143 for m. These values are used in the subsequent analysis (probabilistic prognosis and
optimization under uncertainty).


Load profile optimization: laboratory experiment
In this section, we discuss laboratory experiments that demonstrate how probabilistic damage diagnosis (Lamb wave pitch-catch), probabilistic damage prognosis, and load profile optimization can be used to restrict crack growth in the laboratory test specimen while ensuring a minimum amount of work is performed within
450 the maximum allowable operation time. This process is a surrogate for extended maintenance-free operations of mechanical components.  The component used in laboratory experiment is expected to perform four










455












460












465

missions (tasks) while satisfying performance and damage growth requirements. We begin each experiment by subjecting the specimen to cyclic loading with a constant amplitude. This ensures that the specimen has some initial damage (crack length of about 5mm).
The load profile optimization for the four missions involves minimization of expected value of the final crack length and reliability-based optimization formulations (equation 8). The minimum work (Wmin) to be
performed is estimated using the mean work done W ∗. W ∗ is computed using the mean values of prognosis
model parameters and mean values of upper and lower bounds of the cyclic block load amplitudes. We use W ∗ as the minimum work (Wmin) to be performed during the four missions. In addition, the optimization for the last two missions requires a critical crack size acrit. For this experiment, the critical crack size was considered to be acrit = 15 mm for the last two missions. We remark that since the crack growth is monotonous, any high-enough crack size can be used as critical crack size. However, if acrit is too large compared to the actual
crack size, then a sampling-based computation of probability of failure may become challenging. In real-world applications the critical damage severity can be decided considering the degradation of system performance with damage growth, and desired minimum system performance. The bounds for the design variables and the minimum work done constraint for each mission are given in Tables 2 and 3.



Lower bounds	Upper bounds

Table 2: Lower and upper bound constraints on the number of cycles for each maneuver

Lower bounds	Upper bounds
Cases	Fmax,1	Fmax,2	Fmax,3	Fmax,1	Fmax,2	Fmax,3	Wmin
[lbs]	[lbs]	[lbs]	[lbs]	[lbs]	[lbs]	[J]
Mission 1	3000	4500	4000	4000	5500	5000	1.582867 × 104
Mission 2	3000	4500	4000	4000	5500	5000	3.165734 × 104
Mission 3	3000	4500	4000	4000	5500	5000	1.978584 × 104
Mission 4	3000	4500	4000	4000	5500	5000	1.187150 × 104

Table 3: Lower and upper bound constraints on the fatigue block load amplitude for each maneuver and the minimum work done constraint for each mission









470












475

The results of probabilistic diagnosis are shown in Figures 12 and 13. The diagnosis results were obtained using Markov-chain Monte-Carlo method (Metropolis-Hastings algorithm [31]) . For each actuator-sensor path, the damage indices for the two frequencies (300 kHz and 250 kHz) of interest are computed spectrogram of the sensed voltage output. Then a sequential Bayesian calibration is performed with a Gaussian prior to calibrate crack length. A Markov chain of 105 Monte Carlo samples is constructed and the initial 10000 samples are rejected (initial burn-in samples). Uniform distribution is used as the proposal distribution for the crack lengths. Figure 12 depicts the results of the Bayesian information fusion. It can be seen that the damage diagnosis methodology estimates the damage severity (crack length) and the associated uncertainty using (homogeneous) information from multiple sources.
The results of the load profile optimization are shown in Table  4.  The expected value  of the work   done (E[g(x)]) is greater than Wmin for all missions. Thus, the performance constraint was satisfied for all missions. As discussed in Section 2.3, for the first two missions, the optimizer aimed to minimize the expected


0.35	0.35


0.3	0.3


0.25	0.25


0.2	0.2


0.15	0.15


0.1	0.1


0.05	0.05



0
0	5	10	15	20
Crack length (a) [mm]


0
0	5	10	15	20
Crack length (a) [mm]

(a) Probabilistic diagnosis to quantify the initial crack size (b) Probabilistic diagnosis to quantify the crack size at the
end of mission 4

Figure 12: Bayesian information fusion for probabilistic diagnosis: the figures show sequential information fusion for different actuator-sensor paths, and how the diagnosis methodology estimates crack size

0.6

0.5

0.4

0.3

0.2

0.1

0
0	5	10	15	20
Crack length (a) [mm]

Figure 13: Results for probabilistic diagnosis using ultrasonic guided-wave pitch-catch





480












485

value of the final crack size. We have not reported the probability of failure for these missions. Note that once the optimal load profile is known, the probability of failure for these two cases can be computed using probabilistic damage prognosis discussed in Section 3.2. For the last two missions, we aim at minimization of failure probability and report its value for the optimal loading. The final crack growth for the optimal loading case is lower than the critical crack size (acrit). Thus, the reliability-based optimization methodology is successful in arresting the damage growth below the specified threshold.
The optimal load profile values for some of the missions coincide with the lower bounds specified in Table 3. This result can be explained as follows. The crack growth law provided to the optimizer (Forman’s



Table 4: Optimal design variables for maintenance-free operation period; maximum fatigue block loading amplitudes for each mission, crack size test data atest at the end of each mission, critical crack sizes acrit, the probability of failure Pf for the last two missions, and E[g(x)] (the expectation of the nonlinear function that estimates the work done) using the optimal design variables









490












495

equation) suggests that the rate of crack growth is approximately equal to the m-th power of the change    in SIF. In LEFM, the SIF is directly proportional to the stress concentration at the crack tip, which is approximately proportional to the applied loading (for a fixed crack length). The optimizer implicitly infers that the rate of crack growth is (approximately) proportional to the cube of the applied load (as m ≈ 3). The work done (the performance requirement), on the other hand, is computed using elastic deformation of the plate under the applied load. Hence, it varies (approximately) as the square of the applied load. It may thus be advantageous to allow more cycles at a lower load level, to minimize the crack growth while ensuring that the work requirement is satisfied. Note that the crack sizes used in the experiment are only for the sake of illustration; using larger cracks helped us to speed up the experiments, and we do not expect that real-world mechanical systems (e.g. aircraft) would be allowed to operate (fly) with such crack sizes.


22

20

18

16

14

12

10

8

6

4

2
0	2000	4000	6000	8000	10000	12000
Number of Cycles, N

Figure 14: Predicted crack growth and uncertainty bounds (µ σ) for the optimized load profile (obtained by performing probabilistic damage prognosis), crack growth estimated using probabilistic damage diagnosis (mean standard error),  and  actual crack growth (obtained using high-resolution imaging of the test specimen)

Figure 14 shows how the model predictions are corrected using probabilistic diagnosis information after each mission. It can be seen in Figure 14 that the probabilistic diagnosis reduces the uncertainty in the knowledge about the current state of damage (crack length) at the end of each mission.   This effect is
500 particularly pronounced at the end of the (longest) second mission. The estimate of crack size obtained using probabilistic diagnosis at the end of the i-th mission is fed to the optimizer as the initial crack estimate at the start of the (i + 1)-th mission. We also report the true crack size (obtained using high-resolution imaging) at

the end of each mission. The error in estimate obtained from probabilistic diagnosis, and actual crack size is expected in any real-world system. In spite of not knowing the true crack size, the optimizer was able to
505 direct the missions while attaining the work requirement and minimizing the crack growth.


Conclusion











510












515













520













525













530













535
















540

In this article, we developed a digital twin approach for performing mission optimization under uncertainty aimed at ensuring system safety with respect to fatigue cracking. This is achieved by designing mission load profiles for the mechanical component such that the damage growth in the component is minimized, while the component performs the desired work. We considered three key aspects of condition-based mission design: probabilistic damage diagnosis, probabilistic damage prognosis, and mission optimization under uncertainty. The digital twin approach fused multi-physics multi-fidelity models with sensor data and previous history, and considered aleatory as well as epistemic uncertainty in both diagnosis and prognosis. We explored a hybrid formulation for load profile optimization that combined crack growth minimization with a reliability-based approach. With the help of an illustrative experiment, we showed that the proposed digital twin approach can be successfully used to perform mission optimization to achieve the desired system performance goal while maintaining safety.
The following improvements are needed to the proposed digital twin framework to enable its successful implementation for real-world mechanical systems:
Probabilistic damage diagnosis: a) utilization of heterogeneous data sources and corresponding di- agnostic models into the diagnostic framework, b) estimation of damage severity as well as damage location (currently we assume the damage location to be known), and c) application of the diagnostic methodology for complex geometries.
Probabilistic prognosis: a) accounting for complex geometries, multi-axial loading and complex degrada- tion mechanisms for real-world mechanical components, b) utilization of fully-variable loading histories (instead of block loading used in this work).
Load profile optimization: a) generation of an operation-to-load map that defines loading patterns (families) for various operational regimes, b) parametrization of loading regimes and classification of parameters that define the loading patterns given an operational regime, c) optimization of system oper- ations in the space of the loading parameters while considering the diagnosis and prognosis uncertainty for complex damage growth patterns, component geometries, and boundary conditions.

The methodology discussed in this article could potentially be extended in the future to decide a) damage- adaptive, resilience-enhancing maneuvers for aerospace vehicles, and b) mission profiles that prolong the maintenance-free operation period. The former type of application requires on-board sensing, whereas the latter application could be based on ground inspection. The framework can accommodate on-line damage diagnosis to decide future vehicle maneuvers using the most up-to-date information at the current time in a vehicle during flight.

Acknowledgement

This study was partly funded by a cooperative agreement with the U.S. Army Research Laboratory’s Vehicle Technology Directorate (Director: Dr. Jaret Riddick, Grant No. W911NF-17-2-0159). The support

is gratefully acknowledged. Valuable help from Garrett Thorne (Staff Engineer I) for preparing the test specimens and assistance by undergraduate students Michael Davis and Vamsi Subraveti in conducting the experiments is gratefully acknowledged. The experiments were conducted at Vanderbilt Universitys Labora- tory for Systems Integrity and Reliability (LASIR). The authors also thank Dr. Tzikang Chen at U.S. Army
545 Research Laboratory for valuable discussions.


"
P67LATF4;<pad> implementing the digital twin vision in AFMS design hinders by a conceptual theory and a sound technique basis. a theoretical framework as well as four key enabling techniques are detailed in section 3. the proposed DT-driven RID is extended into validating of the dynamic execution mechanism of AFMS.</s>;"
Digital twin-driven rapid individualised designing of automated flow-shop manufacturing system
Qiang Liu, Hao Zhang, Jiewu Leng & Xin Chen
To cite this article: Qiang Liu, Hao Zhang, Jiewu Leng & Xin Chen (2018): Digital twin-driven rapid individualised designing of automated flow-shop manufacturing system, International Journal of Production Research, DOI: 
To link to this article:  

Full Terms & Conditions of access and use can be found at 

International Journal of Production Research, 2018 
Digital twin-driven rapid individualised designing of automated flow-shop manufacturing system
Qiang Liua,b, Hao Zhanga, Jiewu Lenga,b* and Xin Chenb
a	b
Key Laboratory of Computer Integrated Manufacturing System, Guangdong University of Technology, Guangzhou, China; State
Key Laboratory of Precision Electronic Manufacturing Technology and Equipment, Guangdong University of Technology, Guangzhou, China
(Received 14 November 2017; accepted 25 April 2018)
Under a mass individualisation paradigm, the individualised design of manufacturing systems is difficult as it involves adaptive integrating both new and legacy machines for the formation of part families with uncertainty. A systematic virtual model mirroring the real world of manufacturing system is essential to bridge the gap between its design and operation. This paper presents a digital twin-driven methodology for rapid individualised designing of the automated flow-shop manufacturing system. The digital twin merges physics-based system modelling and distributed semi-physical simulation to provide engineering solution analysis capabilities and generates an authoritative digital design of the system at pre-production phase. An effective feedbacking of collected decision-support information from the intelligent multi-objective optimisation of the dynamic execution is presented to boost the applicability of the digital twin vision in the designing of AFMS. Finally, a bi-level iterative coordination mechanism is proposed to achieve optimal design performance for required functions of AFMS. A case study is conducted to prove the feasibility and effectiveness of the proposed methodology.
Keywords: digital twin; manufacturing system design; rapid individualised designing; semi-physical simulation; cyberphysical systems
1. Introduction
Under various new national advanced manufacturing strategies, such as Industry 4.0 (Moeuf et al. 2018), Industrial Internet, and Made in China 2025, the number of newly designed Automated Flow-Shop Manufacturing System (AFMS) is boosting to achieve smart manufacturing in both developed and developing countries. An AFMS is an automated system converting raw material, typically employing flow-type machines, into a finished product. The design of each AFMS presents distinctive individualised characteristics due to the diversification of workshop venue, difference of the production capacity, constraints of construction cost and integration of various legacy equipment (Gu, Hashemian, and Nee 2004; Koren and Shpitalni 2010). Dealing with the rapidly changing customer demands requires a fast and systematic AFMS design methodology. Failure to design a robust, adaptive and sustainable manufacturing system will lead to high operating costs and early deteriorating (Cherkaoui, Huynh, and Grall 2018).
AFMS design essentially includes finding optimal number of machines in each manufacturing cell, devising optimal buffer or inventory management strategies, designing layout and work-in-process (WIP) flow for efficient production (Shardaa and Banerjee 2013). A systematic designing of AFMS suffers from three major problems. First, since the mass individualisation paradigm deeply affects manufacturing logic (Gao et al. 2011; Jiang et al. 2016), the flexibility of AFMS design has to be improved so downtime and delay can be prevented (Yang et al. 2015). Second, the designers need to find optimal configurations by evaluating multiple design variables (e.g., equipment selection, control scheme and WIP flow) and considering different forms of manufacturing uncertainties (e.g., machine breakdowns, processing times and product demands). Third, unlike mass production paradigm that over-emphasises the system performance metrics on labour efficiency and unit cost of an operation, it is a challenge for balancing cost with sustainability from the aspects of material, energy and waste flows in mass individualisation paradigm (Cochran, Foley, and Bi 2017). The key point underlying these issues is how to effectively connect the design and operation of manufacturing system.
Realistic machining process models are essential to enable the early and efficient assessment of design decisions on both the quality of products and the performance of system. Computer-aided simulation tools play a significant role as it

*Corresponding author. Email: jwleng@gdut.edu.cn
© 2018 Informa UK Limited, trading as Taylor & Francis Group
can deal with validation of manufacturing systems. The designing of AFMS typically involves the understanding of the complex coupling relationships among processes and equipment, which makes it hard to design by using the traditional models (Leng and Jiang 2017a). One newly emerging and promising approach to deal with such complexity is digital twin (DT). With faster algorithms and amount of available data (Leng and Jiang 2016, 2017b), the digital twin (DT) can achieve interoperation and fusion of the physical world and the cyber world of manufacturing. Using a real-time digital copy of physical system (Grieves 2014) can predict performance of multiple designs earlier, reduce reliance on expensive and multiple physical testing, optimise for maximum performance and cut-down design time (Zhong et al. 2015).
However, implementing the digital twin vision in AFMS design hinders by a conceptual theory and a sound technique basis including the synchronising of cyber-physical and the optimising of complex manufacturing systems design at multiple granularity. To bridge above gaps, a new model of DT-driven rapid individualised designing (RID) of AFMS is proposed. Massive digital twin reference models are established for AFMS design in the pre-production phase and later inherited for inspection preparation, uncertainty quantification, process control of established models in production phase. A real-time synchronisation between the digital world and the physical world is established to form closed control loops at multiple granularity, and thus enable remote semi-physical simulation and virtual testing. Also, an effective feedbacking of collected decision-support information from the intelligent multi-objective optimisation of the dynamic execution is presented to boost the applicability of the digital twin vision to design of AFMS. Finally, a bi-level iteration coordination model is proposed to achieve optimal design performance for required functions of AFMS.
The outline is organised as follows. Based on a literature review in section 2, a theoretical framework as well as four key enabling techniques are detailed in section 3, including initial rapid individualised designing based on reference models, real-time cyber-physical synchronisation and distributed semi-physical integration, multi-objective optimisation of dynamic execution, and bi-level programming between static configuration and dynamic execution. A rapid individualised design platform for AFMS is developed and validated via a case study of sheet material production line in section 4. Then, how the proposed DT model benefit RID of AFMS is discussed. Section 5 and 6 present the discussion and summarisation, respectively.
2. Related works
Driven by customers’ differentiation demands, the manufacturing system design (MSD) is a process of selecting, configuring, and transforming based on variant reference models to form a new one (Tseng, Jiao, and Wang 2010). The effort can be categorised into modelling and optimisation.
Conventional MSD study usually focuses on the system modelling aspect. Typical modelling methods used in the MSD include Axiomatic Design (AD) (Suh 1995), Graph with Results and Actions Interrelated (GRAI) (Guy 1989), Structured Analysis and Design Technique (SADT) (Santarek and Buseif 1998), Integrated computer-aided manufacturing DEFinition (IDEF) (Cheng-Leong, Pheng, and Leng 1999), EXPRESS-G/STEP-CNC (Xu, Wang, and Rong 2006) and Petri Net (Li, Dai, and Meng 2009). Moreover, integration modelling with other techniques such as virtual reality, data envelopment analysis, fuzzy logic, multi-agent systems and complex network analytics (Leng and Jiang 2018) are found as emerging ways. For instance, Yang et al. (2015) presented a collaborative design platform for manufacturing systems, which enables a holistic use of virtual factory tools at various levels. Cochran, Foley, and Bi (2017) followed the principles of manufacturing system design decomposition to make effective cost and production system design decisions. These modelling-based methods mainly focus on the definitions of functional requirements, design parameters and process flow of a manufacturing system, which provide solid theoretical foundation to generate and evaluate design solutions of manufacturing systems. However, little attention has been devoted to the coupling relationship among different elements, and no single model is suitable for addressing all types of problems in MSD.
With the development of information technology, many scholars conducted the MSD research from the system optimisation perspective. Typical optimisation methods used in MSD include machine learning, artificial neural networks, and artificial intelligent algorithms (e.g., genetic algorithm, ant colony optimisation and particle swarm optimisation algorithm). Meta-optimisation can approximate the unknown input-output function implied by the underlying simulation to provide robust MSD decision-making support. Different from the static product design methods, the individualised MSD of AFMS will extend to the dynamic execution and operation mechanism, and the key is to perform optimisation existing in the coupling modelling. Chattopadhyay et al. (2013) proved that artificial neural networks are viable tools for stochastic simulation metamodelling through an example of one job shop system. Mohapatra et al. (2015) established multi-objective optimisation model and solved it via modified non-dominated sorting genetic algorithm. The multiple coupling relationships in MSD increase difficulty in identifying an optimal solution from a more holistic view.
With the comprehensive physical and functional describing ability (Tuegel et al. 2011), the DT (Digital Twin), which was firstly adopted in spacecraft design by NASA (Boschert and Rosen 2016; Brenner and Hummel 2017; Ferguson, Bennett, and Ivashchenko 2017; Grieves 2014; Grieves and Vickers 2017), can be viewed as a combination of modelling-based method and optimisation-based method. Simulation and seamless transferring of data from one life cycle phase to the subsequent phase are the core of the digital twin vision (Alam and El Saddik 2017; Derberg et al. 2017). It is a promising approach by incorporating intelligent optimisation algorithms into digital twin model of MSD.
However, current approaches of implementing the DT vision usually concentrate the synchronisation between the digital and the physical world to establish closed-loop control of a product or system. The rapid individualised design of AFMS not only includes the function configuration and layout, but also involves the complex dynamic adaptability of execution system (Li et al. 2017). This will result in insufficient possibilities of the on-line optimisation in DT model, since the pursuit of a systematisation modelling of AFMS exerts a lot of difficulty. Also, it lacks a systematic basis for decoupling and optimising of complex AFMS.
3. Framework and key techniques of DT-driven designing of AFMS
Considering the underlying closed-loop feedback mechanism between static configuration and dynamic execution of manufacturing system, this paper puts forward a kind of ‘iterative optimization between static configuration and dynamic execution’ design logic based on DT.
As shown in Figure 1, once the individualised static design (e.g., assembly solutions, motion script, and control scheme) is formed, it will be fed into the execution engine for further setting of execution mechanism and optimising of operation mode. If it can’t meet the target requirements, it will be returned to the superior individualised static configuration design, and then fed into the next round simulation to decide the operation mode and execution mechanism again. The iteration is essentially a bi-level programming: the static design directly affects the optimisation parameters and algorithm adjustment of execution, while the execution simulation results in amendment and deformation of design scheme. In other words, the static design is validated by the dynamic execution results, while the dynamic execution can be promoted through the adjustments of static design. It is essential to keep the convergence of iterative design optimisation. By limited-times of repeated coordination, the design balance between static configuration and dynamic execution mechanism can be obtained.

Figure 1.	DT-driven rapid individualised design logic.
Since AFMSs must sustain relevant changes and desired product variation, the changes and variety in products are conventionally interconnected with the continuous improvement (CI) of systems themselves. Conventional analysis on qualitative variables lacks the level of detail to identify specific CI areas. One particular focus in the proposed methodology is to improve the engineering design of AFMS before start-of-production, while less effort is conducted to connect changes with system improvement after implementation.
3.1. Initial rapid individualised design of static configuration
3.1.1. Extracting reference models based on granular computing
The initial design of manufacturing system results in late process organisations and operations, and thus constitutes a significant part of the performance (Derberg et al. 2017). It usually involves design solution space deduction and accurate uncertainty representation. For the former step, the reference model-based design is one suitable way in a big data context due to its rapidity to accommodate the demands. And the extracting of reference model has a big advantage in learning from best practice of historic design, since it can benefit from nowadays deep learning and advanced big data mining techniques (Negahban and Smith 2014).
The first step is to create a model of physical equipment using a referencing method. The idea of building a twin refers to producing a digital copy of the AFMS, using it for reasoning about other instances of the similar system, and then establishing a relation among multiple copies. In design of digital copies of the physical AFMS, important model properties such as scalability (Putnik et al. 2013), interoperability, and expansibility should be defined. A five-tuple semantic data model is established as a reference representation of equipment node design and operations, and the digital copies can be called as reference model (Leng and Jiang 2017c) and formulated as:
ei ¼ fli;j;fi;j;ci;j;ti;j;si;jg
where the five elements stand for layout, WIP flow type, capability of transfer line, minimum takt time, and control scheme, respectively. These elements are key features to evaluate the similarity of physical manufacturing system configurations. The XAML schema is adopted as a meta model for the information exchange of reference models, since it allows the modelling of physical and logical AFMS components as data objects by encapsulating different attributes. Different operations on this reference model along the product life-cycle (Tao et al. 2018) such as composition, decomposition, conversion, and evaluation can be addressed. Based on the reference models, an individualised design of AFMS, which is linked-cell flow-type manufacturing system for all aspects of a production value stream (Cochran, Jafri et al. 2016), can be denoted as:
msc ¼ ei;pi;ai;mi
where the four elements represent basic equipment node, relative position, assembly relationship, and manufacturing process, respectively. As shown in Figure 2, the models are used for systems to exchange design information with each other, and can also be available in an open format such as STEP.
3.1.2. Initial design by mapping individualised demands
An effective system design can respond quickly to new product introduction, to quality issues, to changes in customer demand, to manage large product variety and production volume changes, and to continuously reduce cost (Cochran, Foley, and Bi 2017). Designing AFMS by cost alone is biased to reduce direct configuration cost at the expense of everything else, such as system efficiency. The design goal of static configuration is to achieve the individualised requirements effectively based on the best-practice implementation.
The proposed DT-driven RID model is a hybrid approach integrating discrete event models (Negahban and Smith 2014) with system dynamic models (Shardaa and Banerjee 2013) to evaluate design decisions in AFMS. As shown in Figure 3, based on the reference models learned from the granular computing (Leng and Jiang 2017c), the uncertainty predict model is used for users with different technical knowledge to construct physical components via a high-level model. Having defined the model, it will be used by other systems to extract the object information. It will enable the information exchange of the modelled attributes. These models allow designers with different knowledge background to model a DT of the AFMS via the operating and creating exchangeable model data. Finally, it outputs a AFMS design with the ability to efficiently respond to fluctuations in stochastic demand levels for the products being produced as well as other uncertainties such as machine failures and stochastic processing times (Koren, Gu, and Freiheit 2016).

Figure 2.	Design variables for characterising reference models of AFMS.

Figure 3.	Illustration of various reference models and its elements.
3.2. Multi-view synchronisation and distributed semi-physical simulation
Different form discrete manufacturing system, the AFMS has two noteworthy features: strict time series and control logic constraint between the processes/equipment. Accurate modelling and fast coordination are important for designing of whole AFMS. Therefore, unlike conventional MSD that only outputs a static solution using simulation technique, the proposed DT-driven RID is extended into validating of the dynamic execution mechanism of AFMS, among which multiple physical equipment should be verified and integrated into one entire system as a union.
3.2.1. Cyber-physical synchronisation in equipment level
The ‘‘twinning’’ between the physical world and a virtual model is a real-time monitoring data synchronisation for the digital copy of one system (Uhlemann et al. 2017), which is realised by: (1) building the communication between equipment and system through industrial Ethernet; and (2) conducting semi-physical simulation of the entire AFMS.
The traditional design approach realises control logic validation with software testing after the mechanical structure design is assembled. However, as shown in Figure 4, a multi-view synchronisation between physical equipment and digital simulation enables the efficient validating of system performance in a distributed integrating test manner. This multiview synchronisation is a hybrid technology by incorporating Object linking and embedding for Process Control (OPC) with database, public protocols, and inside Application Programming Interface (API). By enabling the cyber-physical synchronisation among Manufacturing Execution System (MES), physical equipment, and digital model, it changes the original serial design to a concurrent design. It verifies a Programmable Logic Controller (PLC) of AFMS using a simulation framework consisting of experiments and statistical analysis. Based on the modular encapsulation of reference model, the input and output interface of each equipment are defined, and then a cyber-physical synchronisation is realised through incorporating multi-view synchronisation technology into PLC.
In detail, to achieve multi-view synchronisation, the output digital signal and switch variables of simulation should be connected to that of physical system. With the binding and mapping among I/O point on the simulation model and I/O address on PLC of equipment, it can guarantee the real-time synchronisation of DT. The DT serves as a paradigm shift incorporating manufacturing process into designing of AFMS, as well as connecting these views and operations in a comprehensive model.
3.2.2. Distributed semi-physical integration
The proposed DT-driven RID can use remote data from distributed individual equipment to perform on-line adjustments. Individual adjustment implies high scan speed on semi-physical simulation and analysis inside the DT. In the equipment level, it is necessary to tackle the kinetic motion, control logic, the sensor layout. The proposed DT-driven RID directly conducts execution logic validation and control test in the digital model rather than waits for physical model built to test. These tests can quickly locate the malfunction reason, rule out the design mistakes, inspect the system whether can meet the practical individualised requirement in advance, and test the practicability of physical AFMS in execution.

Figure 4.	Multi-view real-time synchronisation in equipment-level simulation.
After distributed integrating and testing of equipment at the scene view, the next step is to integrate unit system into an entire line. Since the full-physical simulation of entire AFMS will take up a lot of time and space, a hardware-in-theloop semi-physical simulation that replaces the whole line integration steps is adopted to shorten the alignment test cycle and reduce the repeated test work after the design. As shown in Figure 5, the proposed DT-driven RID for AFMS model avoids multiple vendors do the in situ assembly, which improves test efficiency and thus reduces integration costs.
3.3. Optimal design of dynamic execution
The significance of DT-driven RID is to joint optimise the performance of physical system in a digital context before deployment, which varies by system type (Pandey, Kulkarni, and Vrat 2011). Therefore, this sub-section discusses the universal joint optimisation in dynamic execution of AFMS.
3.3.1 Modelling of multi-optimisation problem
Static configuration and dynamic execution are tightly coupled (Li and Li 2013). The universal coupling of the AFMS design lies in the joint optimisation of quadruple-problem that comprised of grouping, blanking, loading and planning. Figure 6 depicts the joint optimisation of quadruple-problem ‘Grouping-Blanking-Loading-Planning’ (GBLP). It comprises multiple kinds of coupling relationship, including process coupling (i.e., the optimisation goal suffers from un-coordination mutual constraints), objective coupling (i.e. the evaluation elements of an optimisation comes from the solution of another problem), constraint coupling (i.e., the constraint satisfiability determination of an optimisation refers to the solution of another problem), structure constraint (i.e., the solution of one problem acts as parameters of another problem). Isolated or one-sided pursuing of these optimisation goals will easily result in production imbalance, rising energy costs and decrement capacity fell.
Grouping in AFMS refers to clustering order within the production cycle according to the technical specifications, delivery time and other factors. It forms a reasonable production batch by balancing the use of raw materials and energy consumption. It can be formulated as follows:

Figure 5.	Distributed semi-physical integration based on multi-view synchronisation.

Figure 6.	The joint optimisation of grouping-blanking-loading-planning in AFMS.
GP : MaxðaÞ;Minð ÞC ;MinðDÞ;
s	n s:t:Xbi ¼ XOj;r bð Þi \e;i 2 ½1;s;C ¼ h1ð Þ þb	h2ð ÞD :
	i¼1	j¼1
Blanking in AFMS is a layout optimisation to maximise the utilisation rate of raw material according to the production batch quantity, size, specification and quantity. Usually, the layout scheme must meet the Guillotine-Cutting constraints (i.e., any cutting is intended to ensure that the section is separable). It can be formulated as follows:
BP : MaxðakÞ;
s:t:LPðak;MkÞbik1;G~ðMkÞ ¼ 1:
Loading in AFMS is to optimise the capacity according to the layered constraints including capacity, leading process flow order, and equipment transportation way. It can be formulated as follows:
LP : MaxðbikÞ; s:t:BP bik ¼ ak;F~ Lik	¼ 1;
Planning in AFMS is to optimise flow path and order of WIP for minimising makespan, considering the actual configuration, blanking and the loading scheme. It can be formulated as follows: PP : MinðD þ l  R~ðM;LÞÞ; s:t:GCðMÞ ¼ 0;GC Lð Þ ¼ 0:
Typical design variables are identified in Table 1, which can be assigned according to the various implementation scenarios.
The overall objective function aims at minimising makespan, mean WIP and number of machines, while considering multiple factors including uncertainties in processing times, equipment failure and repairs, inventory capacity and product demand. On one hand, subjecting to a complex coupling relationship, the constraints of joint optimisation are difficult to construct with a closed representation. The objective can only be depicted by implicit function, and the design variables are also difficult to form a clear analytical relation. On another hand, the coupling relation of joint optimisation needs many nested computations, and inevitably calls for multiple optimisation variable combination calculation. Blanking and Loading are two opposite goal to coordinate, and thus easy to cause the high computing costs and dramatic iter-
Table 1.	The notations used in formulation of multi-optimisation problem.
ation divergence. Therefore, conventional methods such as multi-objective optimisation are not suitable to solve the ‘BGLP’ joint optimisation. The new decoupling algorithm should be developed.
3.3.2. Decoupling of multi-optimisation problem
The key solution lies in computational decoupling method using a hierarchical diagram calculation. The objective and constraint coupling from GBLP joint optimisation model have a relatively clear partial-order calculating relation, and thereby can be solved with hybrid calculations with nested, recursive and backtrack technique. The intersection relationship has no clear boundary among them, and therefore can be encapsulated into a kind of integrated optimisation problem in combination with special internal negotiation mechanism. Through the topology deformation of the problem structure, a Hierarchical Layered Calculation Diagram (HLCD) of six components is formed in Figure 7.
Firstly, in the upper-level of HLCD, by controlling the clustering radius (i.e., minimum deviation) of batch delivery time, it will form a feasible set of grouping solutions. Since multi-channel recalls exist in this optimisation, it needs a consultation mechanism to avoid the repeated search. The blanking problem and loading problem are encapsulated into an integrated optimization, and combined with the layout optimisation as atomic engine with a coordination mechanism. Then, by using the natural clearance of variable domain to generate Pareto extremum sequence traversal, it can coordinate the calculation and furthermore transfer unit optimisation algorithm into a coordinated one. One major conflict to be coordinated is the blanking rate α and loading rate β, which will first need to resolve variables discretisation. Due to the non-equidistant jump features of grouping space, it generates monotone decreasing α sequence (α1, α2, …, αn) and uses the binary traversal (determine the current approximate equinox according to the previous iteration to determine the best bs11 value from ðb01;b11;...;b1m Þ. By iterative execution in such a manner, the extreme value of sequence ðai;bsi j Þ will be obtained.
Secondly, in the lower level coordinator of HLCD, blanking solutions and loading solutions will serve as the heuristic parameters of the planning problem, while the grouping problem is at the top of the coupling optimisation problems. The cluster radius r(b) shall not be higher than the minimum delivery time deviation epsilon.
Thirdly, based on the integrated optimisation of blanking and loading, the grouping solution needs to be negotiated and coordinated by weighted utility equation (i.e., coordinator) as hða;b;Dn;xÞ ¼ 0; s:t:;ðMMnðbÞ;DnÞ ¼ 0;

Figure 7.	The hierarchical layered calculation diagram for decoupling of BGLP.
where ω should be assigned according to the application scenarios assignment; MMn(β) is a derived function of the shortest makespan. The weighted balance equation will find best design values with the extremum sequence, while the key process constraint equation can implicitly characterise the underlying relationship of blanking and loading.
Combined with the above ideas, the decoupling of design calculation is shown in Figure 7. This decoupling should be handled with a hybrid metaheuristic algorithm to select the optimal quadruple BGLP rules. Hybrid metamodels for multiple-level programming have also been studied (Negahban and Smith 2014), which will not be detailed here for concise reason.
3.4. Iterative design between static configuration and dynamic execution
3.4.1. Iterative design logic
Since commonly identified performance measures (e.g., product quality, lead time, productivity and adaptability) are dynamic and vary with respect to time, the system performance usually relies on its commitment to sustained evolvement and continuous improvement over time (Cochran et al. 2016). It is the sole philosophy of conventional design methodology of AFMS, which was always redesigned based on reconfiguration and adjustment of execution mechanism after implementation. Some unbalance between static configuration and dynamic execution would make the cost and performance inconsistent, which is hard for designers to perceive their presence. A performance-balancing method is essential to solve the multiple-level optimisation problem (Negahban and Smith 2014).
The DT-driven RID of AFMS has the systematic context data and rationale for eliminating this unbalance. This paper proposes an iterative design strategy between static configuration and dynamic execution, in which bi-level programming is used to timely coordinate the unbalance. As shown in Figure 8, in the context of iterative optimisation, a design with multiple outputs and the robustness of their procedures are parallelized and integrated into an integrated model.
3.4.2. Bi-level programming for balance between configuration and execution
Generally, the interaction between static configuration and dynamic execution is characterised by two levels of optimisation problems where the constraint domain of the upper level problem is implicitly determined by the lower level optimisation problem (Colson, Marcotte, and Savard 2007), and can be formulated as a bi-level programming:
Upper level problem: The cost of static configuration includes machines, fixtures, gauging systems, transportation and warehouse equipment (Zhang et al. 2017), etc. It subjects to the satisfaction degree of individualised demands, given as follows:
M minC ¼ Xc eð i;pi;ai;miÞ;
i¼1
s:t:R 2 li;j;fi;j;ci;j;ti;j;si;j:
Lower level problem: The performance metric measures how well the system is designed based on the simulation and decoupling of dynamic execution (Cochran et al. 2016), given as follows:
maxP ¼ h a i;bi;Dn;i;xi; s:t:;MMn;iðbiÞ;Dn;i ¼ 0:
This bi-level programming is a non-cooperative game with leader-follower strategy. The upper level plays first, and lower level observes the action and reacts, which must be the optimal solution. Finally, both levels get their utility

Figure 8.	Iterative design between static configuration and dynamic execution.
according to minC and maxP. Then, if any level would benefit from a unilateral deviation of his action, i.e., choosing a different action fixing other’s action, the resulting action profile is an equilibrium of the bi-level programming, defined as  fsi with respect to ls ∊ LS, when
FUils;fsi FUiðls;fsiÞ:
In our model, the strategy profile ðls;fsÞ is an equilibrium of the bi-level programming if
LU ls ;fsi LU ls ;fsi ;
8i, and all ls* ≠ ls, fsi 6¼fsi.
Equilibrium of the bi-level programming has been discussed in many studies (Amir and Grilo 1999; Baoding 1998; Cruz 1978). It is difficult to find the equilibrium, since the bi-level programming is a non-deterministic polynomial (NP)-hard problem (Benayed and Blair 1990). Even though both the upper and lower level problems are convex, the bilevel programming is still likely to be nonconvex. Moreover, even if we can find the solution of the bi-level programming, it is usually not global optimum but local optimum (Amir and Grilo 1999; Baoding 1998).
To obtain the global optimal or near-optimal solution of the proposed bi-level programming model, several elaborated solution procedures to tackle the NP-hard nature are proposed based on a heuristic process. Figure 9 illustrates the procedure. It initializes with a guess of the optimal upper level design values and moves this initial solution via a heuristic process to achieve a new design solution. By solving the lower level problem, the optimal reaction (i.e., a design of dynamic execution) is obtained and returned to the upper level for each iteration. This procedure continues until an optimal or near-optimal solution is obtained. Many computational intelligent algorithms can finish this heuristic process (Atashpaz-Gargari and Lucas 2007).
4. A demonstrative prototype and case study
4.1. DT-driven individualised design prototype
Based on the proposed key enabling techniques, a DT-driven RID prototype for AFMS is presented in Figure 10. It includes upper-level calculation system and lower-level semi-physical simulation platform. The calculation system is developed with J2EE programming architecture and integrated with intelligent decoupling algorithm. The simulation platform is established through secondary development on the command and information channel based on database and an open source Unity3D engine. The calculation system executes the optimisation kernel and then transmits optimisation results to the simulation platform. The simulation platform feedbacks design information from the field to the calculation system. The simulation platform has real physical properties (e.g., gravity, friction, speed, impact and inertia) to depict the real logistics and manufacturing processes.
The prototype provides an excellent platform to integrate various disciplines of AFMS design. Existing frameworks for each discipline can be linked through the DT. The virtual twinning based on the simulation platform allows the conformance checking of the product specifications with the design intent and customer requirements.

Figure 9.	Bi-level programming for balance between configuration and execution.

Figure 10.	DT-driven rapid individualised designing platform architecture.
4.2. DT-driven design of AFMS
The proposed RID approach is demonstrated on a sheet material AFMS. The manufacturing process of sheet material usually comprises operations including cutting, edge grinding, toughen, pairing store and extrusion, among which the absence or failure of one process will easily lead to production chaos and disorder.
Suffering multiple constraints (capacity, layout and process route), the design of sheet material AFMS is essentially a custom design process. The individualised parameters can be categorised into four major types namely, capacity variables (e.g., inventory capacity and throughput), system configuration parameters (e.g., grid frame configuration, roadway configuration and layout design), control scheme parameters (e.g., motion planning, control logic, sensor layout and convey instructions) and the execution algorithm primary parameters (e.g., inventory management, distribution of roadway and deadlock avoidance).
As shown in Figure 11, the individualised design is conducted as following four specific steps:
Step 1: Initial individualised design of static configuration. Based on a series of predefined reference models (e.g., special equipment, transmission equipment, storage, robots and other equipment), the function and efficiency of equipment are encapsulated based on the actual control scheme, and the standardised data interface is defined. Once the modular encapsulation is carried out on the established model, it can realise quick assembly and layout planning of sheet material AFMS.
Step 2: Distributed semi-physical simulation. According to the requirements on workshop venue, capacity, process, takt and equipment, a quick model assembling is conducted on the simulation platform in a distributed semi-physical manner. The motion design, control scheme and simulation of dynamic execution for every link in the sheet material AFMS are finished and outputted in this step.
Step 3: Design of dynamic execution based on performance evaluation. After the preliminary design of static configuration, the production process is verified with the help of simulation platform for dynamic simulation. According to the operation efficiency and load analysis of design modifications, a new set of suitable design for the whole line of intelligent execution kernel is formed in the upper level calculation system.
Step 4: Iterative design adjustment. The initial design did not consider all metrics associated with design of AFMS. In the initial design iteration, the results from upper level calculation system indicate that the machining centres are highly underutilised. This capacity under-utilisation results in a larger investment. In addition, a larger investment is required to deal with product flow complexity. Therefore, it is necessary to conduct the iterative design. Based on the preliminary design scheme, the production process is verified with the help of bi-level programming for balancing cost and performance. According to the cost analysis and operation efficiency for design modifications, an optimal design of AFMS is finally achieved.

Figure 11.	DT-driven design of sheet material AFMS.
5. Discussions
5.1. Comparative analysis
The prototype has been successfully applied to a sheet material processing enterprise in Chengdu, China. This section contrasts a new AFMS designed and managed by the DT prototype with an existing AFMS managed based on Oracle®. A performance evaluation is made for both systems using 3 heuristic measures (i.e., production packages, unified system cost, unified system performance) listed in Table 2. We have anonymised and normalised part of data due to the confidentiality reason. It illustrates how well each metric is achieved for the new AMFS.
From the system performance view, the results show that the DT prototype significantly improves number of production packages, which apparently results in less in-cell inventory and WIP inventory. The unified performance P of AFMS designed based on DT is about 0.886 on average and improves 28.6 % compared with the original results 0.689. The unified cost of AFMS designed based on DT conserves 22.2% compared with the original results, which is majorly hindered by the AFMS factors of storage disorder and the ascending rules of roadway storage distribution. In conclu-
Table 2.	Results on the system simulation.
sion, the proposed DT prototype significantly improves the performance of sheet material AFMS. Generally, dramatic improvements regarding in-cell inventory between machining and assembly, throughput time, and investment are achieved with the new AFMS design. In the new AFMS, by re-balancing the equipment utilisation to demand, the takt time corresponding to this configuration is then balanced to minimise cost. This design solution enables the reduction of motion waste, resulting in a decrease on unit cost.
5.2. Managerial implications
Based on the key observations and findings from this case, this study proposes some managerial implications.
Firstly, the present work can be easily reproduced for the firms that are already equipped with automation equipment in the flow-type manufacturing system for realising the mass individualisation demands, since they have enough hardware foundation for the implementation. The proposed approach can also be applied for the implementers without digital twin technology in the manufacturing systems, and they only should equip basic PLC and middleware equipment, and do the inter-connection and algorithm development work referring to the proposed approach and API.
Secondly, given the fact that design decisions are usually costly and long term, it becomes imperative to have higher degree of accuracy and consider impacts of process and product mix uncertainties (Shardaa and Banerjee 2013). Benefited from the first two key enabling techniques (i.e., massive reference models and distributed semi-physical simulation), the designer can quickly complete the AFMS layout design, equipment configuration and the virtual assembly of production line model. According to the statistics of our implementations, the proposed prototype takes average a day to satisfy customer’s individualised requirements of AFMS, a week to enable the motion of whole AFMS, and a month to integrate the whole AFMS including the products, equipment and execution system.
Finally, for an implementer who don’t want to introduce digital twin technology into the AFMS, they could also introduce the iterative design strategy and system evaluation metrics (e.g., Packages, Unified Cost C, and Performance P) in proposed approach for balancing the key overall bottleneck resources between static configuration and dynamic execution, though it couldn’t realise the validating and on-line testing of design solutions. Benefited from the last two key enabling techniques (i.e., optimal design of dynamic execution iterative decoupling of joint optimisation problem in design), the digital twin-driven platform can optimise the dynamic execution mechanism. The production performance of the whole AFMS can be virtually analysed and feedback to the initial design. Once a deficient performance, the virtual model can be adjusted and iterated until an optimal design of the whole AFMS is identified.
6. Conclusions
This paper presents a digital twin-driven methodology for rapid individualised designing of automated flow-shop manufacturing system. How the digital twin applies in defining and optimising system behaviour is discussed. Through the analysis of design variables in the configuration, as well as the dynamic execution mechanism needed to meet process constraints and complex coupling relationships, this paper proposes an idea of ‘iterative design optimization between static configuration and dynamic execution’. The objective is defined as a joint optimisation between configuration cost minimisation and system performance maximisation. The use of bi-level programming provides a pareto front of candidate designs, and enable the optimal design in a holistic view. Evidenced by a successful design in sheet material automated flow-shop manufacturing system, the proposed digital twin prototype can provide design with an intelligent simulation and optimisation engine.
However, there remain many issues waiting to be solved. The first is to build a digital twin model combining the planning technique. After releasing process plans to the MES, the digital twin model can generate detailed work instructions associated with the process plans. The second is how to incorporate new metamodelling or deep learning (Leng et al. 2018) approaches for providing more robust and faster decision support. When compared the design with actual manufacturing result, the big data analytics can be introduced to identify whether there is a difference and find out the cause. The third is to build a more comprehensive digital twin-driven cyber-physical-social-connected AFMS for crossenterprise manufacturing coordination (Leng, Jiang, and Ding 2014; Leng, Jiang, and Zheng 2017). Meaning more profound of digital twin model toward smart manufacturing is to accumulate the design and manufacturing knowledge, which can be reused and improved continuously.
Disclosure statement
The authors declare that they have no conflict of interest.
Funding
This work was supported by the National Natural Science Foundation of China [grant numbers 51675108 and 51705091]; the Science and Technology Plan Project of Guangzhou [grant number 201804020092]; the Science and Technology Plan Project of Guangdong Province of China [grant numbers 2015B010128007 and 2016A010106006]; and the Fundamental Research Funds for the Central Universities [grant number 2015ZZ079].
ORCID
Jiewu Leng  
"
CFRQCR34;<pad> a new approach leveraging a few potential and promising technologies and tools such as a reliable and reusable virtual model for vehicles. a machine learning model, the IoT fog or edge data analytics, a data lake for traffic and vehicle data on public cloud environments, and 5G communication lead to a variety of digital disruption. the in-vehicle infotainment system readily communicates, cooperates, corroborates, and correlates with the road infrastructure modules.</s>;"https://doi.org/10.1007/s40860-018-0069-y

A novel digital twin-centric approach for driver intention prediction and traffic congestion avoidance
Sathish A. P. Kumar1 · R. Madhumathi · Pethuru Raj Chelliah3 · Lei Tao4 · Shangguang Wang4
Received: 13 September 2018 / Accepted: 5 October 2018 / Published online: 16 October 2018
© Springer Nature Switzerland AG 2018
Abstract
Road traffic has been exponentially growing with surging people and vehicle population. Road connectivity infrastructure has not been growing correspondingly and hence the research endeavors for optimal resource allocation and utilization of connectivity resources has gained a lot these days. Therefore, insights-driven real-time traffic management is turning out to be an important component in establishing and sustaining smarter cities across the globe. IT solution and service organizations have come forth with a number of automated traffic management solutions and the primary problem with them is they are unfortunately reactive and hence an inefficient solution for the increasingly connected and dynamic city environments. Therefore, unveiling real-time, adaptive, precision-centric and predictive traffic monitoring, measurement, management and enhancement solutions are being insisted as an indispensable requirement toward sustainable cities. We have come out with a novel approach leveraging a few potential and promising technologies and tools such as a reliable and reusable virtual model for vehicles, a machine learning model, the IoT fog or edge data analytics, a data lake for traffic and vehicle data on public cloud environments, and 5G communication. The paper details all these in a cogent fashion and how these technological advancements come handy in avoiding the frequent traffic congestions and snarls due to various reasons.
Keywords Virtual vehicle (VV) model · Edge or fog clouds · Machine and deep learning algorithms · Digital twin · Cloud

computing · Real-time analytics · Intelligent transport
Introduction
The emergence of software-defined cloud infrastructures and scores of integrated platforms along with a bevy of pioneering digital technologies such as machine and deep
learning, streaming analytics, micro services architecture (MSA), container management solutions, the distributed and decentralized IoT architectures, fog or edge data analytics, and5Gcommunicationleadstoavarietyofdigitaldisruption, innovation and transformation for the worldwide corporate andcities.Thenationsacrosstheglobesettingupandsustaining smarter cities are empowered with the faster maturity and stability of game-changing technologies and tools. With the continued advancements and accomplishments in the ICT (information and communication technologies) space, the speed and sagacity with which the establishment of smarter cities is really praiseworthy. The rising complexities due to thearrivalandusageofheterogeneousandmultipletechnologies for realizing smart cities are on the climb. Therefore, the adoption of complexity-mitigation and value-adding technologieshelpsplanners,decision-makers,andadministrators come handy in surmounting those complications to quickly and easily bring forth people-centric, extensible, adaptive, knowledge-driven,innovation-filled,cloud-enabled,andsafe cities.
The transformative technologies for transport and traffic domains
Traffic management becomes an intimate and intense affair for accomplishing smarter city projects. With the growing population of cars and vehicles, our connectivity infrastructures such as roadways, expressways, tunnels, bridges, and underground passages are experiencing a different kind of stress. Traffic snarls, congestions, and blockages damage the productivity of people. There is huge fuel wastage because of many stops and slow movements of vehicles at several junctions on the way to the destination. There have been concertedeffortsbyresearchscholarsandscientiststobringforth strategically sound solutions for real-time intelligent traffic management solutions. However, they are found insufficient due to various causes and reasons. Now with the emergence of path-breaking technologies, automated tools, optimized processes and integrated platforms, researchers across the globe have started to focus on breakthrough solutions to minimize the traffic congestions and road blockages. There is a unified view that real-time decision-enabling, value-adding, andactionabledata-driveninsightsaretheneedofthehourto regulate and rectify traffic issues. That is, capturing all kinds of vehicle movement data, road capacities, driver intention, destination, and any local traffic information and subjecting them to a variety of mining, processing, and analytics is the way forward for smarter traffic management.
The continuous maturity of artificial intelligence (AI) technologies such as machine and deep learning also contribute to the smartness of traffic management. Finally, the recent concepts of fog or edge analytics, digital twin and blockchain are getting a lot of attraction and attention. This paper is to describe new automated transport management solution that is to gain the intended prominence and dominance through the seamless and smart integration of the above-mentioned transformative technologies.
Research problem description
The various traffic statistics across cities say that the number of road accidents is on the rise, the traffic congestion is becoming alarming, the car population is growing fast, the time being spent on the roads is increasing, and the fuel and time wastage due to traffic snarl is definitely higher. On the other hand, pleasure trips and joyrides also contribute to more vehicles on the roads. Roadside hotels and motels are increasing in numbers. The number of traffic signals is steadily growing to regulate the escalating traffic. There is a growing family of traffic management systems that automate several aspects.
There is a realization that for further and deeper automation, big and streaming data analytics is the viable approach and answer. There are integrated platforms (commercialgradeandopensource)forenablingboththeactivities.These platforms are being made readily available in cloud environments. Collecting all kinds of road, car, and traffic data, carrying them to cloud platforms, subjecting the collected, curated, and cleansed data to a variety of investigations to arriveatdecision-enablinginsights,takingdecisionsontime, and plunging into appropriate actions are the major components in the workflow. However, with clouds being operated at remote locations, the idea of real-time data capture, communication, processing, decision enablement, and actuation is out of question. Therefore, analytics professionals are of the opinion that instead of leveraging off-premise, online and on-demand cloud infrastructure, edge device clouds are recommended as the best fit for real-time data collection and crunching to facilitate real-time decision-making and actuation. Thus the faster the maturity and stability of the IoT edge/fog computing signals, the more advanced are the traffic management capabilities.
That is, there is a high synchronization between cloudbased big data analytics and the IoT edge data analytics through edge device clouds. But then, thepronounced advantages of this design are not to be boasted, because the big data analytics typically does deterministic, diagnostic, and historical processing and mining. That is, the processing and analyticslogichavetobecodedmanuallyanddeployed.Still, there are challenges in arriving at competent traffic management systems. This paper has proposed a fresh and futuristic attempt at producing viable, self-learning, and automated traffic management systems.
Embarking on next-generation intelligent transport systems (ITS)
Conventional IT-enabled ITSs are found insufficient and obsoleteintheincreasinglyconnectedandcomplicatedtransport world. The fast-growing traffic conundrum insists on highly sophisticated and technology-intensive solutions for the transport world. Fortunately, the technology domain is also on the fast track producing breakthrough technologies and tools for simplifying and streamlining the process toward producing highly competitive and cognitive transport systems and services. This section illustrates the famous technologies enormously contributing to the faster realization of next-generation transport solutions.
Traffic lights have become very prominent and pervasive in urban areas for enabling smooth flow of pedestrians as well as vehicle drivers. There are high-fidelity video cameras in plenty along the roads, expressways, tunnels, etc. to activate and accelerate a variety of real-time tasks for pedestrians, traffic police, and vehicle drivers. Wireless access points such as Wi-Fi, 3G, 4G, roadside units, and smart traffic lights have been deployed along the roads. Vehicle-to-vehicle(V2V)andvehicle-to-infrastructure(V2I) interactions enrich the application of this scenario. All kinds of connected vehicles and transport systems need actionable insights in time to derive and deliver a rich set of context-aware services. Safety is an important factor for car and road users and there are additional temporal as well as spatial services being worked out. With driverless cars under intense development and testing, insights-driven decisions and knowledge-centric actions are very vital for next-generation transports.
Every vehicle is connected. The in-vehicle infotainment system is being fit in every kind of vehicle on the road. This in-vehicle system acts as the centralized controller and gateway for the outside world. They contribute to the communication module capturing and communicating all kinds of operational, health, and performance parameter values of everysignificantmoduleofthevehicletofarawaycloudenvironments. A cloud-hosted intelligent traffic system (ITS) has to be in place to act as the data cruncher, decision-maker, and actuator. The ITS has to be highly introduced.
Fog/edge analytics through device clouds
Typically, cloud computing prescribes centralized, consolidated, and sometimes federated processing through a variety of cloud models ranging from public, private, hybrid, and community clouds to fulfill new-generation computing needs. Now with the accumulation of distributed and dissimilar devices emerging as the new viable source for data generation,collection,storage,andprocessing,thecloudidea isgettingexpandedsubstantiallyandskillfullytowardtheera of edge or fog clouds, which is a kind of distributed yet local clouds for proximate processing. That is, the growing device ecosystem of resource-constrained as well as powerful fog devices (smartphones, device and sensor gateways, microcontrollers such as Raspberry Pi, etc.) in close collaboration with the traditional clouds are emerging as a venerable force for accomplishing the strategic goal of precision-centric data analytics.
The next-generation data analytics is being expected to be achieved through extended clouds, which is a hybrid version of conventional and edge clouds. That is, the sophisticated analytics happens not only at the faraway cloud servers, but also at the edge devices so that the security of data is ensured and the scarce network bandwidth gets saved immeasurably. The results of such kinds of enhanced clouds are definitely vast and varied. Primarily insights-filled applications and services will be everywhere all the time to be dynamically discoverable and deftly used for building and delivering sophisticated applications to people. There are convincing and captivating business, technical, and use cases for edge clouds and analytics for discovering and disseminating realtime knowledge.
Relevant and real-time vehicle and traffic information through edge clouds
Edge analytics is gaining a lot of momentum these days. With the edge devices being embedded with sufficient processing, storage, and I/O power, they are individually as well as collectively readied to participate in the mainstream computing. These devices can collect and process any incoming data and emit useful information in real time. The shared information can help the various participating sensors and actuators to plan and indulge in performing their activities with cognition, clarity, and confidence. Vehicles on the road are being stuffed with a number of purpose-specific and agnostic sensors and actuators to proactively and preemptively capture all the right and relevant data. The centralized infotainment system or OBD dongle contributes immensely to making smarter vehicles. The road infrastructure is also fitted with various cameras, sensors, Wi-Fi gateways, and other electronics to enable data gathering, aggregation, and communication. The in-vehicle infotainment system readily communicates, cooperates, corroborates, and correlates with the road infrastructure modules to get synched up with one another to collectively do the real-time and secure data capture, cleansing, filtering, decision enablement, and actuation.
Vehicles talk to one another as well as with the roadside IT and electronics equipment to recognize and relay the realtime situation on the road. The roadside infrastructure also comprises a variety of sensors to measure the distance and the speed of approaching vehicles from every direction. The other requirements include detecting the presence of pedestrians and cyclists crossing the street or road to proactively issue“slowdown”warningstoincomingvehiclesandinstantaneously modifying its own cycle to prevent collisions. Besides ensuring utmost safety and the free flow of traffic, all kinds of traffic data need to be captured and stocked to do specific analytics to accurately predict and prescribe the ways and means of substantially improving the traffic system. Ambulances need to get a way out through traffic-free open lanes in the midst of chaotic and cruel traffic.
Digital twin
This is the latest buzz in the IT space. The ground-level entities (physical elements) are being integrated with cloudbased applications (cyber applications). This formal integration accordingly empowers the physical entities to join in the mainstream computing. This is the overall gist of cyberphysical systems (CPSs) and the Internet of things (IoT). Primarily, scores of industrial and manufacturing machines get integrated with remotely held applications and data sources. This setup enables the machines to be extremely and elegantly sensitive, responsive, and adaptive in their actions.
Now, the idea of the digital twin is to have a corresponding virtual image for a physical asset at the ground. That is, the virtual entity has all the structural as well as behavioral properties as the corresponding physical element. The digital twin is to have a dynamic virtual/digital representation for each of the physical systems. This cloud-based virtual representation helps to gain a better and deeper understanding of allkindsofground-levelphysical,mechanical,electrical,and electronicssystemsandhowtheyteamuptocollaborate,corroborate, and correlate with one another in the vicinity. The actions and reactions of these ground-level elements can be easily visualized, modeled, studied, and articulated through their corresponding virtual entities. There are other benefits of having a virtual replica of physical things. Ultimately, the fresh concept of digital twin takes the current IoT capability to the next level.
The machine and deep learning methods
This is the hottest topic on the planet Earth at this point in time. The data being generated and collected from different and distributed sources are growing exponentially. That is, it is the big data era. The data are simply multi-structured. The data size, speed, scope, structure, and schema vary and the hence it is a tremendous challenge to extract useful and usable information out of big data for data engineers and management professionals. There are a number of standardized big data analytics solutions in the form of enabling tools andintegratedplatforms.Theseanalyticalsolutionstypically perform batch processing, which is not liked by many. We are tending toward a real-time analytics of big data. That is, extractingactionableintelligenceintimeoutofbigdataisthe mottobehindtherecentadvancementsintheanalyticalspace. Anotherinterestingandintriguingtrendistheautomatedanalytics. That is, next-generation analytics platforms are being stuffed with a variety of learning algorithms to empower the analyticalplatformstoself-learn,reason,train,model,understand, and articulate newer evidence-based hypotheses.
Data lake for transport and traffic data heaps
Data lakes are becoming commonplace across industrial verticals. All kinds of multi-structured data get stocked in a centralized place to be found, accessed, and used for extracting useful insights out of data heaps. Data scientists are using datalakesgreatlyintheireverydayjob.Forsettingupandsustaining insights-driven transport management systems, data lakes are essential. We have object storage facilities in cloud environmentstofacilitatetherealizationofdatalakes.Application programming interfaces (APIs) are being attached to open up for the outside world to find and bind with data collections to envision futuristic things.
Blockchain technology
Thisisquiteanewparadigmgainingalotofmomentumthese days.Thishasfoundalotoffollowersacrossvariousindustry sectors. This newly introduced technology brought in newer possibilities and opportunities for the transport sector. There are forecasts that as many as 54 million autonomous vehicles will be on the road by 2035. As the number of vehicles increases, so too will the volume of data. Also, by 2020, there will be 8.6 million connected features in cars and there are also estimates that there are up to 100 electronic control units in today’s cars. That equates to 100 million lines of code.Therearestrategicusecasesfortheautomotiveindustry through the fast-evolving blockchain paradigm. Smart contracts are being coded to bring in the required intelligence to vehicles, traffic systems and databases, drivers, owners, etc. All kinds of interactions and transactions between the various participants get securely stored through the blockchain database. Thus, in the days ahead, there will be closer and tighter integration between vehicles and the fast-growing blockchain technology.
The noteworthy factor here is that the smarter traffic system has to learn, decide, and act instantaneously to avert any kind of accidents. That is, the real-time reaction is the crucial need and, hence, the concept of edge clouds out of edge devices for collaboratively collecting different data and processing them instantaneously to spit out insights is gaining widespread and overwhelming momentum. Another point here is that data flows in streams. Thus, all kinds of discrete/simple, as well as complex events need to be precisely and perfectly captured and combined to be subjected to a bevy of investigations to complete appropriate actions. The whole process has to be initiated at the earliest through a powerful and pioneering knowledge discovery and dissemination platform to avoid any kind of losses for people and properties. Here, collecting and sending data to remote cloud servers to arrive at competent decisions are found inappropriate for real-time and low-latency applications. However, the edge data can be aggregated and transmitted to powerful cloud servers casually in batches to have a historical diagnostic and deterministic analytics at a later point in time.
The proposed solution approach
We have come out with a real-time and cognitive traffic congestion avoidance solution. Having studied the current lacunae in the traffic management solutions, we have come out with an advanced, extensible, and AI-inspired solution to precisely and perfectly measure the traffic situation in real time and the driver intention by leveraging the localized fog analytics, the power of the digital twin along with the big data processing using competent machine learning methods.

Fig. 1 Solution approach diagram
The reference architecture for our solution is shown in Fig. 1.
The solution architecture description
There are three principal ingredients for enabling congestion discovery and dispersal, avoidance, and prediction.
Gathering situational information in real time—the current road and vehicle data through fog or edge data analytics.
Gaining driver history, behavior, and intention through machine learning (ML) and deep learning (DL).
Data lake at cloud for stocking historical information.
Intelligent transport system (ITS).
The virtual vehicle (VV) model—digital twin.
Blockchain as a service for vehicles.
The situational details are being captured through a variety of multifaceted cameras deployed along the road and route. Secondly, the driven intention is captured and decided through the VV model, which was explained above in detail. The key device is the vehicle telematics system that acts as the primary gateway between the car and the outside world.
Edge analytics-based virtual vehicle (VV) networks
To address the traffic challenges, here is a viable proposal. With the availability of powerful cameras and sensors along the roads, bridges, expressways, tunnels, signals, etc., a massive amount of real-time as well as historical data get captured,collected,cleaned,andstockedtobecrunched.One of the decision-enabling factors for proactively and preemptively avoid traffic congestion and snarl is to get the drive

Fig. 2 VV architecture
intention. Figure 2 vividly illustrates how the driver intention is deduced from the various data collection and the digital twin, which is formed through a virtual vehicle (VV) model. The need here is to formulate a flexible and futuristic VV model to enable machine and deep learning algorithms to predict the driver intention with accuracy.
Since the proposed VV model makes decisions, it needs detailed driver information, such as preferences as to which lane the driver or automated vehicle is likely to select and route plans that are together considered as ‘intention’. The VVmodelcanobtainscalable,real-timedriverintentiondata, both captured locally from the vehicle, edge cloud, and the remotecloud;byprocessingthemintheedgeandbyinteracting with other VVs, VVs can predict other drivers’ intentions in such a way that this intention information can be used for a variety of scenarios. The VV is a virtual state of the vehicle and driver, which is processed in the edge and exists in the cloud.
The VV can interact with other VVs in the edge, where it is not limited by communication and computation resources. VVs for driverless vehicles can make decisions about path planning and interaction with other vehicles, while VVs for non-autonomousvehiclescanhelpdriversmakedecisionsby mining other drivers’ intentions. By obtaining data directly from the cloud and actively communicating with other VVs, the VV can coordinate with others to form a VV network (VVN). The physical vehicle or traffic controller behaves like an actuator on the road, acting upon directions from the VVN to the edge.
The role of the digital twin in the form of virtual representation for various physical, mechanical, electrical, and electronics assets and artefacts is to grow further in the days to come. The cloud centers emerge as the best-in-class IT environment for activating and accelerating the digital twin capability to produce actionable insights in time. The VV model, the digital twin for the transport industry vertical, is to be realized through integration with various contributing systems to be adaptive.
Thatis,theVVmodelorchestratesthroughseveralentities to be accurate and authentic. A variety of parameters are incorporated to make the VV approach viable and venerable.
The localized data, being captured and filtered through edge or fog devices, convey the realistic and real-time situation at the ground level. The traffic scenario, the road and the vehicle data, and other useful information are collected by fog devices and subjected to a variety of investigations to extricate usable and useful information that can be communicated to the faraway and powerful clouds to synchronize with the historical data to enhance the accuracy of the decisions. The VV model comes in hand in contributing to the knowledgediscoveryanddissemination.Finally,theITSacts based on the insights accrued.
Virtual vehicle (VV) model
Having discussed the various ingredients of the solution, this section describes the VV model. In this VV model, we describe various variables and parameters to arrive at a competent VV model.
For more than a decade, multi-agent systems have been an active area of research [1–5, 6, 7]. Agent technology, which reliesondistribution,providesanaturalsolutiontothehighly distributedanddynamicallychangingproblemoftrafficmanagement and control. Although some existing approaches utilizemulti-agentstosolvetrafficcongestion,thesemethods rely on traffic control centers; traditional agents cannot make decisions for users. However, in our proposed model, the VV is a virtual state of both driver and vehicle, and has personalized knowledge of each vehicle, so that it can make effective decisions compared to the existing agent-based models.
We have designed and developed approaches based on multi-agent systems for several related problems [8–10] and will leverage these approaches for the proposed VV model. These agents can provide distributed data mining and autonomous data to decisions using minimal computing and networking resources without moving huge amounts of data to analytic codes.
OneofthemajorchallengesinthedesignoftheVVmodel ishowtocapturetheintentinformationtomakedecisionsand predictions for real-time control. We will leverage the existing approaches [11–20, 21–29] to capture the vehicle and driver intentions and improve upon them to ingest the intention data for the proposed VV model. Most of the challenges in VV modeling are technical problems that come from handling large amounts of information and modeling highly dynamic interactions. For example, the model constructed should satisfyallpotential VVs; thesamevehicle withdifferent drivers, and different vehicles with the same driver, forms different VVs. There are, therefore, a large number of VVs to track and model. An additional layer of complexity comes from the fact that each VV must intelligently make decisions according to dynamic traffic information, such as the flow of the vehicles and their related information. Therefore, each of the multitudes of VVs should have personalized knowledge about the driver and the vehicle, another source of high informationload.TheneedforVVstointeractwitheachotherand achieve cooperation only increases the model’s complexity. Another key challenge is how to describe the data—both the objective information VV needs to operate and subjective driver preference.
To address these challenges, we have designed the VV model as shown in Fig. 3, with our rule engine at the core. In the proposed VV model, network data would include the information related to edge network, such as bandwidth and boundary data. Sensor data would include the current GPS coordinates, current speed, and average speed of the vehicle. This dynamic vehicle information is captured in the fact database, and the cache of these facts needed by the knowledge session is stored in the working memory. The knowledge base would include the interest reference, such as scenic route; path preference, such as fastest route; and driving characteristics information, such as rash driving. The fact database and the knowledge base information would be used by the learning agent to learn about the VV and capture the vehicle/driver intentions. Examples of the features thatwouldbecapturedintheindividualactionlearning(IAL) phaseincludethecurrentspeed,make,model,andyearofthe vehicle. Example of the features that would be captured for thejointactionlearning(JAL)phasewouldincludetheemergencysituation.Thefeaturescapturedinthefactdatabaseand knowledgebasewouldbeprocessedintheruleenginetogenerate actions. An example of actions that would be generated from the rule engine would include informing other VVs in VVN of known traffic accidents, lane closures, etc. The VVs coordinate with edge devices, cloud, and other VVs through the execution agent (where the decision/action is generated) and interaction interface (where the decision/action is executed through network). Please refer to the other resource section of the facilities, equipment and other resources section for one or more features that we identified as important for a given action based on the preliminary study.
There are some key solutions needed to construct the VV model, including the technology to translate the different formats of incoming data, such as data from the roadside sensor and the network data from the intelligent transportation systems (ITS). To meet these technological needs, we plan to introduce the ontology needed to model the enormous dataset that the proposed project will handle. Similarly, the VV model requires technology to accurately describe the driver’s knowledge. In the VV model, knowledge provides the matching rules, and the VV makes decisions according to the result of fact matching. However, different drivers may have different knowledge, and drivers’ knowledge may change from interaction with other VVs. The matching rule must be accurate in describing the driver’s knowledge, mining the user’s intentions, and capturing them in the VV information space. Finally, VV interaction is another key technology. In our interactive VV model, each VV may take different actions in the same scenario, depending on each driver’s intentions.
Virtual vehicle and driver intention learning model
Since VV is a virtual state of both vehicle and driver, it must learn the features of each through interaction; it must do the samewithotherVVs,interactivelylearningtheircurrentstate and their intentions. Thus, the higher the number of VVs, the easier and more effective would the learning be. Machine learning (ML) algorithms have been traditionally applied for learning [30–38, 39].
We have designed a deep learning technique, recurrent neuralnetworks(RNN),toharnesstheknowledgeneededfor routeselection.Recentstudieshaveshownthatdeeplearning techniques such as long short-term memory (LSTM)-based sequence to sequence RNNs perform better for connected vehicle applications [40–42]. For this reason, we utilized LSTM-based RNNs for VV learning. In this algorithm, a fixed number of neural networks are set and neural networks are used to be trained from new sample data each time. One proposed research task is to deduce the generalizable theory that underlies our already developed algorithm and verify it; another is to leverage the learning approach algorithms and techniques developed earlier for related problems [43, 44–47].
Research challenges and future work recommendations
The VV must interactively learn large amounts of information both accurately and quickly. However, the present approaches to learning all have high time complexity and cannot be directly adopted for VV knowledge acquisition. For this reason, fast, efficient learning is a key research challenge for this objective. To solve this challenge, we divide the process of VV learning into two phases: the individual action learning (IAL) phase, which uses an RNN model for acquisition of knowledge of common functions, and the joint action learning (JAL phase, which adapts the incremental learning model to allow VVs to acquire other vehicles’ intentions and knowledge through real-time interactions. The IAL model consists of three layers: (1) the input layer, the fact database of the vehicle acts as the input for the RNNs; (2) the hidden layer, where we set an activation function and a threshold to solve the nonlinear problem; the knowledge base of the driver initializes the activation function and the threshold in the RNNs; additional hidden layers can be set to improve learning accuracy; and (3) the output layer is where the VVs can obtain common knowledge, which can be stored to give newly created VVs immediate knowledge. In the JAL phase, VVs must quickly and effectively acquire knowledge from the corresponding vehicle and driver and from other VVs through online interaction using the incremental learning model. The JAL model consists of four layers: (1) in JAL layer one, each node represents an input variable and directly transmits the input signal to layer two; (2) in JAL layer two, each node represents the membership value of each input variable; (3) in JAL layer three, each node represents the “if” part of if–then rules obtained by the sum-product composition and the total number of such rules; (4) in JAL layer four, each node corresponds to an output variable that is given by the weighted sum of the output of each normalized rule. This model allows the VV to dynamically learn from other VVs. Moreover,wehaverealdata,asetoftaxicabtracescontaining recorded GPS trajectories from more than 7000 taxicabs in Novemberof2012[48]thatwecanusetotrainourmodel.We will use the drivers’ experience and the taxis’ GPS trajectoriesastheinputfortheIALmodelandtrafficconditions,such as vehicle speed and weather, as the input for the JAL model.
This research objective aims to produce a fast and deep learning approach that will allow VVs to make correct deci-
sions for the driver and we will design several algorithms to accomplish this goal. To evaluate this outcome, we plan to use an existing dataset for testing purposes.
The intelligent ITS:VV coordination
Some of the existing vehicle cooperation approaches such as vehicular ad hoc networks (VANET) and navigation-based approaches lack the ability to coordinate automated vehicles or communication between vehicles and traffic infrastructure [6, 49, 50, 51–54] efficiently. Furthermore, data collection, a key requirement for enabling routing and coordinating services in the vehicular network, has recently attracted considerable research interest.
Experimentation and results
To reduce data redundancy, we propose a VV cooperation approach that is based on the use of a coalition game algorithm in the cloud. In this approach, as shown in Fig. 2, each VV need not upload its (possibly redundant) captured data directly to the data center; instead, each VV interacts with other VVs, forming a coalition to collect data cooperatively. In our coalition algorithm, VVs first ascertain how captured data can be gathered and then form coalitions by exchanging this data gathering information. This coalition formation means that members can individually contribute to a scalable view of the data.
Ourpreliminaryexperimentsweredrivenbythedatafrom TAPAS [55], a system that computes mobility plans for an areapopulation,generatedfrominformationaboutGermans’ traveling habits and the infrastructure of the areas in which they live. We used the traffic simulation software SUMo [56] to generate vehicle traces from real data. We divided all the roads into 100 segments in an area of 600 m×600 m, numbering each segment so that individual vehicles could be linked to their trace. We used the following three related algorithms to compare and evaluate the effectiveness of our approach: (i) the Max Greedy algorithm, where the sensing center selects a virtual vehicle that has the highest number of non-repeated data blocks; (ii) the Min Greedy algorithm, where a virtual vehicle is selected by the sensing center if it has both the least number of non-repeated data blocks for the sensing center and the least number of data blocks compared with the last vehicles; and (iii) the random algorithm, where virtual vehicles gather data and individually transmit it to the sensing center in a random manner.
We evaluated our algorithm with the related algorithms using two metrics: the ratio redundancy metric and the success rate metric. The ratio redundancy is defined as follows:
ξ  ,
 i
where n denotes the number of virtual vehicles that can provide complete data with M blocks, and Mi(≤ M) denotes the number of blocks that vehicle i can gather. The success rate is defined as ρ 1−nNc , where nc denotes the number of virtual vehicles in the stable coalition and n denotes the total number of virtual vehicles in our experiments. This is

Fig. 4 Experimental results for VV coordination. a Number of vehicles vs. ratio of redundancy; b number of vehicles vs. success rate
based on the rationale that coalitions with fewer vehicles can help achieve coordination faster. The results of our preliminary experiments, in which M and n are set to 100 for rate of redundancy and success rate metrics, respectively, are shown in Fig. 4, which demonstrates that our approach is an effective way to solve data redundancy without central control. Data collection is a relatively simple task for VVs in coordination, but traffic management requires a huge number of coordinated vehicles, and the coordinating process is more complex than data collection.
Our paper illustrates a novel intelligent traffic management framework. Intelligent traffic management is acquiring special significance as the number of smart cities across the countries is growing steadily. The much-needed intelligence is realized by accurately predicting traffic congestions and chaosatcertainplacesandbyprescribingthewaysandmeans of moderating the traffic jams and snarls.
The technologies and tools used are software-defined cloud environments, digital twin, artificial intelligence (AI) (machine and deep learning algorithms), data lake, real-time data capture, storage, processing, analytics, decision-making and action through IoT edge analytics, edge and public cloud integration, etc. By leveraging the proven, potential, and promising technologies, we arrive at a framework, which guarantees the much-needed accuracy in decision-making and subsequent actions. The digital twin is the virtual and logicalrepresentationofphysicalassetsandprocesses.There is a direct communication between physical and digital systems to collect the latest data.
Machine and deep learning algorithms are capable of analyzing big data in real time to extract actionable insights in time and the discovered knowledge gets disseminated to the particular junctions and locations to streamline the traffic movement in a smooth manner so as to avoid time wastage in those places.
Research challenges and future work recommendations
There are two key challenges to the coordination research objective: (1) many VVs must achieve coordination with each other in a short time; and (2) we must consider the intention of every VV in the process of coordination. In our VV architecture, as shown in Fig. 2, VVs can cooperate and send the cooperative results to the vehicles to provide a safe and pleasant experience for vehicles on the road. We, therefore, propose an approach based on the contract net protocol to overcome the challenges of a virtual transportation network. In our approach, we first assign weights between two VVs; vehicles that may produce traffic congestion are assigned traffic dispersion tasks. When a VV accepts the task, it becomes a manager and is responsible for sending and allocating the task to other VVs. These vehicles can then communicate with other VVs to make decisions. We plan to assess the proposed approach by validating the decisions that the vehicles make.
Conclusion
The transport sector is poised for accomplishing better and bigger things in the years ahead with the consistent flow of path-breaking technologies and tools. A bevy of pioneering technologies in information, communication, sensing, perception,vision,integration,knowledgediscoveryanddissemination, and decision enablement collectively are bound todoa lotof greater things for theautomotive industry. There are already intelligent transport systems (ITS) and, now with the addition of real-time information gathering and analytics, we can safely expect ground-breaking accomplishments for the transport and logistics industry verticals. The faster proliferation of machine and deep learning algorithms along with the evolving concept of digital twin and blockchain goes a long way in bringing more sophisticated and smarter cars, trucks, buses, ships, trains, rocket and satellites, aeroplanes,andothertransportsolutions.Inshort,itisgoingtobe a technology-splurged and software-defined world bringing immense and immeasurable benefits for every citizen of this planet Earth.
"
